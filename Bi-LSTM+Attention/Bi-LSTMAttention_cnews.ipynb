{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.18\n",
    "    @Purpose: 中文文本分类实战（五）—— Bi-LSTM + Attention模型\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets: cnews 中文文本分类数据集。\n",
    "    @Link : https://github.com/jiangxinyang227/textClassifier/tree/master/Bi-LSTM%2BAttention\n",
    "    @Reference : https://www.cnblogs.com/jiangxinyang/p/10208227.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集介绍\n",
    "\n",
    "cnews 中文文本分类数据集，其目标是判断一段话的类别，数据预处理如文本分类实战（一）—— word2vec预训练词向量中一样，预处理后的文件为../data/cnews_data/labeledCharTrain.csv。\n",
    "\n",
    "\n",
    "##  Bi-LSTM + Attention 模型介绍\n",
    "　  \n",
    "Bi-LSTM + Attention模型来源于论文Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification。关于Attention的介绍见这篇。\n",
    "\n",
    "Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，<span class=\"burk\">在Bi-LSTM中我们会用最后一个时序的输出向量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类</span>。在实验中，加上Attention确实对结果有所提升。其模型结构如下图：\n",
    "  \n",
    "![avatar](../images/BI-LSTM+Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 4\n",
    "    evaluateEvery = 10\n",
    "    checkpointEvery = 10\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5   \n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/cnews_data/labeledCharTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/cnews_data/stopwords.txt\"\n",
    "    \n",
    "    numClasses = 10\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据\n",
    "\n",
    "　　1）将数据加载进来，将句子分割成词表示，并去除低频词和停用词。\n",
    "\n",
    "　　2）将词映射成索引表示，构建词汇-索引映射表，并保存成json的数据格式，之后做inference时可以用到。（注意，有的词可能不在word2vec的预训练词向量中，这种词直接用UNK表示）\n",
    "\n",
    "　　3）从预训练的词向量模型中读取出词向量，作为初始化值输入到模型中。\n",
    "\n",
    "　　4）将数据集分割成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.trainReviews[0:2]:[[   1    1    1    1    1    1    1    1    1    1    1    1 1131    1\n",
      "     1    1    1  478  142    1   27    2    1  483    1    1    1    1\n",
      "     1    1 1281    1    1    1    1    1    1  886    1    1    1    1\n",
      "     1    1    1    1    1    1 1005    1    1    1   13    1    1    1\n",
      "     1    1    1    1    1    1    1  203  215    1    1    1    1    1\n",
      "     1    1   27    1    1    1    1    1  102    1  152    1    1    1\n",
      "     1  451    1    1  753    1    1  226    1    1   65    1    1    1\n",
      "     1    1    1   19    1    1    1    1    1  849    1    1 2243    1\n",
      "     1    1    1    1    1    1    1    1  878    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1  419    1    1\n",
      "   196    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1   10    1    1    8    1    1 1654    1    1    1    1    1\n",
      "     1    1    1   36    1    1    1 1151    1    1    1    1    1    1\n",
      "     1    1  120    1    1 1162    1    1    1    1    1    1    1    1\n",
      "     1 1077 1901    1]\n",
      " [   1    1    1    1    1    1    1    1  518   14    1    1    1    1\n",
      "     1    1    1    1    1   84   44    1  375    1    1    1    1    1\n",
      "     1    1    1    1    1  363    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1   27    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1   19    1    1    1    1    1    1  263    1    1    1\n",
      "   264  609    1    1 1121    1    1   86   14   86  920    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1 1122    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1   27    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1 1163    1    1\n",
      "   419    1    1    1    1    1    1    1    1    1    1 1070    1    1\n",
      "     1    1    1    1    1    1  716    1    1    1    1    1    1   27\n",
      "     2    1    1    1    1    1    1  503    1    1    1    1   27    1\n",
      "   811    1   14  364]\n",
      " [ 388    1   51    1    1    1    1    1    1  501    1    1    1    1\n",
      "     1    1    1    1    1    4  241    1  388    1    1    1  241    6\n",
      "   397    1  294    1    1  439    1    1    1    1    1    1    1    1\n",
      "   104    1    3  379    1    1    1    1   51    1    1    1    1    1\n",
      "     3    1    1    1    1    1    1    1    1  190  198    1    1    1\n",
      "     1  341  198    1  152    1    1    1    1    1    1    1    1    1\n",
      "    74  237  198    1    1    1    1    1  909    1    1    1  223  278\n",
      "   198    1    1    1    1    1    1    1    1    1    1    1  588    1\n",
      "     1    1    1   65    1    1    1    1    1    1  104    1    1    1\n",
      "     1  379    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1 1616    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1   11    1    1    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "self.trainLabels[0:2]:[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "self.evalReviews[0:2]:[[   1   73    1  516    1    1    1   16  516    1    1    1    1    1\n",
      "     1    1 1352    1    1    1    1    1  967    1  966    1   73  889\n",
      "   286    1 1723    1    1    1 2621    1  516    1    1    1    1    1\n",
      "     1    1    1    1   43    1    1  919 2538    1    1    1    1    1\n",
      "   596    1    1    1    1    1    1    1  949    1    1    1    1    1\n",
      "     1    1    1    1  190    1  994    1    1  369    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1 1034    1    1    1    1    1\n",
      "     1    1   60    1    1    1  534    1  516    1    1    1    1    1\n",
      "     1    1   47    1    1    1    1    1    1    1    1  873    1    1\n",
      "     1    1    1   47  516    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1   43  207    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1   31    1    1    1    1    1    1\n",
      "     1    1    1    1    1 1060    1  101    1    1    1    1    1    1\n",
      "   687    1   40    1    1  516    1    1    1    1    1    1    1   47\n",
      "     1    1    1    1]\n",
      " [   1    1    1    1 1225  516    1  313    1  209    1   16    1    1\n",
      "     1  516  602    1    1   62    1    1    1    1   24    1   52    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1   52\n",
      "     1    1    1   24    1    1  234  654    1    1    1 1723    1  339\n",
      "   289    1    1    1   32    1  874    1    1    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1    1    1 1202    1    1    1   11   35    1    1    1    1    1\n",
      "     1    1    1    1  100    1    1 1457    1    1    1    1    1    1\n",
      "     1    1    1    1  184    1    1   71    1    1   52    1  208  177\n",
      "     1    1    1    1    1    1    1    1    1    1    1   52  208    1\n",
      "     1   78    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1  126    1    1   34\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1   16    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1   78    1    1    1    1    1    1    1    2\n",
      "     1    1  244    1    1    1    1    1   41    1    1    1    1    1\n",
      "     1   35    1    1    1    1    1    1  339    1    1    1   32    1\n",
      "   652  243  546   35  118    1    1    1    1   35    1    1  145    1\n",
      "     1    1    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "self.evalLabels[0:2]:[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 整块 update\n",
    "import sys\n",
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self.numClasses = config.numClasses        # update\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        self.label_to_index = {}\n",
    "        self.index_to_label = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"class\"].tolist()                # update\n",
    "        review = df[\"review_cut_word\"].tolist()\n",
    "        reviews = [str(line).strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _labelProcess(self, y, label_to_index):\n",
    "        \"\"\"\n",
    "        将数据集中的每个label用index表示\n",
    "        label_to_index“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        input_y = []\n",
    "        for item in y:\n",
    "            temp = [0] * self.numClasses\n",
    "            temp[label_to_index[item]] = 1\n",
    "            input_y.append(temp)\n",
    "        \n",
    "        return input_y\n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        input_y = self._labelProcess(y,self.label_to_index)\n",
    "#         print(\"input_y:{0}\".format(input_y[0:2]))\n",
    "        \n",
    "#         sys.exit(0)\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append(input_y[i])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews,labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "\n",
    "        labels = set(labels)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            self.label_to_index[label] = i\n",
    "            self.index_to_label[int(i)] = label\n",
    "        \n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex_cnews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord_cnews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "        with open('../data/wordJson/label_to_index_cnews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.label_to_index, f, ensure_ascii=False)\n",
    "\n",
    "        with open('../data/wordJson/index_to_label_cnews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.index_to_label, f, ensure_ascii=False)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../data/ChnSentiCorp_cn_data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                #print(word + \"不存在于词向量中\")\n",
    "                pass\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\", encoding=\"utf-8\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews,labels)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        print(\"self.trainReviews[0:2]:{0}\".format(self.trainReviews[0:3]))\n",
    "        print(\"self.trainLabels[0:2]:{0}\".format(self.trainLabels[0:3]))\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        print(\"self.evalReviews[0:2]:{0}\".format(self.evalReviews[0:3]))\n",
    "        print(\"self.evalLabels[0:2]:{0}\".format(self.evalLabels[0:3]))\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (3999, 200)\n",
      "train label shape: (3999, 10)\n",
      "eval data shape: (1000, 200)\n",
      "wordEmbedding data shape: (3302, 200)\n",
      "_wordToIndex data shape: 3302\n",
      "_indexToWord data shape: 3302\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"wordEmbedding data shape: {}\".format(data.wordEmbedding.shape))\n",
    "print(\"_wordToIndex data shape: {}\".format(len(data._wordToIndex)))\n",
    "print(\"_indexToWord data shape: {}\".format(len(data._indexToWord)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成batch数据集\n",
    "\n",
    "　　采用生成器的形式向模型输入batch数据集，（生成器可以避免将所有的数据加入到内存中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        '''\n",
    "         生成batch数据集，用生成器的方式输出\n",
    "        \n",
    "        :param x:      list    \n",
    "        :param y:      list    \n",
    "        :param batchSize:      int    \n",
    "        :return: \n",
    "            batchX        nnumpy array\n",
    "            batchY        nnumpy array\n",
    "            \n",
    "        '''\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM + Attention模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM + Attention模型 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.numClasses = config.numClasses      # update\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, self.numClasses], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, self.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[self.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            \n",
    "            # update\n",
    "#             self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "#             self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "            self.scores = tf.nn.xw_plus_b(output, outputW, outputB, name='scores')\n",
    "            self.predictions = tf.argmax(self.scores, 1, name='predictions')\n",
    "    \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # update\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "        # accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.inputY, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = 0\n",
    "    try:\n",
    "        auc = roc_auc_score(trueY, predY)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\pythonWp\\nlp\\textClassifier\\Bi-LSTM+Attention\\summarys\n",
      "\n",
      "start training model\n",
      "2019-04-18T10:02:00.094999: step 1, loss 2.40577, acc 0.101562\n",
      "2019-04-18T10:02:07.516152: step 2, loss 2.53451, acc 0.117188\n",
      "2019-04-18T10:02:15.359179: step 3, loss 2.24244, acc 0.15625\n",
      "2019-04-18T10:02:23.103470: step 4, loss 2.24462, acc 0.117188\n",
      "2019-04-18T10:02:30.562523: step 5, loss 2.2474, acc 0.09375\n",
      "2019-04-18T10:02:38.104353: step 6, loss 2.16655, acc 0.15625\n",
      "2019-04-18T10:02:45.914468: step 7, loss 2.23574, acc 0.148438\n",
      "2019-04-18T10:02:53.242872: step 8, loss 2.22543, acc 0.140625\n",
      "2019-04-18T10:03:00.941283: step 9, loss 2.1987, acc 0.132812\n",
      "2019-04-18T10:03:08.687568: step 10, loss 2.22808, acc 0.164062\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:03:48.371446: step 11, loss 5.19992, acc 0\n",
      "Evaluation:2019-04-18T10:04:25.521101: step 12, loss 4.6368, acc 0\n",
      "Evaluation:2019-04-18T10:05:01.998552: step 13, loss 3.85868, acc 0\n",
      "Evaluation:2019-04-18T10:05:37.648218: step 14, loss 3.02478, acc 0\n",
      "Evaluation:2019-04-18T10:06:12.972754: step 15, loss 2.24723, acc 0.138\n",
      "Evaluation:2019-04-18T10:06:47.949221: step 16, loss 1.616, acc 0.5\n",
      "Evaluation:2019-04-18T10:07:22.934662: step 17, loss 1.18814, acc 0.502\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-10\n",
      "\n",
      "2019-04-18T10:07:32.764375: step 18, loss 3.86323, acc 0\n",
      "2019-04-18T10:07:40.715114: step 19, loss 3.97502, acc 0\n",
      "2019-04-18T10:07:48.440455: step 20, loss 3.67168, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:08:21.865072: step 21, loss 1.00041, acc 0.515\n",
      "Evaluation:2019-04-18T10:08:56.780710: step 22, loss 1.04244, acc 0.5\n",
      "Evaluation:2019-04-18T10:09:32.292736: step 23, loss 1.06284, acc 0.5\n",
      "Evaluation:2019-04-18T10:10:07.498587: step 24, loss 1.06112, acc 0.5\n",
      "Evaluation:2019-04-18T10:10:40.044553: step 25, loss 1.04262, acc 0.549\n",
      "Evaluation:2019-04-18T10:11:14.992124: step 26, loss 1.01371, acc 0.542\n",
      "Evaluation:2019-04-18T10:11:50.055330: step 27, loss 0.978601, acc 0.539\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-20\n",
      "\n",
      "2019-04-18T10:11:58.466836: step 28, loss 3.76718, acc 0\n",
      "2019-04-18T10:12:05.666583: step 29, loss 3.73573, acc 0\n",
      "2019-04-18T10:12:13.700102: step 30, loss 3.60932, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:12:48.384348: step 31, loss 1.00745, acc 0.5\n",
      "Evaluation:2019-04-18T10:13:22.568931: step 32, loss 1.04045, acc 0.5\n",
      "Evaluation:2019-04-18T10:13:56.718608: step 33, loss 1.0613, acc 0.495\n",
      "Evaluation:2019-04-18T10:14:31.392884: step 34, loss 1.07019, acc 0.5\n",
      "Evaluation:2019-04-18T10:15:06.896302: step 35, loss 1.06933, acc 0.5\n",
      "Evaluation:2019-04-18T10:15:41.674298: step 36, loss 1.06053, acc 0.5\n",
      "Evaluation:2019-04-18T10:16:18.068973: step 37, loss 1.04515, acc 0.5\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-30\n",
      "\n",
      "2019-04-18T10:16:26.952215: step 38, loss 3.32966, acc 0\n",
      "2019-04-18T10:16:34.252696: step 39, loss 3.39663, acc 0\n",
      "2019-04-18T10:16:41.970057: step 40, loss 3.23468, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:17:16.919593: step 41, loss 1.06957, acc 0.529\n",
      "Evaluation:2019-04-18T10:17:51.964875: step 42, loss 1.09109, acc 0.526\n",
      "Evaluation:2019-04-18T10:18:27.233561: step 43, loss 1.10049, acc 0.53\n",
      "Evaluation:2019-04-18T10:19:01.624592: step 44, loss 1.09924, acc 0.53\n",
      "Evaluation:2019-04-18T10:19:37.057838: step 45, loss 1.09035, acc 0.542\n",
      "Evaluation:2019-04-18T10:20:11.905646: step 46, loss 1.07647, acc 0.548\n",
      "Evaluation:2019-04-18T10:20:47.342882: step 47, loss 1.05909, acc 0.597\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-40\n",
      "\n",
      "2019-04-18T10:20:56.134372: step 48, loss 3.29096, acc 0\n",
      "2019-04-18T10:21:03.814833: step 49, loss 3.30861, acc 0\n",
      "2019-04-18T10:21:11.422488: step 50, loss 3.40617, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:21:47.035253: step 51, loss 1.06348, acc 0.5\n",
      "Evaluation:2019-04-18T10:22:25.400657: step 52, loss 1.07664, acc 0.501\n",
      "Evaluation:2019-04-18T10:23:00.061964: step 53, loss 1.0822, acc 0.515\n",
      "Evaluation:2019-04-18T10:23:34.911772: step 54, loss 1.08073, acc 0.589\n",
      "Evaluation:2019-04-18T10:24:10.267223: step 55, loss 1.07302, acc 0.548\n",
      "Evaluation:2019-04-18T10:24:44.896618: step 56, loss 1.06004, acc 0.545\n",
      "Evaluation:2019-04-18T10:25:20.333852: step 57, loss 1.04288, acc 0.539\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-50\n",
      "\n",
      "2019-04-18T10:25:29.319822: step 58, loss 3.32126, acc 0\n",
      "2019-04-18T10:25:36.851679: step 59, loss 3.40935, acc 0\n",
      "2019-04-18T10:25:44.542114: step 60, loss 3.31298, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:26:22.069758: step 61, loss 1.04413, acc 0.537\n",
      "Evaluation:2019-04-18T10:27:01.077444: step 62, loss 1.05627, acc 0.542\n",
      "Evaluation:2019-04-18T10:27:39.712128: step 63, loss 1.06175, acc 0.546\n",
      "Evaluation:2019-04-18T10:28:18.689897: step 64, loss 1.0611, acc 0.55\n",
      "Evaluation:2019-04-18T10:28:56.685831: step 65, loss 1.05503, acc 0.547\n",
      "Evaluation:2019-04-18T10:29:39.204129: step 66, loss 1.04436, acc 0.549\n",
      "Evaluation:2019-04-18T10:30:21.561364: step 67, loss 1.02989, acc 0.55\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-60\n",
      "\n",
      "2019-04-18T10:30:33.187274: step 68, loss 3.39699, acc 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-18T10:30:41.300593: step 69, loss 3.3822, acc 0\n",
      "2019-04-18T10:30:49.114683: step 70, loss 3.34916, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:31:29.406932: step 71, loss 1.04023, acc 0.557\n",
      "Evaluation:2019-04-18T10:32:07.374401: step 72, loss 1.05551, acc 0.562\n",
      "Evaluation:2019-04-18T10:32:46.436940: step 73, loss 1.06378, acc 0.551\n",
      "Evaluation:2019-04-18T10:33:25.554333: step 74, loss 1.06499, acc 0.546\n",
      "Evaluation:2019-04-18T10:34:03.280447: step 75, loss 1.05979, acc 0.549\n",
      "Evaluation:2019-04-18T10:34:40.969658: step 76, loss 1.04921, acc 0.548\n",
      "Evaluation:2019-04-18T10:35:18.110336: step 77, loss 1.03436, acc 0.549\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-70\n",
      "\n",
      "2019-04-18T10:35:29.451010: step 78, loss 3.26976, acc 0\n",
      "2019-04-18T10:35:38.107858: step 79, loss 3.29771, acc 0\n",
      "2019-04-18T10:35:45.968838: step 80, loss 3.36189, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:36:30.442417: step 81, loss 1.04213, acc 0.659\n",
      "Evaluation:2019-04-18T10:37:15.464022: step 82, loss 1.0576, acc 0.63\n",
      "Evaluation:2019-04-18T10:37:56.854337: step 83, loss 1.06517, acc 0.631\n",
      "Evaluation:2019-04-18T10:38:35.346401: step 84, loss 1.06461, acc 0.669\n",
      "Evaluation:2019-04-18T10:39:13.330824: step 85, loss 1.05675, acc 0.648\n",
      "Evaluation:2019-04-18T10:39:49.642719: step 86, loss 1.04308, acc 0.569\n",
      "Evaluation:2019-04-18T10:40:34.181613: step 87, loss 1.02454, acc 0.55\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-80\n",
      "\n",
      "start training model\n",
      "2019-04-18T10:40:47.790222: step 88, loss 3.25235, acc 0\n",
      "2019-04-18T10:40:56.571737: step 89, loss 3.41536, acc 0\n",
      "2019-04-18T10:41:04.928390: step 90, loss 3.32156, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:41:50.618207: step 91, loss 1.02787, acc 0.547\n",
      "Evaluation:2019-04-18T10:42:32.041431: step 92, loss 1.03547, acc 0.606\n",
      "Evaluation:2019-04-18T10:43:08.022214: step 93, loss 1.0356, acc 0.701\n",
      "Evaluation:2019-04-18T10:43:44.199468: step 94, loss 1.0288, acc 0.726\n",
      "Evaluation:2019-04-18T10:44:19.542953: step 95, loss 0.997644, acc 0.749\n",
      "Evaluation:2019-04-18T10:44:56.175989: step 96, loss 0.993582, acc 0.609\n",
      "Evaluation:2019-04-18T10:45:34.854125: step 97, loss 0.980877, acc 0.649\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-90\n",
      "\n",
      "2019-04-18T10:45:45.311158: step 98, loss 3.38113, acc 0\n",
      "2019-04-18T10:45:52.818085: step 99, loss 3.33517, acc 0\n",
      "2019-04-18T10:46:00.614235: step 100, loss 3.16523, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:46:36.531186: step 101, loss 0.940594, acc 0.834\n",
      "Evaluation:2019-04-18T10:47:12.625663: step 102, loss 0.936185, acc 0.862\n",
      "Evaluation:2019-04-18T10:47:48.648333: step 103, loss 0.916755, acc 0.904\n",
      "Evaluation:2019-04-18T10:48:24.248131: step 104, loss 0.875742, acc 0.883\n",
      "Evaluation:2019-04-18T10:49:00.711621: step 105, loss 0.78573, acc 0.914\n",
      "Evaluation:2019-04-18T10:49:36.044135: step 106, loss 0.705956, acc 0.937\n",
      "Evaluation:2019-04-18T10:50:11.760624: step 107, loss 0.618075, acc 0.96\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-100\n",
      "\n",
      "2019-04-18T10:50:20.940073: step 108, loss 3.38833, acc 0\n",
      "2019-04-18T10:50:28.813020: step 109, loss 3.42387, acc 0\n",
      "2019-04-18T10:50:36.560304: step 110, loss 3.22752, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:51:12.813354: step 111, loss 0.870601, acc 0.815\n",
      "Evaluation:2019-04-18T10:51:53.011855: step 112, loss 0.832862, acc 0.913\n",
      "Evaluation:2019-04-18T10:52:31.869941: step 113, loss 0.675187, acc 0.987\n",
      "Evaluation:2019-04-18T10:53:12.355676: step 114, loss 0.609271, acc 0.985\n",
      "Evaluation:2019-04-18T10:53:47.930542: step 115, loss 0.566461, acc 0.976\n",
      "Evaluation:2019-04-18T10:54:27.210501: step 116, loss 0.47649, acc 0.992\n",
      "Evaluation:2019-04-18T10:55:03.561291: step 117, loss 0.403205, acc 0.997\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-110\n",
      "\n",
      "2019-04-18T10:55:12.834491: step 118, loss 3.50258, acc 0\n",
      "2019-04-18T10:55:20.441150: step 119, loss 3.29359, acc 0\n",
      "2019-04-18T10:55:28.063767: step 120, loss 3.17007, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T10:56:06.550844: step 121, loss 0.848286, acc 0.993\n",
      "Evaluation:2019-04-18T10:56:42.626372: step 122, loss 0.723653, acc 0.997\n",
      "Evaluation:2019-04-18T10:57:24.259038: step 123, loss 0.705264, acc 0.994\n",
      "Evaluation:2019-04-18T10:58:01.506430: step 124, loss 0.613676, acc 0.993\n",
      "Evaluation:2019-04-18T10:58:40.419371: step 125, loss 0.524558, acc 0.994\n",
      "Evaluation:2019-04-18T10:59:16.813048: step 126, loss 0.412526, acc 0.996\n",
      "Evaluation:2019-04-18T10:59:56.667471: step 127, loss 0.331351, acc 0.995\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-120\n",
      "\n",
      "2019-04-18T11:00:07.940678: step 128, loss 3.51157, acc 0\n",
      "2019-04-18T11:00:15.368813: step 129, loss 3.69231, acc 0\n",
      "2019-04-18T11:00:22.762044: step 130, loss 3.35641, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:00:59.979517: step 131, loss 0.681843, acc 0.972\n",
      "Evaluation:2019-04-18T11:01:35.245211: step 132, loss 0.680257, acc 0.99\n",
      "Evaluation:2019-04-18T11:02:11.393541: step 133, loss 0.574256, acc 0.995\n",
      "Evaluation:2019-04-18T11:02:48.064476: step 134, loss 0.514599, acc 0.997\n",
      "Evaluation:2019-04-18T11:03:23.720125: step 135, loss 0.468628, acc 0.996\n",
      "Evaluation:2019-04-18T11:03:59.714870: step 136, loss 0.417508, acc 0.994\n",
      "Evaluation:2019-04-18T11:04:35.647777: step 137, loss 0.356344, acc 0.996\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-130\n",
      "\n",
      "2019-04-18T11:04:46.541646: step 138, loss 3.39987, acc 0\n",
      "2019-04-18T11:04:54.295911: step 139, loss 3.34702, acc 0\n",
      "2019-04-18T11:05:01.883619: step 140, loss 3.12337, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:05:45.754300: step 141, loss 0.601695, acc 0.967\n",
      "Evaluation:2019-04-18T11:06:22.894977: step 142, loss 0.59555, acc 0.992\n",
      "Evaluation:2019-04-18T11:06:58.991448: step 143, loss 0.495781, acc 0.986\n",
      "Evaluation:2019-04-18T11:07:37.734842: step 144, loss 0.387831, acc 0.993\n",
      "Evaluation:2019-04-18T11:08:24.139746: step 145, loss 0.312858, acc 0.998\n",
      "Evaluation:2019-04-18T11:09:11.125611: step 146, loss 0.257805, acc 0.998\n",
      "Evaluation:2019-04-18T11:09:57.016889: step 147, loss 0.215046, acc 0.999\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-140\n",
      "\n",
      "2019-04-18T11:10:10.851892: step 148, loss 3.57205, acc 0\n",
      "2019-04-18T11:10:20.362460: step 149, loss 3.34949, acc 0\n",
      "2019-04-18T11:10:29.707468: step 150, loss 3.23104, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:11:28.573050: step 151, loss 0.495628, acc 0.979\n",
      "Evaluation:2019-04-18T11:12:20.352582: step 152, loss 0.503209, acc 0.994\n",
      "Evaluation:2019-04-18T11:13:06.579961: step 153, loss 0.48525, acc 0.999\n",
      "Evaluation:2019-04-18T11:13:51.547708: step 154, loss 0.441965, acc 0.996\n",
      "Evaluation:2019-04-18T11:14:34.433024: step 155, loss 0.38376, acc 0.995\n",
      "Evaluation:2019-04-18T11:15:12.968087: step 156, loss 0.327063, acc 0.998\n",
      "Evaluation:2019-04-18T11:15:51.513010: step 157, loss 0.279246, acc 0.998\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-150\n",
      "\n",
      "2019-04-18T11:16:04.098356: step 158, loss 3.30706, acc 0\n",
      "2019-04-18T11:16:12.203682: step 159, loss 3.11656, acc 0.0078125\n",
      "2019-04-18T11:16:20.591250: step 160, loss 3.02992, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:17:00.205316: step 161, loss 0.550559, acc 0.957\n",
      "Evaluation:2019-04-18T11:17:37.225317: step 162, loss 0.530846, acc 0.986\n",
      "Evaluation:2019-04-18T11:18:15.017253: step 163, loss 0.47775, acc 0.998\n",
      "Evaluation:2019-04-18T11:18:54.425867: step 164, loss 0.439401, acc 0.994\n",
      "Evaluation:2019-04-18T11:19:33.351772: step 165, loss 0.393907, acc 0.989\n",
      "Evaluation:2019-04-18T11:20:16.341808: step 166, loss 0.325517, acc 0.989\n",
      "Evaluation:2019-04-18T11:20:59.047603: step 167, loss 0.257619, acc 0.995\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-160\n",
      "\n",
      "2019-04-18T11:21:08.271937: step 168, loss 3.43121, acc 0\n",
      "2019-04-18T11:21:16.535840: step 169, loss 3.33534, acc 0\n",
      "2019-04-18T11:21:24.860576: step 170, loss 3.19187, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:22:03.766534: step 171, loss 0.354702, acc 0.988\n",
      "Evaluation:2019-04-18T11:22:41.652222: step 172, loss 0.395269, acc 0.983\n",
      "Evaluation:2019-04-18T11:23:18.849746: step 173, loss 0.364234, acc 0.99\n",
      "Evaluation:2019-04-18T11:23:58.690204: step 174, loss 0.306571, acc 0.998\n",
      "Evaluation:2019-04-18T11:24:38.854798: step 175, loss 0.256342, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:2019-04-18T11:25:16.610832: step 176, loss 0.222357, acc 0.999\n",
      "Evaluation:2019-04-18T11:25:55.819978: step 177, loss 0.199274, acc 0.999\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-170\n",
      "\n",
      "2019-04-18T11:26:07.922614: step 178, loss 3.31153, acc 0\n",
      "2019-04-18T11:26:15.938178: step 179, loss 3.1325, acc 0\n",
      "2019-04-18T11:26:24.678804: step 180, loss 3.13228, acc 0.0078125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:27:04.170197: step 181, loss 0.221092, acc 1\n",
      "Evaluation:2019-04-18T11:27:43.374357: step 182, loss 0.264569, acc 0.999\n",
      "Evaluation:2019-04-18T11:28:25.465797: step 183, loss 0.301743, acc 0.998\n",
      "Evaluation:2019-04-18T11:29:04.261051: step 184, loss 0.316897, acc 0.997\n",
      "Evaluation:2019-04-18T11:29:43.406367: step 185, loss 0.304886, acc 0.995\n",
      "Evaluation:2019-04-18T11:30:22.272158: step 186, loss 0.2736, acc 0.995\n",
      "Evaluation:2019-04-18T11:31:01.315746: step 187, loss 0.234461, acc 0.997\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-180\n",
      "\n",
      "2019-04-18T11:31:12.985541: step 188, loss 3.00182, acc 0\n",
      "start training model\n",
      "2019-04-18T11:31:21.060945: step 189, loss 2.81835, acc 0.0078125\n",
      "2019-04-18T11:31:29.603101: step 190, loss 2.86296, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:32:09.421620: step 191, loss 0.343388, acc 0.994\n",
      "Evaluation:2019-04-18T11:32:50.104824: step 192, loss 0.400101, acc 0.991\n",
      "Evaluation:2019-04-18T11:33:29.261112: step 193, loss 0.358736, acc 0.993\n",
      "Evaluation:2019-04-18T11:34:07.944666: step 194, loss 0.28071, acc 0.996\n",
      "Evaluation:2019-04-18T11:34:47.842971: step 195, loss 0.21783, acc 0.998\n",
      "Evaluation:2019-04-18T11:35:27.411156: step 196, loss 0.178443, acc 0.998\n",
      "Evaluation:2019-04-18T11:36:11.089352: step 197, loss 0.151905, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-190\n",
      "\n",
      "2019-04-18T11:36:26.838237: step 198, loss 2.84044, acc 0.0078125\n",
      "2019-04-18T11:36:35.737438: step 199, loss 2.92846, acc 0.03125\n",
      "2019-04-18T11:36:43.968427: step 200, loss 2.71143, acc 0.015625\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:37:29.083780: step 201, loss 0.181567, acc 1\n",
      "Evaluation:2019-04-18T11:38:19.140917: step 202, loss 0.200896, acc 1\n",
      "Evaluation:2019-04-18T11:38:56.814173: step 203, loss 0.196751, acc 0.999\n",
      "Evaluation:2019-04-18T11:39:34.602121: step 204, loss 0.182659, acc 0.999\n",
      "Evaluation:2019-04-18T11:40:11.561285: step 205, loss 0.169244, acc 0.998\n",
      "Evaluation:2019-04-18T11:40:47.587942: step 206, loss 0.157245, acc 0.998\n",
      "Evaluation:2019-04-18T11:41:25.151490: step 207, loss 0.145514, acc 0.998\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-200\n",
      "\n",
      "2019-04-18T11:41:36.605857: step 208, loss 2.75841, acc 0.0078125\n",
      "2019-04-18T11:41:45.008391: step 209, loss 2.67495, acc 0.03125\n",
      "2019-04-18T11:41:52.921227: step 210, loss 2.48781, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:42:30.056919: step 211, loss 0.194424, acc 0.999\n",
      "Evaluation:2019-04-18T11:43:07.698262: step 212, loss 0.274225, acc 0.99\n",
      "Evaluation:2019-04-18T11:43:50.375133: step 213, loss 0.305242, acc 0.984\n",
      "Evaluation:2019-04-18T11:44:33.309320: step 214, loss 0.251568, acc 0.987\n",
      "Evaluation:2019-04-18T11:45:18.108680: step 215, loss 0.176496, acc 0.994\n",
      "Evaluation:2019-04-18T11:46:17.935757: step 216, loss 0.124464, acc 0.999\n",
      "Evaluation:2019-04-18T11:47:17.037911: step 217, loss 0.094787, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-210\n",
      "\n",
      "2019-04-18T11:47:31.861849: step 218, loss 2.72718, acc 0.0078125\n",
      "2019-04-18T11:47:42.555548: step 219, loss 2.76997, acc 0.046875\n",
      "2019-04-18T11:47:53.023236: step 220, loss 2.59463, acc 0.0703125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:48:46.499959: step 221, loss 0.105501, acc 1\n",
      "Evaluation:2019-04-18T11:49:41.798329: step 222, loss 0.127362, acc 0.999\n",
      "Evaluation:2019-04-18T11:50:39.121403: step 223, loss 0.146208, acc 0.999\n",
      "Evaluation:2019-04-18T11:51:37.323492: step 224, loss 0.159595, acc 0.999\n",
      "Evaluation:2019-04-18T11:52:29.374391: step 225, loss 0.160783, acc 0.999\n",
      "Evaluation:2019-04-18T11:53:18.572517: step 226, loss 0.149014, acc 0.999\n",
      "Evaluation:2019-04-18T11:54:07.534617: step 227, loss 0.129272, acc 0.999\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-220\n",
      "\n",
      "2019-04-18T11:54:20.793414: step 228, loss 2.43217, acc 0.0859375\n",
      "2019-04-18T11:54:31.136482: step 229, loss 2.39593, acc 0.09375\n",
      "2019-04-18T11:54:41.437910: step 230, loss 2.32513, acc 0.0859375\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T11:55:30.035733: step 231, loss 0.148862, acc 0.999\n",
      "Evaluation:2019-04-18T11:56:19.535020: step 232, loss 0.166399, acc 0.996\n",
      "Evaluation:2019-04-18T11:57:08.966658: step 233, loss 0.158128, acc 0.996\n",
      "Evaluation:2019-04-18T11:57:58.148267: step 234, loss 0.131559, acc 0.999\n",
      "Evaluation:2019-04-18T11:58:46.842520: step 235, loss 0.102318, acc 1\n",
      "Evaluation:2019-04-18T11:59:35.974410: step 236, loss 0.0789297, acc 1\n",
      "Evaluation:2019-04-18T12:00:23.425055: step 237, loss 0.0626091, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-230\n",
      "\n",
      "2019-04-18T12:00:35.065894: step 238, loss 2.42505, acc 0.125\n",
      "2019-04-18T12:00:45.171090: step 239, loss 2.4069, acc 0.09375\n",
      "2019-04-18T12:00:55.959408: step 240, loss 2.35075, acc 0.148438\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:01:46.042131: step 241, loss 0.0645079, acc 1\n",
      "Evaluation:2019-04-18T12:02:27.388130: step 242, loss 0.0769447, acc 1\n",
      "Evaluation:2019-04-18T12:03:05.501208: step 243, loss 0.0885697, acc 0.999\n",
      "Evaluation:2019-04-18T12:03:43.865614: step 244, loss 0.095799, acc 0.996\n",
      "Evaluation:2019-04-18T12:04:21.789200: step 245, loss 0.0956227, acc 0.994\n",
      "Evaluation:2019-04-18T12:05:00.379003: step 246, loss 0.0877478, acc 0.997\n",
      "Evaluation:2019-04-18T12:05:38.063227: step 247, loss 0.0752437, acc 0.997\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-240\n",
      "\n",
      "2019-04-18T12:05:47.587757: step 248, loss 2.19887, acc 0.203125\n",
      "2019-04-18T12:05:56.402456: step 249, loss 2.15702, acc 0.148438\n",
      "2019-04-18T12:06:06.876190: step 250, loss 2.16552, acc 0.203125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:06:56.777556: step 251, loss 0.0601565, acc 0.999\n",
      "Evaluation:2019-04-18T12:07:46.172832: step 252, loss 0.0642924, acc 0.998\n",
      "Evaluation:2019-04-18T12:08:35.607460: step 253, loss 0.0659183, acc 0.998\n",
      "Evaluation:2019-04-18T12:09:25.166394: step 254, loss 0.0632064, acc 0.998\n",
      "Evaluation:2019-04-18T12:10:13.635820: step 255, loss 0.0570409, acc 0.998\n",
      "Evaluation:2019-04-18T12:11:03.108428: step 256, loss 0.0495251, acc 0.998\n",
      "Evaluation:2019-04-18T12:11:52.769725: step 257, loss 0.0422619, acc 0.999\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-250\n",
      "\n",
      "2019-04-18T12:12:04.156816: step 258, loss 2.25242, acc 0.164062\n",
      "2019-04-18T12:12:14.727739: step 259, loss 2.16255, acc 0.210938\n",
      "2019-04-18T12:12:24.924606: step 260, loss 2.11131, acc 0.210938\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:13:14.792385: step 261, loss 0.059672, acc 0.998\n",
      "Evaluation:2019-04-18T12:14:03.313479: step 262, loss 0.085977, acc 0.995\n",
      "Evaluation:2019-04-18T12:14:52.905253: step 263, loss 0.101109, acc 0.991\n",
      "Evaluation:2019-04-18T12:15:41.025529: step 264, loss 0.0920171, acc 0.991\n",
      "Evaluation:2019-04-18T12:16:31.516625: step 265, loss 0.0691811, acc 0.997\n",
      "Evaluation:2019-04-18T12:17:20.832861: step 266, loss 0.0482472, acc 0.997\n",
      "Evaluation:2019-04-18T12:18:10.860464: step 267, loss 0.034471, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-260\n",
      "\n",
      "2019-04-18T12:18:22.785622: step 268, loss 2.34296, acc 0.140625\n",
      "2019-04-18T12:18:33.539223: step 269, loss 2.21494, acc 0.179688\n",
      "2019-04-18T12:18:42.529183: step 270, loss 2.05883, acc 0.226562\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:19:23.331069: step 271, loss 0.0377647, acc 0.998\n",
      "Evaluation:2019-04-18T12:20:03.844729: step 272, loss 0.0476506, acc 0.997\n",
      "Evaluation:2019-04-18T12:20:45.382648: step 273, loss 0.0554196, acc 0.996\n",
      "Evaluation:2019-04-18T12:21:25.737731: step 274, loss 0.0569999, acc 0.996\n",
      "Evaluation:2019-04-18T12:22:05.889357: step 275, loss 0.0521913, acc 0.996\n",
      "Evaluation:2019-04-18T12:22:46.326223: step 276, loss 0.0443948, acc 0.997\n",
      "Evaluation:2019-04-18T12:23:26.855838: step 277, loss 0.0368091, acc 0.997\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-270\n",
      "\n",
      "2019-04-18T12:23:36.408292: step 278, loss 2.2965, acc 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-18T12:23:44.799852: step 279, loss 2.23875, acc 0.109375\n",
      "2019-04-18T12:23:52.609967: step 280, loss 2.0972, acc 0.179688\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:24:33.441774: step 281, loss 0.050577, acc 0.998\n",
      "Evaluation:2019-04-18T12:25:13.433827: step 282, loss 0.0739406, acc 0.995\n",
      "Evaluation:2019-04-18T12:25:53.630333: step 283, loss 0.0876724, acc 0.988\n",
      "Evaluation:2019-04-18T12:26:34.439203: step 284, loss 0.0790844, acc 0.991\n",
      "Evaluation:2019-04-18T12:27:14.615764: step 285, loss 0.0578904, acc 0.997\n",
      "Evaluation:2019-04-18T12:27:54.077235: step 286, loss 0.0393936, acc 0.998\n",
      "Evaluation:2019-04-18T12:28:34.251803: step 287, loss 0.0278246, acc 0.998\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-280\n",
      "\n",
      "2019-04-18T12:28:43.558914: step 288, loss 2.76914, acc 0.0390625\n",
      "2019-04-18T12:28:52.153928: step 289, loss 2.34858, acc 0.101562\n",
      "start training model\n",
      "2019-04-18T12:29:00.184452: step 290, loss 2.20186, acc 0.15625\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:29:39.975046: step 291, loss 0.0362801, acc 0.996\n",
      "Evaluation:2019-04-18T12:30:19.812047: step 292, loss 0.0714218, acc 0.989\n",
      "Evaluation:2019-04-18T12:30:59.292470: step 293, loss 0.110607, acc 0.981\n",
      "Evaluation:2019-04-18T12:31:39.584720: step 294, loss 0.12061, acc 0.981\n",
      "Evaluation:2019-04-18T12:32:16.869015: step 295, loss 0.102427, acc 0.985\n",
      "Evaluation:2019-04-18T12:32:51.018693: step 296, loss 0.0768705, acc 0.991\n",
      "Evaluation:2019-04-18T12:33:27.707581: step 297, loss 0.056492, acc 0.994\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-290\n",
      "\n",
      "2019-04-18T12:33:37.479449: step 298, loss 3.05572, acc 0.015625\n",
      "2019-04-18T12:33:45.615690: step 299, loss 2.5086, acc 0.140625\n",
      "2019-04-18T12:33:53.856652: step 300, loss 2.16886, acc 0.195312\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:34:31.792204: step 301, loss 0.155793, acc 0.992\n",
      "Evaluation:2019-04-18T12:35:11.259661: step 302, loss 0.24217, acc 0.979\n",
      "Evaluation:2019-04-18T12:35:50.977449: step 303, loss 0.23631, acc 0.984\n",
      "Evaluation:2019-04-18T12:36:30.057939: step 304, loss 0.184491, acc 0.991\n",
      "Evaluation:2019-04-18T12:37:09.912361: step 305, loss 0.124374, acc 0.997\n",
      "Evaluation:2019-04-18T12:37:49.449631: step 306, loss 0.0788669, acc 0.998\n",
      "Evaluation:2019-04-18T12:38:29.316021: step 307, loss 0.0508707, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-300\n",
      "\n",
      "2019-04-18T12:38:38.660031: step 308, loss 2.56749, acc 0.0859375\n",
      "2019-04-18T12:38:47.016684: step 309, loss 2.2968, acc 0.164062\n",
      "2019-04-18T12:38:55.642618: step 310, loss 2.32801, acc 0.101562\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:39:36.246037: step 311, loss 0.0365653, acc 1\n",
      "Evaluation:2019-04-18T12:40:14.031988: step 312, loss 0.0518002, acc 1\n",
      "Evaluation:2019-04-18T12:40:53.430630: step 313, loss 0.0721315, acc 0.998\n",
      "Evaluation:2019-04-18T12:41:33.237179: step 314, loss 0.0770959, acc 0.998\n",
      "Evaluation:2019-04-18T12:42:12.706629: step 315, loss 0.0751551, acc 0.997\n",
      "Evaluation:2019-04-18T12:42:52.702674: step 316, loss 0.0692242, acc 0.996\n",
      "Evaluation:2019-04-18T12:43:32.065410: step 317, loss 0.0588735, acc 0.997\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-310\n",
      "\n",
      "2019-04-18T12:43:41.494195: step 318, loss 2.4447, acc 0.0703125\n",
      "2019-04-18T12:43:49.102848: step 319, loss 2.31196, acc 0.140625\n",
      "2019-04-18T12:43:57.026660: step 320, loss 2.15748, acc 0.21875\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:44:36.590857: step 321, loss 0.0368346, acc 1\n",
      "Evaluation:2019-04-18T12:45:16.928095: step 322, loss 0.0414811, acc 1\n",
      "Evaluation:2019-04-18T12:45:56.669823: step 323, loss 0.0517355, acc 1\n",
      "Evaluation:2019-04-18T12:46:36.552168: step 324, loss 0.0672978, acc 0.997\n",
      "Evaluation:2019-04-18T12:47:16.372678: step 325, loss 0.0829368, acc 0.995\n",
      "Evaluation:2019-04-18T12:47:56.407620: step 326, loss 0.0893272, acc 0.995\n",
      "Evaluation:2019-04-18T12:48:36.071550: step 327, loss 0.0818781, acc 0.995\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-320\n",
      "\n",
      "2019-04-18T12:48:45.895280: step 328, loss 2.16323, acc 0.148438\n",
      "2019-04-18T12:48:54.122290: step 329, loss 2.16657, acc 0.179688\n",
      "2019-04-18T12:49:02.231592: step 330, loss 2.04091, acc 0.234375\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:49:42.794122: step 331, loss 0.0415116, acc 1\n",
      "Evaluation:2019-04-18T12:50:22.892889: step 332, loss 0.0364585, acc 1\n",
      "Evaluation:2019-04-18T12:51:02.359347: step 333, loss 0.0320668, acc 1\n",
      "Evaluation:2019-04-18T12:51:40.992034: step 334, loss 0.0284952, acc 1\n",
      "Evaluation:2019-04-18T12:52:20.608096: step 335, loss 0.02558, acc 1\n",
      "Evaluation:2019-04-18T12:53:01.014041: step 336, loss 0.0231801, acc 1\n",
      "Evaluation:2019-04-18T12:53:40.552308: step 337, loss 0.0211828, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-330\n",
      "\n",
      "2019-04-18T12:53:50.261345: step 338, loss 2.08587, acc 0.195312\n",
      "2019-04-18T12:53:58.655897: step 339, loss 2.01297, acc 0.179688\n",
      "2019-04-18T12:54:07.216005: step 340, loss 2.17723, acc 0.203125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:54:47.641899: step 341, loss 0.0190922, acc 1\n",
      "Evaluation:2019-04-18T12:55:27.741664: step 342, loss 0.0195308, acc 1\n",
      "Evaluation:2019-04-18T12:56:08.120683: step 343, loss 0.0198246, acc 1\n",
      "Evaluation:2019-04-18T12:56:47.962138: step 344, loss 0.0199534, acc 1\n",
      "Evaluation:2019-04-18T12:57:27.817557: step 345, loss 0.0199058, acc 1\n",
      "Evaluation:2019-04-18T12:58:07.621114: step 346, loss 0.0196846, acc 1\n",
      "Evaluation:2019-04-18T12:58:48.090896: step 347, loss 0.0192954, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-340\n",
      "\n",
      "2019-04-18T12:58:57.104786: step 348, loss 2.07653, acc 0.203125\n",
      "2019-04-18T12:59:05.449471: step 349, loss 2.0747, acc 0.195312\n",
      "2019-04-18T12:59:13.055132: step 350, loss 1.9551, acc 0.257812\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T12:59:53.801168: step 351, loss 0.0205466, acc 1\n",
      "Evaluation:2019-04-18T13:00:34.001665: step 352, loss 0.0215298, acc 1\n",
      "Evaluation:2019-04-18T13:01:15.681205: step 353, loss 0.0221946, acc 1\n",
      "Evaluation:2019-04-18T13:01:55.165616: step 354, loss 0.0224633, acc 1\n",
      "Evaluation:2019-04-18T13:02:35.597495: step 355, loss 0.0223016, acc 1\n",
      "Evaluation:2019-04-18T13:03:15.870796: step 356, loss 0.0217431, acc 1\n",
      "Evaluation:2019-04-18T13:03:54.766783: step 357, loss 0.0208698, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-350\n",
      "\n",
      "2019-04-18T13:04:03.988122: step 358, loss 1.95447, acc 0.25\n",
      "2019-04-18T13:04:12.991052: step 359, loss 1.99592, acc 0.242188\n",
      "2019-04-18T13:04:21.601020: step 360, loss 2.00674, acc 0.226562\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T13:05:02.525582: step 361, loss 0.0218279, acc 1\n",
      "Evaluation:2019-04-18T13:05:41.300887: step 362, loss 0.0230064, acc 1\n",
      "Evaluation:2019-04-18T13:06:21.725784: step 363, loss 0.0236261, acc 1\n",
      "Evaluation:2019-04-18T13:07:02.821885: step 364, loss 0.0235819, acc 1\n",
      "Evaluation:2019-04-18T13:07:43.920979: step 365, loss 0.0228678, acc 1\n",
      "Evaluation:2019-04-18T13:08:24.657041: step 366, loss 0.0216258, acc 1\n",
      "Evaluation:2019-04-18T13:09:04.736860: step 367, loss 0.020045, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-360\n",
      "\n",
      "2019-04-18T13:09:13.391717: step 368, loss 1.92541, acc 0.25\n",
      "2019-04-18T13:09:21.928886: step 369, loss 1.94765, acc 0.226562\n",
      "2019-04-18T13:09:30.228691: step 370, loss 1.86158, acc 0.304688\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T13:10:10.172872: step 371, loss 0.0188634, acc 1\n",
      "Evaluation:2019-04-18T13:10:50.877023: step 372, loss 0.0196081, acc 1\n",
      "Evaluation:2019-04-18T13:11:30.807240: step 373, loss 0.0198475, acc 1\n",
      "Evaluation:2019-04-18T13:12:11.297961: step 374, loss 0.0195423, acc 1\n",
      "Evaluation:2019-04-18T13:12:51.618136: step 375, loss 0.018755, acc 1\n",
      "Evaluation:2019-04-18T13:13:31.225221: step 376, loss 0.0176177, acc 1\n",
      "Evaluation:2019-04-18T13:14:11.524451: step 377, loss 0.0162765, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-370\n",
      "\n",
      "2019-04-18T13:14:20.757760: step 378, loss 1.98902, acc 0.1875\n",
      "2019-04-18T13:14:29.002712: step 379, loss 1.89798, acc 0.257812\n",
      "2019-04-18T13:14:37.892941: step 380, loss 1.95227, acc 0.203125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T13:15:18.259617: step 381, loss 0.0133742, acc 1\n",
      "Evaluation:2019-04-18T13:15:58.832117: step 382, loss 0.0141517, acc 1\n",
      "Evaluation:2019-04-18T13:16:39.380682: step 383, loss 0.0149919, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:2019-04-18T13:17:20.914611: step 384, loss 0.0156322, acc 1\n",
      "Evaluation:2019-04-18T13:18:01.278672: step 385, loss 0.0158676, acc 1\n",
      "Evaluation:2019-04-18T13:18:42.370781: step 386, loss 0.0156801, acc 1\n",
      "Evaluation:2019-04-18T13:19:24.158035: step 387, loss 0.0151514, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-380\n",
      "\n",
      "2019-04-18T13:19:33.992736: step 388, loss 2.29336, acc 0.179688\n",
      "2019-04-18T13:19:42.191810: step 389, loss 2.06356, acc 0.195312\n",
      "2019-04-18T13:19:50.623265: step 390, loss 2.00098, acc 0.195312\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-18T13:20:32.058456: step 391, loss 0.0186418, acc 1\n",
      "Evaluation:2019-04-18T13:21:12.841396: step 392, loss 0.021822, acc 1\n",
      "Evaluation:2019-04-18T13:21:54.085102: step 393, loss 0.0244704, acc 1\n",
      "Evaluation:2019-04-18T13:22:35.104409: step 394, loss 0.0255945, acc 1\n",
      "Evaluation:2019-04-18T13:23:15.672920: step 395, loss 0.0249659, acc 1\n",
      "Evaluation:2019-04-18T13:23:56.032989: step 396, loss 0.0231145, acc 1\n",
      "Evaluation:2019-04-18T13:24:36.494787: step 397, loss 0.0207225, acc 1\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import os\n",
    "import shutil\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        acc_summary = tf.summary.scalar('accuracy', lstm.accuracy)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        shutil.rmtree('../model/Bi-LSTM-Attention/')\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM-Attention/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "#             _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "#                 [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "#                 feed_dict)\n",
    "            _, summaries,step, loss, accuracy = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.accuracy],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "#             acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "#             print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(timeStr, step, loss, accuracy))\n",
    "            trainSummaryWriter.add_summary(summaries, step)\n",
    "\n",
    "        def devStep(batchX, batchY, writer=None):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "#             summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "#                 [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "#                 feed_dict)\n",
    "            \n",
    "#             acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "#             evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "#             return loss, acc, auc, precision, recall\n",
    "            _, summaries,step, loss, accuracy = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.accuracy],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "            \n",
    "            #acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            print(\"Evaluation:{}: step {}, loss {:g}, acc {:g}\".format(timeStr, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        devStep(evalReviews, evalLabels, writer=evalSummaryWriter)\n",
    "                        \n",
    "            \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM-Attention/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
