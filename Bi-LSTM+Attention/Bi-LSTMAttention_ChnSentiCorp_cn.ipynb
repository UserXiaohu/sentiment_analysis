{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.18\n",
    "    @Purpose: 中文文本分类实战（五）—— Bi-LSTM + Attention模型\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets: ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度。\n",
    "    @Link : https://github.com/jiangxinyang227/textClassifier/tree/master/Bi-LSTM%2BAttention\n",
    "    @Reference : https://www.cnblogs.com/jiangxinyang/p/10208227.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集介绍\n",
    "\n",
    "ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度，总共有三个数据文件，在/data/ChnSentiCorp_cn_data目录下，包括ChnSentiCorp_htl_all.csv。在进行文本分类时需要有标签的数据（labeledTrainData），数据预处理如文本分类实战（一）—— word2vec预训练词向量中一样，预处理后的文件为/data/ChnSentiCorp_htl_all.csv/labeledTrain.csv。\n",
    "\n",
    "\n",
    "##  Bi-LSTM + Attention 模型介绍\n",
    "　  \n",
    "Bi-LSTM + Attention模型来源于论文Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification。关于Attention的介绍见这篇。\n",
    "\n",
    "Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类。在实验中，加上Attention确实对结果有所提升。其模型结构如下图：\n",
    "  \n",
    "![avatar](../images/BI-LSTM+Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 4\n",
    "    evaluateEvery = 10\n",
    "    checkpointEvery = 10\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5   \n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/ChnSentiCorp_cn_data/labeledCharTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/cnews_data/stopwords.txt\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据\n",
    "\n",
    "　　1）将数据加载进来，将句子分割成词表示，并去除低频词和停用词。\n",
    "\n",
    "　　2）将词映射成索引表示，构建词汇-索引映射表，并保存成json的数据格式，之后做inference时可以用到。（注意，有的词可能不在word2vec的预训练词向量中，这种词直接用UNK表示）\n",
    "\n",
    "　　3）从预训练的词向量模型中读取出词向量，作为初始化值输入到模型中。\n",
    "\n",
    "　　4）将数据集分割成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource          # 数据文件路径\n",
    "        self._stopWordSource = config.stopWordSource  # 分词文件路径\n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        '''\n",
    "        从csv文件中读取数据集\n",
    "        \n",
    "        :param filePath:   str   数据集文件路径\n",
    "        :return:\n",
    "            reviews:   list    数据集\n",
    "            labels：   list    标签集\n",
    "        '''\n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"label\"].tolist()\n",
    "        review = df[\"review_cut_word\"].tolist()\n",
    "        reviews = [str(line).strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        '''\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \n",
    "        :param review:   \n",
    "        :param sequenceLength:  \n",
    "        :return:\n",
    "            reviewVec:   list    \n",
    "        '''\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        '''\n",
    "        生成训练集和验证集\n",
    "        \n",
    "        :param x:     list     数据\n",
    "        :param y:     list     数据标签\n",
    "        :param rate:  float    分割比例\n",
    "        :return:\n",
    "            trainReviews:   list    \n",
    "            trainLabels:   list    \n",
    "            evalReviews:   list    \n",
    "            evalLabels:   list    \n",
    "        '''\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        '''\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \n",
    "        :param reviews:     list     数据\n",
    "        :return:\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        '''\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \n",
    "        :param words:     list     数据\n",
    "        :return:\n",
    "             vocab                       list            词列表\n",
    "             np.array(wordEmbedding)     numpy array     词向量\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../data/ChnSentiCorp_cn_data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        '''\n",
    "        读取停用词\n",
    "        \n",
    "        :param stopWordPath:     str     停用词文件路径\n",
    "        :return: \n",
    "            \n",
    "        '''\n",
    "\n",
    "        with open(stopWordPath, \"r\", encoding=\"utf-8\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        '''\n",
    "         初始化训练集和验证集\n",
    "        \n",
    "        :param :\n",
    "        :return: \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (4612, 200)\n",
      "train label shape: (4612, 1)\n",
      "eval data shape: (1154, 200)\n",
      "wordEmbedding data shape: (4732, 200)\n",
      "_wordToIndex data shape: 4732\n",
      "_indexToWord data shape: 4732\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"wordEmbedding data shape: {}\".format(data.wordEmbedding.shape))\n",
    "print(\"_wordToIndex data shape: {}\".format(len(data._wordToIndex)))\n",
    "print(\"_indexToWord data shape: {}\".format(len(data._indexToWord)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成batch数据集\n",
    "\n",
    "　　采用生成器的形式向模型输入batch数据集，（生成器可以避免将所有的数据加入到内存中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        '''\n",
    "         生成batch数据集，用生成器的方式输出\n",
    "        \n",
    "        :param x:      list    \n",
    "        :param y:      list    \n",
    "        :param batchSize:      int    \n",
    "        :return: \n",
    "            batchX        nnumpy array\n",
    "            batchY        nnumpy array\n",
    "            \n",
    "        '''\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM + Attention模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = 0\n",
    "    try:\n",
    "        auc = roc_auc_score(trueY, predY)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\pythonWp\\nlp\\textClassifier\\Bi-LSTM+Attention\\summarys\n",
      "\n",
      "start training model\n",
      "2019-03-19T09:06:02.549763, step: 1, loss: 0.7349516153335571, acc: 0.9219, auc: 0.4322, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:10.813674, step: 2, loss: 0.6384334564208984, acc: 0.8594, auc: 0.4697, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:19.372803, step: 3, loss: 0.5092806816101074, acc: 0.8516, auc: 0.5326, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:28.130386, step: 4, loss: 0.5109715461730957, acc: 0.8672, auc: 0.3969, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:36.651610, step: 5, loss: 0.49502885341644287, acc: 0.8281, auc: 0.5952, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:45.102021, step: 6, loss: 0.5341507196426392, acc: 0.8281, auc: 0.4198, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:06:53.638203, step: 7, loss: 0.4217533469200134, acc: 0.875, auc: 0.5173, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:07:01.683699, step: 8, loss: 0.4373316466808319, acc: 0.8672, auc: 0.4738, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:07:10.308645, step: 9, loss: 0.4483315646648407, acc: 0.8594, auc: 0.4697, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:07:19.288643, step: 10, loss: 0.3923351764678955, acc: 0.8828, auc: 0.515, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:08:39.913131, step: 10, loss: 1.8639781210157607, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-10\n",
      "\n",
      "2019-03-19T09:08:49.881485, step: 11, loss: 0.41823023557662964, acc: 0.8594, auc: 0.5045, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:08:58.579236, step: 12, loss: 0.340424507856369, acc: 0.8906, auc: 0.5952, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:09:07.070538, step: 13, loss: 0.3565092980861664, acc: 0.875, auc: 0.702, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:09:15.260646, step: 14, loss: 0.5017192363739014, acc: 0.8281, auc: 0.6261, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:09:23.773894, step: 15, loss: 0.328097403049469, acc: 0.875, auc: 0.8047, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:09:33.150825, step: 16, loss: 0.5156115889549255, acc: 0.7812, auc: 0.6396, precision: 0.5, recall: 0.0357\n",
      "2019-03-19T09:09:42.294385, step: 17, loss: 0.4720189869403839, acc: 0.8516, auc: 0.4701, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:09:51.034024, step: 18, loss: 0.3519285321235657, acc: 0.8906, auc: 0.7192, precision: 0.6667, recall: 0.1333\n",
      "2019-03-19T09:09:59.561231, step: 19, loss: 0.3253623843193054, acc: 0.8906, auc: 0.7652, precision: 1.0, recall: 0.1765\n",
      "2019-03-19T09:10:08.242026, step: 20, loss: 0.34496867656707764, acc: 0.8672, auc: 0.8103, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:11:25.387815, step: 20, loss: 2.1322775416904025, acc: 0.006933333333333333, auc: 0.0, precision: 0.4444444444444444, recall: 0.006933333333333333\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-20\n",
      "\n",
      "2019-03-19T09:11:34.973194, step: 21, loss: 0.3296734094619751, acc: 0.8828, auc: 0.7015, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:11:43.598140, step: 22, loss: 0.3776915669441223, acc: 0.8672, auc: 0.6995, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:11:52.064510, step: 23, loss: 0.40922611951828003, acc: 0.8516, auc: 0.7513, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:12:01.171167, step: 24, loss: 0.34081414341926575, acc: 0.8672, auc: 0.7954, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:12:10.318715, step: 25, loss: 0.27207136154174805, acc: 0.8984, auc: 0.8475, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:12:19.148115, step: 26, loss: 0.4638974666595459, acc: 0.7656, auc: 0.8281, precision: 1.0, recall: 0.0323\n",
      "2019-03-19T09:12:28.268737, step: 27, loss: 0.30233752727508545, acc: 0.9062, auc: 0.7481, precision: 1.0, recall: 0.1429\n",
      "2019-03-19T09:12:37.206844, step: 28, loss: 0.37200039625167847, acc: 0.8516, auc: 0.8333, precision: 1.0, recall: 0.2083\n",
      "2019-03-19T09:12:45.811843, step: 29, loss: 0.3346606492996216, acc: 0.8984, auc: 0.7924, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:12:54.442771, step: 30, loss: 0.2859823703765869, acc: 0.9062, auc: 0.911, precision: 0.6667, recall: 0.2857\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:14:11.745143, step: 30, loss: 1.2802958488464355, acc: 0.19791111111111112, auc: 0.0, precision: 1.0, recall: 0.19791111111111112\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-30\n",
      "\n",
      "2019-03-19T09:14:21.455188, step: 31, loss: 0.3542443513870239, acc: 0.8594, auc: 0.8257, precision: 1.0, recall: 0.2174\n",
      "2019-03-19T09:14:30.124015, step: 32, loss: 0.3007583022117615, acc: 0.8672, auc: 0.814, precision: 0.5, recall: 0.1176\n",
      "2019-03-19T09:14:38.953414, step: 33, loss: 0.27595385909080505, acc: 0.8828, auc: 0.8343, precision: 1.0, recall: 0.0625\n",
      "2019-03-19T09:14:47.977294, step: 34, loss: 0.32490602135658264, acc: 0.875, auc: 0.8287, precision: 0.8333, recall: 0.25\n",
      "2019-03-19T09:14:56.551375, step: 35, loss: 0.3702808618545532, acc: 0.8359, auc: 0.8616, precision: 0.8571, recall: 0.2308\n",
      "2019-03-19T09:15:05.354181, step: 36, loss: 0.23977099359035492, acc: 0.9219, auc: 0.9398, precision: 0.6429, recall: 0.6429\n",
      "start training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T09:15:14.031985, step: 37, loss: 0.2690598666667938, acc: 0.9141, auc: 0.8728, precision: 0.6471, recall: 0.6875\n",
      "2019-03-19T09:15:22.889310, step: 38, loss: 0.35539042949676514, acc: 0.8984, auc: 0.721, precision: 0.6667, recall: 0.375\n",
      "2019-03-19T09:15:31.850748, step: 39, loss: 0.19076469540596008, acc: 0.9219, auc: 0.8739, precision: 0.4, recall: 0.2222\n",
      "2019-03-19T09:15:40.564489, step: 40, loss: 0.21420636773109436, acc: 0.8906, auc: 0.9428, precision: 1.0, recall: 0.0667\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:16:59.570012, step: 40, loss: 1.9896997743182712, acc: 0.07726666666666665, auc: 0.0, precision: 1.0, recall: 0.07726666666666665\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-40\n",
      "\n",
      "2019-03-19T09:17:09.377795, step: 41, loss: 0.2961292862892151, acc: 0.8438, auc: 0.9106, precision: 0.0, recall: 0.0\n",
      "2019-03-19T09:17:18.025679, step: 42, loss: 0.24848869442939758, acc: 0.875, auc: 0.9061, precision: 1.0, recall: 0.1111\n",
      "2019-03-19T09:17:27.009030, step: 43, loss: 0.34949156641960144, acc: 0.8516, auc: 0.893, precision: 0.8571, recall: 0.25\n",
      "2019-03-19T09:17:35.607896, step: 44, loss: 0.22557057440280914, acc: 0.8984, auc: 0.9318, precision: 1.0, recall: 0.2778\n",
      "2019-03-19T09:17:44.343534, step: 45, loss: 0.25281453132629395, acc: 0.9062, auc: 0.9254, precision: 0.8571, recall: 0.5455\n",
      "2019-03-19T09:17:52.997157, step: 46, loss: 0.30017367005348206, acc: 0.8594, auc: 0.8828, precision: 0.5, recall: 0.3333\n",
      "2019-03-19T09:18:01.627329, step: 47, loss: 0.2934417128562927, acc: 0.8906, auc: 0.8601, precision: 0.6364, recall: 0.4118\n",
      "2019-03-19T09:18:10.319120, step: 48, loss: 0.2927149832248688, acc: 0.8828, auc: 0.8962, precision: 0.6429, recall: 0.4737\n",
      "2019-03-19T09:18:18.801439, step: 49, loss: 0.2186252474784851, acc: 0.9062, auc: 0.9306, precision: 0.6923, recall: 0.5294\n",
      "2019-03-19T09:18:27.614100, step: 50, loss: 0.27200061082839966, acc: 0.8672, auc: 0.9145, precision: 0.6667, recall: 0.2105\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:19:46.303408, step: 50, loss: 1.2485186788770888, acc: 0.28123333333333334, auc: 0.0, precision: 1.0, recall: 0.28123333333333334\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-50\n",
      "\n",
      "2019-03-19T09:19:55.823957, step: 51, loss: 0.24315056204795837, acc: 0.8906, auc: 0.9189, precision: 0.7273, recall: 0.4211\n",
      "2019-03-19T09:20:04.382082, step: 52, loss: 0.24267378449440002, acc: 0.8984, auc: 0.9365, precision: 0.9091, recall: 0.4545\n",
      "2019-03-19T09:20:12.888343, step: 53, loss: 0.29161399602890015, acc: 0.8672, auc: 0.8667, precision: 0.75, recall: 0.1579\n",
      "2019-03-19T09:20:21.636213, step: 54, loss: 0.2379472553730011, acc: 0.8828, auc: 0.9185, precision: 0.4, recall: 0.1429\n",
      "2019-03-19T09:20:30.452197, step: 55, loss: 0.20145952701568604, acc: 0.9219, auc: 0.8609, precision: 0.6667, recall: 0.1818\n",
      "2019-03-19T09:20:39.221755, step: 56, loss: 0.23291057348251343, acc: 0.875, auc: 0.9163, precision: 0.6, recall: 0.1765\n",
      "2019-03-19T09:20:47.828748, step: 57, loss: 0.24559205770492554, acc: 0.8828, auc: 0.9241, precision: 0.8571, recall: 0.3\n",
      "2019-03-19T09:20:56.356392, step: 58, loss: 0.3173810541629791, acc: 0.8594, auc: 0.862, precision: 1.0, recall: 0.1\n",
      "2019-03-19T09:21:05.192984, step: 59, loss: 0.3547014594078064, acc: 0.8594, auc: 0.8656, precision: 1.0, recall: 0.28\n",
      "2019-03-19T09:21:13.923647, step: 60, loss: 0.20572608709335327, acc: 0.9219, auc: 0.9465, precision: 1.0, recall: 0.4444\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:22:31.928140, step: 60, loss: 0.881682919131385, acc: 0.4956666666666667, auc: 0.0, precision: 1.0, recall: 0.4956666666666667\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-60\n",
      "\n",
      "2019-03-19T09:22:42.031134, step: 61, loss: 0.21065889298915863, acc: 0.9219, auc: 0.9245, precision: 0.7778, recall: 0.4667\n",
      "2019-03-19T09:22:50.720908, step: 62, loss: 0.22999849915504456, acc: 0.9375, auc: 0.8618, precision: 0.5556, recall: 0.5556\n",
      "2019-03-19T09:22:59.455560, step: 63, loss: 0.20809099078178406, acc: 0.9297, auc: 0.9198, precision: 0.8571, recall: 0.4286\n",
      "2019-03-19T09:23:08.323854, step: 64, loss: 0.2994005084037781, acc: 0.8828, auc: 0.9061, precision: 0.8667, recall: 0.5\n",
      "2019-03-19T09:23:16.990689, step: 65, loss: 0.2174539566040039, acc: 0.9141, auc: 0.9141, precision: 0.7778, recall: 0.4375\n",
      "2019-03-19T09:23:25.624610, step: 66, loss: 0.2477092742919922, acc: 0.875, auc: 0.937, precision: 0.8, recall: 0.3636\n",
      "2019-03-19T09:23:33.897494, step: 67, loss: 0.23584561049938202, acc: 0.9062, auc: 0.918, precision: 0.7143, recall: 0.3333\n",
      "2019-03-19T09:23:42.701961, step: 68, loss: 0.1786150336265564, acc: 0.9062, auc: 0.955, precision: 0.7778, recall: 0.4118\n",
      "2019-03-19T09:23:51.714870, step: 69, loss: 0.2097378969192505, acc: 0.9141, auc: 0.93, precision: 0.75, recall: 0.5294\n",
      "2019-03-19T09:24:00.581169, step: 70, loss: 0.21211111545562744, acc: 0.8984, auc: 0.9528, precision: 1.0, recall: 0.4091\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:25:20.340355, step: 70, loss: 0.9245819515652127, acc: 0.5459888888888889, auc: 0.0, precision: 1.0, recall: 0.5459888888888889\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-70\n",
      "\n",
      "2019-03-19T09:25:30.056383, step: 71, loss: 0.2117493450641632, acc: 0.9141, auc: 0.9016, precision: 0.6667, recall: 0.4286\n",
      "2019-03-19T09:25:38.874810, step: 72, loss: 0.1930815875530243, acc: 0.9219, auc: 0.9488, precision: 0.8462, recall: 0.5789\n",
      "start training model\n",
      "2019-03-19T09:25:47.744362, step: 73, loss: 0.13535811007022858, acc: 0.9453, auc: 0.9847, precision: 0.9333, recall: 0.7\n",
      "2019-03-19T09:25:56.573887, step: 74, loss: 0.1666654646396637, acc: 0.9297, auc: 0.9759, precision: 0.9474, recall: 0.6923\n",
      "2019-03-19T09:26:05.406340, step: 75, loss: 0.17991803586483002, acc: 0.9297, auc: 0.9551, precision: 0.7778, recall: 0.7368\n",
      "2019-03-19T09:26:14.100149, step: 76, loss: 0.19954559206962585, acc: 0.9062, auc: 0.9424, precision: 0.75, recall: 0.5\n",
      "2019-03-19T09:26:22.801890, step: 77, loss: 0.21454289555549622, acc: 0.9062, auc: 0.9353, precision: 0.6923, recall: 0.5294\n",
      "2019-03-19T09:26:32.253625, step: 78, loss: 0.17339783906936646, acc: 0.9219, auc: 0.9638, precision: 0.8462, recall: 0.5789\n",
      "2019-03-19T09:26:41.805100, step: 79, loss: 0.1989009827375412, acc: 0.9141, auc: 0.9525, precision: 1.0, recall: 0.3889\n",
      "2019-03-19T09:26:51.578969, step: 80, loss: 0.21278119087219238, acc: 0.9219, auc: 0.9551, precision: 0.7917, recall: 0.7917\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:28:13.942320, step: 80, loss: 0.7881862852308485, acc: 0.6692555555555555, auc: 0.0, precision: 1.0, recall: 0.6692555555555555\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-80\n",
      "\n",
      "2019-03-19T09:28:23.697045, step: 81, loss: 0.08995556831359863, acc: 0.9766, auc: 0.9925, precision: 1.0, recall: 0.7857\n",
      "2019-03-19T09:28:32.605719, step: 82, loss: 0.15294957160949707, acc: 0.9453, auc: 0.981, precision: 0.7826, recall: 0.9\n",
      "2019-03-19T09:28:41.329842, step: 83, loss: 0.19461765885353088, acc: 0.9453, auc: 0.9279, precision: 0.7692, recall: 0.7143\n",
      "2019-03-19T09:28:50.186170, step: 84, loss: 0.2102336585521698, acc: 0.9219, auc: 0.9338, precision: 0.8182, recall: 0.5294\n",
      "2019-03-19T09:28:58.924812, step: 85, loss: 0.28360646963119507, acc: 0.8828, auc: 0.9212, precision: 0.8667, recall: 0.5\n",
      "2019-03-19T09:29:07.708335, step: 86, loss: 0.14460986852645874, acc: 0.9453, auc: 0.974, precision: 0.7857, recall: 0.7333\n",
      "2019-03-19T09:29:16.501826, step: 87, loss: 0.18951770663261414, acc: 0.9141, auc: 0.952, precision: 0.8889, recall: 0.4444\n",
      "2019-03-19T09:29:25.295322, step: 88, loss: 0.19728656113147736, acc: 0.9141, auc: 0.9474, precision: 0.7857, recall: 0.5789\n",
      "2019-03-19T09:29:34.228567, step: 89, loss: 0.18084503710269928, acc: 0.9219, auc: 0.9345, precision: 1.0, recall: 0.3333\n",
      "2019-03-19T09:29:43.068633, step: 90, loss: 0.187540203332901, acc: 0.9219, auc: 0.9442, precision: 0.875, recall: 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:31:02.492370, step: 90, loss: 1.0979623927010431, acc: 0.4045111111111111, auc: 0.0, precision: 1.0, recall: 0.4045111111111111\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-90\n",
      "\n",
      "2019-03-19T09:31:12.643311, step: 91, loss: 0.2643435597419739, acc: 0.9297, auc: 0.8309, precision: 0.8889, recall: 0.5\n",
      "2019-03-19T09:31:21.364998, step: 92, loss: 0.149213045835495, acc: 0.9297, auc: 0.9718, precision: 0.8571, recall: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T09:31:30.346381, step: 93, loss: 0.15146566927433014, acc: 0.9375, auc: 0.9483, precision: 1.0, recall: 0.3333\n",
      "2019-03-19T09:31:39.309952, step: 94, loss: 0.1742052286863327, acc: 0.9141, auc: 0.9652, precision: 1.0, recall: 0.4211\n",
      "2019-03-19T09:31:48.046151, step: 95, loss: 0.22071437537670135, acc: 0.8672, auc: 0.9615, precision: 0.875, recall: 0.3043\n",
      "2019-03-19T09:31:56.748890, step: 96, loss: 0.11588962376117706, acc: 0.9688, auc: 0.9899, precision: 0.9333, recall: 0.8235\n",
      "2019-03-19T09:32:05.454620, step: 97, loss: 0.12433183193206787, acc: 0.9453, auc: 0.9759, precision: 0.75, recall: 0.5455\n",
      "2019-03-19T09:32:14.271054, step: 98, loss: 0.1519247591495514, acc: 0.9375, auc: 0.9793, precision: 0.6471, recall: 0.8462\n",
      "2019-03-19T09:32:23.243072, step: 99, loss: 0.18683400750160217, acc: 0.9297, auc: 0.9365, precision: 0.6667, recall: 0.6154\n",
      "2019-03-19T09:32:31.948800, step: 100, loss: 0.12584403157234192, acc: 0.9375, auc: 0.9883, precision: 0.9091, recall: 0.5882\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:33:51.179441, step: 100, loss: 1.7050836748547025, acc: 0.46441111111111116, auc: 0.0, precision: 1.0, recall: 0.46441111111111116\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-100\n",
      "\n",
      "2019-03-19T09:34:01.091947, step: 101, loss: 0.1025206595659256, acc: 0.9453, auc: 0.9826, precision: 0.875, recall: 0.5385\n",
      "2019-03-19T09:34:09.910372, step: 102, loss: 0.15162765979766846, acc: 0.9375, auc: 0.974, precision: 0.9444, recall: 0.7083\n",
      "2019-03-19T09:34:19.126737, step: 103, loss: 0.37693753838539124, acc: 0.875, auc: 0.8857, precision: 0.7778, recall: 0.5385\n",
      "2019-03-19T09:34:28.010989, step: 104, loss: 0.2353682965040207, acc: 0.9219, auc: 0.9088, precision: 0.5833, recall: 0.5833\n",
      "2019-03-19T09:34:37.018910, step: 105, loss: 0.2750450670719147, acc: 0.8828, auc: 0.9213, precision: 0.6, recall: 0.6316\n",
      "2019-03-19T09:34:45.486277, step: 106, loss: 0.1829298436641693, acc: 0.9297, auc: 0.9631, precision: 0.9333, recall: 0.6364\n",
      "2019-03-19T09:34:54.521127, step: 107, loss: 0.1881178319454193, acc: 0.9219, auc: 0.9512, precision: 0.6667, recall: 0.4615\n",
      "2019-03-19T09:35:03.279717, step: 108, loss: 0.1533200591802597, acc: 0.9375, auc: 0.9866, precision: 1.0, recall: 0.6\n",
      "start training model\n",
      "2019-03-19T09:35:12.135048, step: 109, loss: 0.21283848583698273, acc: 0.8594, auc: 0.9961, precision: 1.0, recall: 0.1818\n",
      "2019-03-19T09:35:20.904606, step: 110, loss: 0.18763229250907898, acc: 0.8984, auc: 0.9591, precision: 0.7778, recall: 0.3889\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:36:41.505979, step: 110, loss: 1.5366055700514052, acc: 0.23786666666666667, auc: 0.0, precision: 1.0, recall: 0.23786666666666667\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-110\n",
      "\n",
      "2019-03-19T09:36:51.467352, step: 111, loss: 0.12326208502054214, acc: 0.9453, auc: 0.9776, precision: 1.0, recall: 0.2222\n",
      "2019-03-19T09:37:00.613904, step: 112, loss: 0.22326433658599854, acc: 0.8828, auc: 0.9591, precision: 1.0, recall: 0.2857\n",
      "2019-03-19T09:37:09.474220, step: 113, loss: 0.16743025183677673, acc: 0.9219, auc: 0.9559, precision: 1.0, recall: 0.375\n",
      "2019-03-19T09:37:18.440255, step: 114, loss: 0.16510917246341705, acc: 0.9453, auc: 0.9611, precision: 0.9231, recall: 0.6667\n",
      "2019-03-19T09:37:27.430224, step: 115, loss: 0.1537056267261505, acc: 0.9609, auc: 0.976, precision: 0.8667, recall: 0.8125\n",
      "2019-03-19T09:37:36.587746, step: 116, loss: 0.20315901935100555, acc: 0.9219, auc: 0.9565, precision: 0.6, recall: 0.6923\n",
      "2019-03-19T09:37:45.857968, step: 117, loss: 0.14144062995910645, acc: 0.9375, auc: 0.9924, precision: 0.7273, recall: 0.8889\n",
      "2019-03-19T09:37:54.521809, step: 118, loss: 0.15194807946681976, acc: 0.9453, auc: 0.9628, precision: 0.9, recall: 0.6\n",
      "2019-03-19T09:38:03.458921, step: 119, loss: 0.14212468266487122, acc: 0.9531, auc: 0.9525, precision: 0.8333, recall: 0.5\n",
      "2019-03-19T09:38:12.364116, step: 120, loss: 0.2642078101634979, acc: 0.8672, auc: 0.9702, precision: 1.0, recall: 0.3462\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:39:31.437752, step: 120, loss: 1.7036155594719782, acc: 0.3533, auc: 0.0, precision: 1.0, recall: 0.3533\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-120\n",
      "\n",
      "2019-03-19T09:39:41.718272, step: 121, loss: 0.10072440654039383, acc: 0.9219, auc: 0.9994, precision: 1.0, recall: 0.375\n",
      "2019-03-19T09:39:50.516753, step: 122, loss: 0.1207963302731514, acc: 0.9688, auc: 0.9763, precision: 0.9412, recall: 0.8421\n",
      "2019-03-19T09:39:59.274345, step: 123, loss: 0.15624094009399414, acc: 0.9453, auc: 0.9667, precision: 0.8667, recall: 0.7222\n",
      "2019-03-19T09:40:08.580468, step: 124, loss: 0.15566664934158325, acc: 0.9609, auc: 0.9551, precision: 0.8824, recall: 0.8333\n",
      "2019-03-19T09:40:17.220375, step: 125, loss: 0.246611550450325, acc: 0.9297, auc: 0.9359, precision: 0.8947, recall: 0.7083\n",
      "2019-03-19T09:40:26.115598, step: 126, loss: 0.11932912468910217, acc: 0.9531, auc: 0.9942, precision: 0.8261, recall: 0.9048\n",
      "2019-03-19T09:40:34.739546, step: 127, loss: 0.13869819045066833, acc: 0.9375, auc: 0.9825, precision: 0.9048, recall: 0.76\n",
      "2019-03-19T09:40:43.512096, step: 128, loss: 0.1813245713710785, acc: 0.9375, auc: 0.9666, precision: 0.913, recall: 0.7778\n",
      "2019-03-19T09:40:52.626734, step: 129, loss: 0.15908238291740417, acc: 0.9219, auc: 0.9643, precision: 0.75, recall: 0.5625\n",
      "2019-03-19T09:41:01.569827, step: 130, loss: 0.0731944665312767, acc: 0.9609, auc: 0.9963, precision: 1.0, recall: 0.7059\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:42:21.924040, step: 130, loss: 1.665114786889818, acc: 0.4435777777777778, auc: 0.0, precision: 1.0, recall: 0.4435777777777778\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-130\n",
      "\n",
      "2019-03-19T09:42:31.648049, step: 131, loss: 0.09332704544067383, acc: 0.9688, auc: 0.99, precision: 1.0, recall: 0.7333\n",
      "2019-03-19T09:42:40.503378, step: 132, loss: 0.10401695221662521, acc: 0.9688, auc: 0.982, precision: 0.9333, recall: 0.8235\n",
      "2019-03-19T09:42:49.664889, step: 133, loss: 0.03447084128856659, acc: 0.9922, auc: 0.9981, precision: 1.0, recall: 0.8889\n",
      "2019-03-19T09:42:58.356657, step: 134, loss: 0.1388995349407196, acc: 0.9375, auc: 0.9794, precision: 1.0, recall: 0.5\n",
      "2019-03-19T09:43:07.198032, step: 135, loss: 0.17160965502262115, acc: 0.9375, auc: 0.9409, precision: 0.7778, recall: 0.7778\n",
      "2019-03-19T09:43:16.043379, step: 136, loss: 0.11568447947502136, acc: 0.9375, auc: 0.9884, precision: 0.75, recall: 0.9\n",
      "2019-03-19T09:43:25.041330, step: 137, loss: 0.09167751669883728, acc: 0.9688, auc: 0.9915, precision: 1.0, recall: 0.8095\n",
      "2019-03-19T09:43:34.477107, step: 138, loss: 0.12957991659641266, acc: 0.9688, auc: 0.9755, precision: 0.9048, recall: 0.9048\n",
      "2019-03-19T09:43:44.041541, step: 139, loss: 0.09405717253684998, acc: 0.9844, auc: 0.9967, precision: 0.8667, recall: 1.0\n",
      "2019-03-19T09:43:56.258883, step: 140, loss: 0.12155631929636002, acc: 0.9531, auc: 0.9684, precision: 0.875, recall: 0.5833\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T09:45:24.107032, step: 140, loss: 1.5429297420713637, acc: 0.5060777777777778, auc: 0.0, precision: 1.0, recall: 0.5060777777777778\n",
      "Saved model checkpoint to ../model/Bi-LSTM-Attention/model/my-model-140\n",
      "\n",
      "2019-03-19T09:45:34.734624, step: 141, loss: 0.1477259397506714, acc: 0.9375, auc: 0.9692, precision: 0.8571, recall: 0.6667\n",
      "2019-03-19T09:45:43.768476, step: 142, loss: 0.06900741159915924, acc: 0.9531, auc: 0.9988, precision: 1.0, recall: 0.6\n",
      "2019-03-19T09:45:53.070611, step: 143, loss: 0.10628021508455276, acc: 0.9531, auc: 0.9903, precision: 0.9333, recall: 0.7368\n",
      "2019-03-19T09:46:02.200207, step: 144, loss: 0.22749896347522736, acc: 0.9141, auc: 0.9494, precision: 0.7895, recall: 0.6818\n",
      "WARNING:tensorflow:From <ipython-input-22-3a10478a55e0>:138: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../model/Bi-LSTM-Attention/savedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM-Attention/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM-Attention/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
