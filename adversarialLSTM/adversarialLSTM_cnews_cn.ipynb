{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.19\n",
    "    @Purpose: 中文文本分类实战（七）—— Adversarial LSTM模型\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets:cnews 中文文本分类数据集。\n",
    "    @Link : https://github.com/jiangxinyang227/textClassifier/tree/master/RCNN\n",
    "    @Reference : https://www.cnblogs.com/jiangxinyang/p/10208290.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "cnews 中文文本分类数据集，其目标是判断一段话的类别，数据预处理如文本分类实战（一）—— word2vec预训练词向量中一样，预处理后的文件为../data/cnews_data/labeledCharTrain.csv。\n",
    "\n",
    "\n",
    "## Adversarial LSTM模型结构\n",
    "\n",
    "Adversarial LSTM 模型来源于论文Adversarial Training Methods For Semi-Supervised Text Classification {https://arxiv.org/abs/1605.07725}。其模型结构如下右图所示：\n",
    "\n",
    "![avatar](../images/Adversarial_LSTM.png)\n",
    "\n",
    "　上图中左边为正常的LSTM结构，右图为Adversarial LSTM结构，可以看出在输出时加上了噪声。\n",
    "\n",
    "　　Adversarial LSTM的<span class=\"burk\">核心思想是</span>通过<span class=\"girk\">对word Embedding上添加噪音生成对抗样本，将对抗样本以和原始样本 同样的形式喂给模型，得到一个Adversarial Loss，通过和原始样本的loss相加得到新的损失，通过优化该新 的损失来训练模型</span>，作者认为这种方法能对word embedding加上正则化，避免过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import threading\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 2\n",
    "    evaluateEvery = 10\n",
    "    checkpointEvery = 10\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = 128  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    epsilon = 5\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/cnews_data/labeledCharTrain.csv\"                               # update\n",
    "    \n",
    "    stopWordSource = \"../data/cnews_data/stopwords.txt\"\n",
    "    \n",
    "    numClasses = 10                                                                     # update\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据\n",
    "\n",
    "　　1）将数据加载进来，将句子分割成词表示，并去除低频词和停用词。\n",
    "\n",
    "　　2）将词映射成索引表示，构建词汇-索引映射表，并保存成json的数据格式，之后做inference时可以用到。（注意，有的词可能不在word2vec的预训练词向量中，这种词直接用UNK表示）\n",
    "\n",
    "　　3）从预训练的词向量模型中读取出词向量，作为初始化值输入到模型中。\n",
    "\n",
    "　　4）将数据集分割成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.trainReviews[0:2]:[[   1    1    1    1    1    1    1    1    1    1    1    1 1131    1\n",
      "     1    1    1  478  142    1   27    2    1  483    1    1    1    1\n",
      "     1    1 1281    1    1    1    1    1    1  886    1    1    1    1\n",
      "     1    1    1    1    1    1 1005    1    1    1   13    1    1    1\n",
      "     1    1    1    1    1    1    1  203  215    1    1    1    1    1\n",
      "     1    1   27    1    1    1    1    1  102    1  152    1    1    1\n",
      "     1  451    1    1  753    1    1  226    1    1   65    1    1    1\n",
      "     1    1    1   19    1    1    1    1    1  849    1    1 2243    1\n",
      "     1    1    1    1    1    1    1    1  878    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1  419    1    1\n",
      "   196    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1   10    1    1    8    1    1 1654    1    1    1    1    1\n",
      "     1    1    1   36    1    1    1 1151    1    1    1    1    1    1\n",
      "     1    1  120    1    1 1162    1    1    1    1    1    1    1    1\n",
      "     1 1077 1901    1]\n",
      " [   1    1    1    1    1    1    1    1  518   14    1    1    1    1\n",
      "     1    1    1    1    1   84   44    1  375    1    1    1    1    1\n",
      "     1    1    1    1    1  363    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1   27    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1   19    1    1    1    1    1    1  263    1    1    1\n",
      "   264  609    1    1 1121    1    1   86   14   86  920    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1 1122    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1   27    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1 1163    1    1\n",
      "   419    1    1    1    1    1    1    1    1    1    1 1070    1    1\n",
      "     1    1    1    1    1    1  716    1    1    1    1    1    1   27\n",
      "     2    1    1    1    1    1    1  503    1    1    1    1   27    1\n",
      "   811    1   14  364]\n",
      " [ 388    1   51    1    1    1    1    1    1  501    1    1    1    1\n",
      "     1    1    1    1    1    4  241    1  388    1    1    1  241    6\n",
      "   397    1  294    1    1  439    1    1    1    1    1    1    1    1\n",
      "   104    1    3  379    1    1    1    1   51    1    1    1    1    1\n",
      "     3    1    1    1    1    1    1    1    1  190  198    1    1    1\n",
      "     1  341  198    1  152    1    1    1    1    1    1    1    1    1\n",
      "    74  237  198    1    1    1    1    1  909    1    1    1  223  278\n",
      "   198    1    1    1    1    1    1    1    1    1    1    1  588    1\n",
      "     1    1    1   65    1    1    1    1    1    1  104    1    1    1\n",
      "     1  379    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1 1616    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1   11    1    1    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "self.trainLabels[0:2]:[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "self.evalReviews[0:2]:[[   1   73    1  516    1    1    1   16  516    1    1    1    1    1\n",
      "     1    1 1352    1    1    1    1    1  967    1  966    1   73  889\n",
      "   286    1 1723    1    1    1 2621    1  516    1    1    1    1    1\n",
      "     1    1    1    1   43    1    1  919 2538    1    1    1    1    1\n",
      "   596    1    1    1    1    1    1    1  949    1    1    1    1    1\n",
      "     1    1    1    1  190    1  994    1    1  369    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1 1034    1    1    1    1    1\n",
      "     1    1   60    1    1    1  534    1  516    1    1    1    1    1\n",
      "     1    1   47    1    1    1    1    1    1    1    1  873    1    1\n",
      "     1    1    1   47  516    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1   43  207    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1   31    1    1    1    1    1    1\n",
      "     1    1    1    1    1 1060    1  101    1    1    1    1    1    1\n",
      "   687    1   40    1    1  516    1    1    1    1    1    1    1   47\n",
      "     1    1    1    1]\n",
      " [   1    1    1    1 1225  516    1  313    1  209    1   16    1    1\n",
      "     1  516  602    1    1   62    1    1    1    1   24    1   52    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1   52\n",
      "     1    1    1   24    1    1  234  654    1    1    1 1723    1  339\n",
      "   289    1    1    1   32    1  874    1    1    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1    1    1 1202    1    1    1   11   35    1    1    1    1    1\n",
      "     1    1    1    1  100    1    1 1457    1    1    1    1    1    1\n",
      "     1    1    1    1  184    1    1   71    1    1   52    1  208  177\n",
      "     1    1    1    1    1    1    1    1    1    1    1   52  208    1\n",
      "     1   78    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1  126    1    1   34\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1   16    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1   78    1    1    1    1    1    1    1    2\n",
      "     1    1  244    1    1    1    1    1   41    1    1    1    1    1\n",
      "     1   35    1    1    1    1    1    1  339    1    1    1   32    1\n",
      "   652  243  546   35  118    1    1    1    1   35    1    1  145    1\n",
      "     1    1    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "self.evalLabels[0:2]:[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 整块 update\n",
    "import sys\n",
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self.numClasses = config.numClasses        # update\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.indexFreqs = []  # 统计词空间中的词在出现在多少个review中\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        self.label_to_index = {}\n",
    "        self.index_to_label = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"class\"].tolist()                # update\n",
    "        review = df[\"review_cut_word\"].tolist()\n",
    "        reviews = [str(line).strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _labelProcess(self, y, label_to_index):\n",
    "        \"\"\"\n",
    "        将数据集中的每个label用index表示\n",
    "        label_to_index“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        input_y = []\n",
    "        for item in y:\n",
    "            temp = [0] * self.numClasses\n",
    "            temp[label_to_index[item]] = 1\n",
    "            input_y.append(temp)\n",
    "        \n",
    "        return input_y\n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        input_y = self._labelProcess(y,self.label_to_index)\n",
    "#         print(\"input_y:{0}\".format(input_y[0:2]))\n",
    "        \n",
    "#         sys.exit(0)\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append(input_y[i])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews,labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 得到逆词频\n",
    "        self._getWordIndexFreq(vocab, reviews)\n",
    "\n",
    "        labels = set(labels)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            self.label_to_index[label] = i\n",
    "            self.index_to_label[int(i)] = label\n",
    "        \n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex_cnews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord_cnews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "        with open('../data/wordJson/label_to_index_cnews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.label_to_index, f, ensure_ascii=False)\n",
    "\n",
    "        with open('../data/wordJson/index_to_label_cnews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.index_to_label, f, ensure_ascii=False)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../data/ChnSentiCorp_cn_data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                #print(word + \"不存在于词向量中\")\n",
    "                pass\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _getWordIndexFreq(self, vocab, reviews):\n",
    "        \"\"\"\n",
    "        统计词汇空间中各个词出现在多少个文本中\n",
    "        \"\"\"\n",
    "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for review in reviewDicts:\n",
    "                if word in review:\n",
    "                    count += 1\n",
    "            indexFreqs[self._wordToIndex[word]] = count\n",
    "        \n",
    "        self.indexFreqs = indexFreqs\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\", encoding=\"utf-8\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews,labels)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        print(\"self.trainReviews[0:2]:{0}\".format(self.trainReviews[0:3]))\n",
    "        print(\"self.trainLabels[0:2]:{0}\".format(self.trainLabels[0:3]))\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        print(\"self.evalReviews[0:2]:{0}\".format(self.evalReviews[0:3]))\n",
    "        print(\"self.evalLabels[0:2]:{0}\".format(self.evalLabels[0:3]))\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (3999, 200)\n",
      "train label shape: (3999, 10)\n",
      "eval data shape: (1000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3302, 200)\n"
     ]
    }
   ],
   "source": [
    "print(data.wordEmbedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成batch数据集\n",
    "\n",
    "　　采用生成器的形式向模型输入batch数据集，（生成器可以避免将所有的数据加入到内存中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class AdversarialLSTM(object):\n",
    "    \"\"\"\n",
    "    Adversarial LSTM模型 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.numClasses = config.numClasses      # update\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, self.numClasses], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.config = config\n",
    "        \n",
    "        # 根据词的频率计算权重\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
    "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用词频计算新的词嵌入矩阵\n",
    "            normWordEmbedding = self._normalize(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), weights)\n",
    "            \n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "            \n",
    "         # 计算二元交叉熵损失 \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.scores,self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                #self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.inputY)\n",
    "                loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturScores,perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturScores, labels=self.inputY)\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "                \n",
    "        self.loss = loss + perturLoss\n",
    "        \n",
    "        # accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.inputY, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "            \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        \"\"\"\n",
    "        Bi-LSTM + Attention 的模型结构\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        \n",
    "        # 定义双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "           \n",
    "            # 定义前向LSTM结构\n",
    "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "            # 定义反向LSTM结构\n",
    "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                          self.embeddedWords, dtype=tf.float32,\n",
    "                                                                          scope=\"bi-lstm\")\n",
    "\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, self.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[self.numClasses]), name=\"outputB\")\n",
    "            self.scores = tf.nn.xw_plus_b(output, outputW, outputB, name='scores')\n",
    "            self.predictions = tf.argmax(self.scores, 1, name='predictions')\n",
    "            \n",
    "        return self.scores,self.predictions\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        \"\"\"\n",
    "        对word embedding 结合权重做标准化处理\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "        print(mean)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "        print(var)\n",
    "        stddev = tf.sqrt(1e-6 + var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    def _addPerturbation(self, embedded, loss):\n",
    "        \"\"\"\n",
    "        添加波动到word embedding\n",
    "        \"\"\"\n",
    "        grad, = tf.gradients(\n",
    "            loss,\n",
    "            embedded,\n",
    "            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
    "        return embedded + perturb\n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        # shape(x) = (batch, num_timesteps, d)\n",
    "        # Divide x by max(abs(x)) for a numerically stable L2 norm.\n",
    "        # 2norm(x) = a * 2norm(x/a)\n",
    "        # Scale over the full sequence, dims (1, 2)\n",
    "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(\n",
    "            tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        return norm_length * x_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = 0\n",
    "    try:\n",
    "        auc = roc_auc_score(trueY, predY)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/MatMul:0\", shape=(1, 200), dtype=float32)\n",
      "Tensor(\"embedding/MatMul_1:0\", shape=(1, 200), dtype=float32)\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/hist is illegal; using Bi-LSTM/outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/sparsity is illegal; using Bi-LSTM/outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using loss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using loss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\pythonWp\\nlp\\textClassifier\\adversarialLSTM\\summarys\n",
      "\n",
      "start training model\n",
      "2019-04-19T10:36:29.320433: step 1, loss 5.1262, acc 0.101562\n",
      "2019-04-19T10:36:34.478499: step 2, loss 4.86705, acc 0.1875\n",
      "2019-04-19T10:36:39.557540: step 3, loss 4.48235, acc 0.210938\n",
      "2019-04-19T10:36:44.855476: step 4, loss 4.64573, acc 0.140625\n",
      "2019-04-19T10:36:50.074421: step 5, loss 4.36489, acc 0.15625\n",
      "2019-04-19T10:36:55.303876: step 6, loss 4.35404, acc 0.117188\n",
      "2019-04-19T10:37:00.634774: step 7, loss 4.41535, acc 0.101562\n",
      "2019-04-19T10:37:05.420617: step 8, loss 4.41234, acc 0.164062\n",
      "2019-04-19T10:37:10.569616: step 9, loss 4.23088, acc 0.195312\n",
      "2019-04-19T10:37:15.646787: step 10, loss 4.18644, acc 0.195312\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:37:32.488054: step 11, loss 9.40917, acc 0\n",
      "Evaluation:2019-04-19T10:37:49.082372: step 12, loss 8.47762, acc 0\n",
      "Evaluation:2019-04-19T10:38:06.637691: step 13, loss 7.28607, acc 0\n",
      "Evaluation:2019-04-19T10:38:24.175559: step 14, loss 6.34548, acc 0\n",
      "Evaluation:2019-04-19T10:38:41.357284: step 15, loss 5.38009, acc 0.051\n",
      "Evaluation:2019-04-19T10:38:58.269826: step 16, loss 4.60738, acc 0.158\n",
      "Evaluation:2019-04-19T10:39:15.845823: step 17, loss 3.93397, acc 0.168\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-10\n",
      "\n",
      "2019-04-19T10:39:21.954487: step 18, loss 5.08353, acc 0.117188\n",
      "2019-04-19T10:39:27.146603: step 19, loss 5.33864, acc 0.09375\n",
      "2019-04-19T10:39:32.265913: step 20, loss 5.55527, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:39:49.489854: step 21, loss 2.75425, acc 0.737\n",
      "Evaluation:2019-04-19T10:40:06.953154: step 22, loss 2.60482, acc 0.747\n",
      "Evaluation:2019-04-19T10:40:24.209008: step 23, loss 2.46604, acc 0.581\n",
      "Evaluation:2019-04-19T10:40:42.075230: step 24, loss 2.28261, acc 0.793\n",
      "Evaluation:2019-04-19T10:40:59.933472: step 25, loss 2.10878, acc 0.799\n",
      "Evaluation:2019-04-19T10:41:17.907407: step 26, loss 1.8627, acc 0.805\n",
      "Evaluation:2019-04-19T10:41:36.055874: step 27, loss 1.72604, acc 0.831\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-20\n",
      "\n",
      "2019-04-19T10:41:42.291198: step 28, loss 6.75834, acc 0\n",
      "2019-04-19T10:41:47.307783: step 29, loss 7.09996, acc 0\n",
      "2019-04-19T10:41:52.330353: step 30, loss 6.74193, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:42:10.655349: step 31, loss 1.58148, acc 0.839\n",
      "Evaluation:2019-04-19T10:42:29.130940: step 32, loss 1.57681, acc 0.861\n",
      "Evaluation:2019-04-19T10:42:48.057328: step 33, loss 1.55304, acc 0.891\n",
      "Evaluation:2019-04-19T10:43:06.739368: step 34, loss 1.51837, acc 0.923\n",
      "Evaluation:2019-04-19T10:43:25.521143: step 35, loss 1.47336, acc 0.908\n",
      "Evaluation:2019-04-19T10:43:44.204182: step 36, loss 1.4209, acc 0.891\n",
      "Evaluation:2019-04-19T10:44:03.017870: step 37, loss 1.36336, acc 0.883\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-30\n",
      "\n",
      "2019-04-19T10:44:09.526463: step 38, loss 7.05825, acc 0\n",
      "2019-04-19T10:44:15.030743: step 39, loss 6.9128, acc 0\n",
      "2019-04-19T10:44:20.390412: step 40, loss 6.70573, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:44:39.648914: step 41, loss 1.2707, acc 0.881\n",
      "Evaluation:2019-04-19T10:44:58.979218: step 42, loss 1.27709, acc 0.904\n",
      "Evaluation:2019-04-19T10:45:18.779633: step 43, loss 1.26011, acc 0.91\n",
      "Evaluation:2019-04-19T10:45:38.167787: step 44, loss 1.23555, acc 0.916\n",
      "Evaluation:2019-04-19T10:45:58.130404: step 45, loss 1.19877, acc 0.923\n",
      "Evaluation:2019-04-19T10:46:18.019215: step 46, loss 1.15202, acc 0.931\n",
      "Evaluation:2019-04-19T10:46:37.852177: step 47, loss 1.09816, acc 0.937\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-40\n",
      "\n",
      "2019-04-19T10:46:43.840166: step 48, loss 6.40695, acc 0\n",
      "2019-04-19T10:46:49.519975: step 49, loss 6.57185, acc 0\n",
      "2019-04-19T10:46:54.854721: step 50, loss 6.32405, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:47:15.271114: step 51, loss 1.10889, acc 0.936\n",
      "Evaluation:2019-04-19T10:47:34.248364: step 52, loss 1.12466, acc 0.937\n",
      "Evaluation:2019-04-19T10:47:53.607594: step 53, loss 1.1162, acc 0.94\n",
      "Evaluation:2019-04-19T10:48:14.533633: step 54, loss 1.08604, acc 0.941\n",
      "Evaluation:2019-04-19T10:48:34.548110: step 55, loss 1.03426, acc 0.939\n",
      "Evaluation:2019-04-19T10:48:54.822892: step 56, loss 0.971409, acc 0.939\n",
      "Evaluation:2019-04-19T10:49:15.486632: step 57, loss 0.916089, acc 0.936\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-50\n",
      "\n",
      "2019-04-19T10:49:21.806731: step 58, loss 5.95787, acc 0.0078125\n",
      "2019-04-19T10:49:27.060685: step 59, loss 5.95885, acc 0\n",
      "2019-04-19T10:49:32.705586: step 60, loss 5.94396, acc 0\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:49:53.102045: step 61, loss 0.86302, acc 0.95\n",
      "Evaluation:2019-04-19T10:50:13.664055: step 62, loss 0.877468, acc 0.949\n",
      "Evaluation:2019-04-19T10:50:33.291568: step 63, loss 0.878234, acc 0.951\n",
      "Evaluation:2019-04-19T10:50:54.012158: step 64, loss 0.863304, acc 0.955\n",
      "Evaluation:2019-04-19T10:51:14.783609: step 65, loss 0.833092, acc 0.955\n",
      "Evaluation:2019-04-19T10:51:35.940033: step 66, loss 0.791049, acc 0.959\n",
      "Evaluation:2019-04-19T10:51:56.587819: step 67, loss 0.741859, acc 0.963\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-60\n",
      "\n",
      "2019-04-19T10:52:03.067489: step 68, loss 5.8362, acc 0.0234375\n",
      "2019-04-19T10:52:08.581743: step 69, loss 5.6461, acc 0.03125\n",
      "2019-04-19T10:52:13.635228: step 70, loss 5.40282, acc 0.0234375\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:52:34.141392: step 71, loss 0.693017, acc 0.967\n",
      "Evaluation:2019-04-19T10:52:55.939103: step 72, loss 0.705342, acc 0.972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:2019-04-19T10:53:17.617130: step 73, loss 0.705468, acc 0.972\n",
      "Evaluation:2019-04-19T10:53:38.918165: step 74, loss 0.693587, acc 0.973\n",
      "Evaluation:2019-04-19T10:53:59.451257: step 75, loss 0.671539, acc 0.972\n",
      "Evaluation:2019-04-19T10:54:21.021573: step 76, loss 0.642029, acc 0.973\n",
      "Evaluation:2019-04-19T10:54:42.162039: step 77, loss 0.607703, acc 0.973\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-70\n",
      "\n",
      "2019-04-19T10:54:48.847162: step 78, loss 5.50112, acc 0.0234375\n",
      "2019-04-19T10:54:54.489074: step 79, loss 5.21653, acc 0.0390625\n",
      "2019-04-19T10:54:59.773943: step 80, loss 5.15896, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:55:20.840606: step 81, loss 0.598361, acc 0.979\n",
      "Evaluation:2019-04-19T10:55:42.222428: step 82, loss 0.621716, acc 0.979\n",
      "Evaluation:2019-04-19T10:56:04.181705: step 83, loss 0.632874, acc 0.978\n",
      "Evaluation:2019-04-19T10:56:26.229743: step 84, loss 0.62964, acc 0.978\n",
      "Evaluation:2019-04-19T10:56:48.141147: step 85, loss 0.612801, acc 0.978\n",
      "Evaluation:2019-04-19T10:57:09.738392: step 86, loss 0.584784, acc 0.978\n",
      "Evaluation:2019-04-19T10:57:32.164423: step 87, loss 0.549746, acc 0.98\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-80\n",
      "\n",
      "start training model\n",
      "2019-04-19T10:57:38.539373: step 88, loss 5.07057, acc 0.0703125\n",
      "2019-04-19T10:57:44.313932: step 89, loss 4.97152, acc 0.0625\n",
      "2019-04-19T10:57:49.716485: step 90, loss 5.14585, acc 0.0703125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T10:58:12.231274: step 91, loss 0.543061, acc 0.976\n",
      "Evaluation:2019-04-19T10:58:34.409963: step 92, loss 0.566074, acc 0.976\n",
      "Evaluation:2019-04-19T10:58:55.951358: step 93, loss 0.575483, acc 0.976\n",
      "Evaluation:2019-04-19T10:59:18.444207: step 94, loss 0.570784, acc 0.976\n",
      "Evaluation:2019-04-19T10:59:41.658130: step 95, loss 0.554193, acc 0.977\n",
      "Evaluation:2019-04-19T11:00:04.694793: step 96, loss 0.529626, acc 0.98\n",
      "Evaluation:2019-04-19T11:00:27.586577: step 97, loss 0.50106, acc 0.979\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-90\n",
      "\n",
      "2019-04-19T11:00:33.770040: step 98, loss 4.54196, acc 0.140625\n",
      "2019-04-19T11:00:39.509693: step 99, loss 4.79702, acc 0.132812\n",
      "2019-04-19T11:00:45.479726: step 100, loss 4.71085, acc 0.132812\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:01:07.928693: step 101, loss 0.495415, acc 0.98\n",
      "Evaluation:2019-04-19T11:01:30.876328: step 102, loss 0.518606, acc 0.978\n",
      "Evaluation:2019-04-19T11:01:53.571637: step 103, loss 0.530409, acc 0.978\n",
      "Evaluation:2019-04-19T11:02:13.093429: step 104, loss 0.525069, acc 0.98\n",
      "Evaluation:2019-04-19T11:02:32.580319: step 105, loss 0.506033, acc 0.98\n",
      "Evaluation:2019-04-19T11:02:55.906942: step 106, loss 0.479587, acc 0.981\n",
      "Evaluation:2019-04-19T11:03:18.780770: step 107, loss 0.451488, acc 0.979\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-100\n",
      "\n",
      "2019-04-19T11:03:25.326268: step 108, loss 4.46246, acc 0.1875\n",
      "2019-04-19T11:03:31.066916: step 109, loss 4.46663, acc 0.132812\n",
      "2019-04-19T11:03:36.840489: step 110, loss 4.52147, acc 0.164062\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:03:59.822020: step 111, loss 0.444367, acc 0.981\n",
      "Evaluation:2019-04-19T11:04:22.993055: step 112, loss 0.465625, acc 0.983\n",
      "Evaluation:2019-04-19T11:04:46.077322: step 113, loss 0.476942, acc 0.982\n",
      "Evaluation:2019-04-19T11:05:09.634325: step 114, loss 0.475516, acc 0.981\n",
      "Evaluation:2019-04-19T11:05:31.777111: step 115, loss 0.462506, acc 0.982\n",
      "Evaluation:2019-04-19T11:05:51.672907: step 116, loss 0.441388, acc 0.982\n",
      "Evaluation:2019-04-19T11:06:11.417108: step 117, loss 0.415547, acc 0.984\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-110\n",
      "\n",
      "2019-04-19T11:06:17.990528: step 118, loss 3.98099, acc 0.273438\n",
      "2019-04-19T11:06:23.774063: step 119, loss 4.21764, acc 0.164062\n",
      "2019-04-19T11:06:29.435925: step 120, loss 3.95772, acc 0.257812\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:06:53.469650: step 121, loss 0.401619, acc 0.983\n",
      "Evaluation:2019-04-19T11:07:17.155310: step 122, loss 0.4132, acc 0.983\n",
      "Evaluation:2019-04-19T11:07:40.863423: step 123, loss 0.415509, acc 0.982\n",
      "Evaluation:2019-04-19T11:08:04.382529: step 124, loss 0.408227, acc 0.984\n",
      "Evaluation:2019-04-19T11:08:28.131019: step 125, loss 0.394448, acc 0.984\n",
      "Evaluation:2019-04-19T11:08:51.554384: step 126, loss 0.373804, acc 0.986\n",
      "Evaluation:2019-04-19T11:09:13.697166: step 127, loss 0.348486, acc 0.986\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-120\n",
      "\n",
      "2019-04-19T11:09:19.969394: step 128, loss 4.20045, acc 0.210938\n",
      "2019-04-19T11:09:25.996278: step 129, loss 4.00858, acc 0.257812\n",
      "2019-04-19T11:09:31.614252: step 130, loss 4.12844, acc 0.28125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:09:55.714808: step 131, loss 0.357266, acc 0.986\n",
      "Evaluation:2019-04-19T11:10:18.974602: step 132, loss 0.374495, acc 0.986\n",
      "Evaluation:2019-04-19T11:10:42.193510: step 133, loss 0.379366, acc 0.984\n",
      "Evaluation:2019-04-19T11:11:05.867204: step 134, loss 0.378117, acc 0.983\n",
      "Evaluation:2019-04-19T11:11:29.744351: step 135, loss 0.371538, acc 0.982\n",
      "Evaluation:2019-04-19T11:11:53.077951: step 136, loss 0.357474, acc 0.982\n",
      "Evaluation:2019-04-19T11:12:16.217073: step 137, loss 0.335657, acc 0.982\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-130\n",
      "\n",
      "2019-04-19T11:12:22.652862: step 138, loss 3.7759, acc 0.28125\n",
      "2019-04-19T11:12:28.327688: step 139, loss 3.85303, acc 0.28125\n",
      "2019-04-19T11:12:34.466272: step 140, loss 3.83553, acc 0.304688\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:12:58.550864: step 141, loss 0.344832, acc 0.983\n",
      "Evaluation:2019-04-19T11:13:22.754140: step 142, loss 0.367781, acc 0.984\n",
      "Evaluation:2019-04-19T11:13:46.719055: step 143, loss 0.366985, acc 0.985\n",
      "Evaluation:2019-04-19T11:14:11.164681: step 144, loss 0.351065, acc 0.987\n",
      "Evaluation:2019-04-19T11:14:35.281188: step 145, loss 0.33379, acc 0.985\n",
      "Evaluation:2019-04-19T11:14:59.762721: step 146, loss 0.318512, acc 0.983\n",
      "Evaluation:2019-04-19T11:15:24.467905: step 147, loss 0.301531, acc 0.981\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-140\n",
      "\n",
      "2019-04-19T11:15:30.996446: step 148, loss 3.82328, acc 0.335938\n",
      "2019-04-19T11:15:36.634370: step 149, loss 4.04569, acc 0.289062\n",
      "2019-04-19T11:15:42.422890: step 150, loss 3.52253, acc 0.328125\n",
      "\n",
      "Evaluation:\n",
      "Evaluation:2019-04-19T11:16:06.548372: step 151, loss 0.330552, acc 0.986\n",
      "Evaluation:2019-04-19T11:16:30.881302: step 152, loss 0.381678, acc 0.982\n",
      "Evaluation:2019-04-19T11:16:54.554995: step 153, loss 0.398633, acc 0.981\n",
      "Evaluation:2019-04-19T11:17:19.391576: step 154, loss 0.38174, acc 0.984\n",
      "Evaluation:2019-04-19T11:17:44.046643: step 155, loss 0.350585, acc 0.988\n",
      "Evaluation:2019-04-19T11:18:09.711012: step 156, loss 0.3227, acc 0.987\n",
      "Evaluation:2019-04-19T11:18:34.088824: step 157, loss 0.302643, acc 0.987\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-150\n",
      "\n",
      "2019-04-19T11:18:40.883651: step 158, loss 3.65694, acc 0.320312\n",
      "2019-04-19T11:18:46.688131: step 159, loss 3.36052, acc 0.414062\n",
      "2019-04-19T11:18:52.234297: step 160, loss 3.56816, acc 0.335938\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import os\n",
    "import shutil\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "indexFreqs = data.indexFreqs\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        acc_summary = tf.summary.scalar('accuracy', lstm.accuracy)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        shutil.rmtree('../model/AdversarialLSTM/savedModel')  # 去除 model 目录下文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/AdversarialLSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summaries,step, loss, accuracy = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.accuracy],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(timeStr, step, loss, accuracy))\n",
    "            trainSummaryWriter.add_summary(summaries, step)\n",
    "\n",
    "        def devStep(batchX, batchY, writer=None):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            _, summaries,step, loss, accuracy = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.accuracy],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            print(\"Evaluation:{}: step {}, loss {:g}, acc {:g}\".format(timeStr, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        devStep(evalReviews, evalLabels, writer=evalSummaryWriter)\n",
    "                        \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/AdversarialLSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
