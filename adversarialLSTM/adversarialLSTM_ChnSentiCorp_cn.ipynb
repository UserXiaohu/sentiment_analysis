{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.19\n",
    "    @Purpose: 中文文本分类实战（七）—— Adversarial LSTM模型\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets: ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度。\n",
    "    @Link : https://github.com/jiangxinyang227/textClassifier/tree/master/RCNN\n",
    "    @Reference : https://www.cnblogs.com/jiangxinyang/p/10208290.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度，总共有三个数据文件，在/data/ChnSentiCorp_cn_data目录下，包括ChnSentiCorp_htl_all.csv。在进行文本分类时需要有标签的数据（labeledTrainData），数据预处理如文本分类实战（一）—— word2vec预训练词向量中一样，预处理后的文件为/data/ChnSentiCorp_htl_all.csv/labeledTrain.csv。\n",
    "\n",
    "\n",
    "## Adversarial LSTM模型结构\n",
    "\n",
    "RCNN模型来源于论文Adversarial Training Methods For Semi-Supervised Text Classification {https://arxiv.org/abs/1605.07725}。其模型结构如下右图所示：\n",
    "\n",
    "![avatar](../images/Adversarial_LSTM.png)\n",
    "\n",
    "　上图中左边为正常的LSTM结构，右图为Adversarial LSTM结构，可以看出在输出时加上了噪声。\n",
    "\n",
    "　　Adversarial LSTM的核心思想是通过对word Embedding上添加噪音生成对抗样本，将对抗样本以和原始样本 同样的形式喂给模型，得到一个Adversarial Loss，通过和原始样本的loss相加得到新的损失，通过优化该新 的损失来训练模型，作者认为这种方法能对word embedding加上正则化，避免过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import threading\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 10\n",
    "    checkpointEvery = 10\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = 128  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    epsilon = 5\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/ChnSentiCorp_cn_data/labeledCharTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/cnews_data/stopwords.txt\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据\n",
    "\n",
    "　　1）将数据加载进来，将句子分割成词表示，并去除低频词和停用词。\n",
    "\n",
    "　　2）将词映射成索引表示，构建词汇-索引映射表，并保存成json的数据格式，之后做inference时可以用到。（注意，有的词可能不在word2vec的预训练词向量中，这种词直接用UNK表示）\n",
    "\n",
    "　　3）从预训练的词向量模型中读取出词向量，作为初始化值输入到模型中。\n",
    "\n",
    "　　4）将数据集分割成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.indexFreqs = []  # 统计词空间中的词在出现在多少个review中\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath,encoding=\"utf-8\")\n",
    "        labels = df[\"label\"].tolist()\n",
    "        review = df[\"review_cut_word\"].tolist()\n",
    "        #print(\"review:{0}\".format(review[0:1]))\n",
    "        reviews = [str(line).strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 得到逆词频\n",
    "        self._getWordIndexFreq(vocab, reviews)\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../data/ChnSentiCorp_cn_data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _getWordIndexFreq(self, vocab, reviews):\n",
    "        \"\"\"\n",
    "        统计词汇空间中各个词出现在多少个文本中\n",
    "        \"\"\"\n",
    "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for review in reviewDicts:\n",
    "                if word in review:\n",
    "                    count += 1\n",
    "            indexFreqs[self._wordToIndex[word]] = count\n",
    "        \n",
    "        self.indexFreqs = indexFreqs\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\" ,encoding='utf-8') as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (4612, 200)\n",
      "train label shape: (4612, 1)\n",
      "eval data shape: (1154, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4732, 200)\n"
     ]
    }
   ],
   "source": [
    "print(data.wordEmbedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成batch数据集\n",
    "\n",
    "　　采用生成器的形式向模型输入batch数据集，（生成器可以避免将所有的数据加入到内存中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class AdversarialLSTM(object):\n",
    "    \"\"\"\n",
    "    Adversarial LSTM模型 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.config = config\n",
    "        \n",
    "        # 根据词的频率计算权重\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
    "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用词频计算新的词嵌入矩阵\n",
    "            normWordEmbedding = self._normalize(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), weights)\n",
    "            \n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "            \n",
    "         # 计算二元交叉熵损失 \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "                loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "        \n",
    "        self.loss = loss + perturLoss\n",
    "            \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        \"\"\"\n",
    "        Bi-LSTM + Attention 的模型结构\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        \n",
    "        # 定义双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "           \n",
    "            # 定义前向LSTM结构\n",
    "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "            # 定义反向LSTM结构\n",
    "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                          self.embeddedWords, dtype=tf.float32,\n",
    "                                                                          scope=\"bi-lstm\")\n",
    "\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        \"\"\"\n",
    "        对word embedding 结合权重做标准化处理\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "        print(mean)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "        print(var)\n",
    "        stddev = tf.sqrt(1e-6 + var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    def _addPerturbation(self, embedded, loss):\n",
    "        \"\"\"\n",
    "        添加波动到word embedding\n",
    "        \"\"\"\n",
    "        grad, = tf.gradients(\n",
    "            loss,\n",
    "            embedded,\n",
    "            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
    "        return embedded + perturb\n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        # shape(x) = (batch, num_timesteps, d)\n",
    "        # Divide x by max(abs(x)) for a numerically stable L2 norm.\n",
    "        # 2norm(x) = a * 2norm(x/a)\n",
    "        # Scale over the full sequence, dims (1, 2)\n",
    "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(\n",
    "            tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        return norm_length * x_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = 0\n",
    "    try:\n",
    "        auc = roc_auc_score(trueY, predY)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/MatMul:0\", shape=(1, 200), dtype=float32)\n",
      "Tensor(\"embedding/MatMul_1:0\", shape=(1, 200), dtype=float32)\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/hist is illegal; using Bi-LSTM/outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/sparsity is illegal; using Bi-LSTM/outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using loss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using loss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\pythonWp\\nlp\\textClassifier\\adversarialLSTM\\summarys\n",
      "\n",
      "start training model\n",
      "2019-03-19T11:26:25.859389, step: 1, loss: 1.2750738859176636, acc: 0.8438, auc: 0.5954, precision: 0.25, recall: 0.125\n",
      "2019-03-19T11:26:28.991134, step: 2, loss: 0.9444738626480103, acc: 0.8438, auc: 0.4417, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:32.167187, step: 3, loss: 0.8461273312568665, acc: 0.8828, auc: 0.5351, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:35.166208, step: 4, loss: 1.0138545036315918, acc: 0.8594, auc: 0.5071, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:38.219306, step: 5, loss: 0.6769015192985535, acc: 0.9062, auc: 0.4713, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:41.180328, step: 6, loss: 0.6545708179473877, acc: 0.9062, auc: 0.6365, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:44.315197, step: 7, loss: 1.0453097820281982, acc: 0.8281, auc: 0.5892, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:47.913176, step: 8, loss: 0.8649338483810425, acc: 0.8438, auc: 0.5829, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:51.110684, step: 9, loss: 0.6770297288894653, acc: 0.9141, auc: 0.533, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:26:54.446476, step: 10, loss: 0.7169842720031738, acc: 0.875, auc: 0.6726, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:27:28.078109, step: 10, loss: 3.3249892128838434, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-10\n",
      "\n",
      "2019-03-19T11:27:32.852630, step: 11, loss: 0.8250782489776611, acc: 0.8672, auc: 0.5614, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:36.858539, step: 12, loss: 1.005397081375122, acc: 0.8047, auc: 0.574, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:40.810285, step: 13, loss: 0.647107720375061, acc: 0.9141, auc: 0.4786, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:44.778434, step: 14, loss: 0.7924854755401611, acc: 0.8672, auc: 0.5665, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:48.705222, step: 15, loss: 0.9336377382278442, acc: 0.8438, auc: 0.3755, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:52.870511, step: 16, loss: 0.9950075149536133, acc: 0.8203, auc: 0.4348, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:27:56.914300, step: 17, loss: 0.7895441055297852, acc: 0.8594, auc: 0.5242, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:28:00.952581, step: 18, loss: 0.8967840671539307, acc: 0.8203, auc: 0.6154, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:28:05.022838, step: 19, loss: 0.7985674142837524, acc: 0.8516, auc: 0.7175, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:28:09.038566, step: 20, loss: 0.903200089931488, acc: 0.8281, auc: 0.5416, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:28:46.372424, step: 20, loss: 3.546246396170722, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-20\n",
      "\n",
      "2019-03-19T11:28:51.366079, step: 21, loss: 0.7524601817131042, acc: 0.8594, auc: 0.7348, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:28:55.716452, step: 22, loss: 0.905228853225708, acc: 0.8047, auc: 0.6867, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:28:59.949137, step: 23, loss: 0.9547019004821777, acc: 0.8281, auc: 0.5712, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:29:03.957423, step: 24, loss: 0.8237386345863342, acc: 0.8672, auc: 0.5829, precision: 0.5, recall: 0.0588\n",
      "2019-03-19T11:29:08.201081, step: 25, loss: 0.7209958434104919, acc: 0.8906, auc: 0.7798, precision: 0.8333, recall: 0.2778\n",
      "2019-03-19T11:29:12.259233, step: 26, loss: 0.7283744812011719, acc: 0.8828, auc: 0.6917, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:29:16.324366, step: 27, loss: 0.7332513928413391, acc: 0.8594, auc: 0.6359, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:29:20.581986, step: 28, loss: 0.9086834192276001, acc: 0.8281, auc: 0.678, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:29:24.670059, step: 29, loss: 0.8937257528305054, acc: 0.8203, auc: 0.6261, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:29:29.011454, step: 30, loss: 0.77576744556427, acc: 0.8672, auc: 0.6184, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:30:06.786963, step: 30, loss: 3.6149361398484974, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-30\n",
      "\n",
      "2019-03-19T11:30:11.779617, step: 31, loss: 0.6482806205749512, acc: 0.8906, auc: 0.7463, precision: 1.0, recall: 0.0667\n",
      "2019-03-19T11:30:16.142953, step: 32, loss: 0.5921734571456909, acc: 0.8984, auc: 0.709, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:30:20.279896, step: 33, loss: 0.5132133364677429, acc: 0.9141, auc: 0.7576, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:30:24.624285, step: 34, loss: 0.737790584564209, acc: 0.875, auc: 0.6194, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:30:28.871930, step: 35, loss: 0.7170193791389465, acc: 0.8672, auc: 0.8405, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:30:33.371901, step: 36, loss: 0.7444589138031006, acc: 0.8359, auc: 0.8789, precision: 0.0, recall: 0.0\n",
      "start training model\n",
      "2019-03-19T11:30:37.725264, step: 37, loss: 0.7369426488876343, acc: 0.875, auc: 0.6529, precision: 1.0, recall: 0.0588\n",
      "2019-03-19T11:30:41.916063, step: 38, loss: 0.6162967085838318, acc: 0.8828, auc: 0.8454, precision: 1.0, recall: 0.0625\n",
      "2019-03-19T11:30:46.283389, step: 39, loss: 0.5431201457977295, acc: 0.9141, auc: 0.7894, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:30:50.480171, step: 40, loss: 0.508272111415863, acc: 0.8984, auc: 0.8087, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T11:31:29.669418, step: 40, loss: 2.7524514728122287, acc: 0.030355555555555556, auc: 0.0, precision: 1.0, recall: 0.030355555555555556\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-40\n",
      "\n",
      "2019-03-19T11:31:34.772776, step: 41, loss: 0.5997612476348877, acc: 0.875, auc: 0.8797, precision: 1.0, recall: 0.0588\n",
      "2019-03-19T11:31:39.185981, step: 42, loss: 0.6401287317276001, acc: 0.875, auc: 0.7801, precision: 0.5, recall: 0.0625\n",
      "2019-03-19T11:31:43.667002, step: 43, loss: 0.4940829873085022, acc: 0.9141, auc: 0.8139, precision: 1.0, recall: 0.0833\n",
      "2019-03-19T11:31:48.007400, step: 44, loss: 0.9688690900802612, acc: 0.7734, auc: 0.7779, precision: 1.0, recall: 0.0333\n",
      "2019-03-19T11:31:52.314888, step: 45, loss: 0.8004611134529114, acc: 0.8125, auc: 0.7889, precision: 0.0, recall: 0.0\n",
      "2019-03-19T11:31:56.621374, step: 46, loss: 0.6899350881576538, acc: 0.8438, auc: 0.7824, precision: 1.0, recall: 0.0476\n",
      "2019-03-19T11:32:00.946813, step: 47, loss: 0.5371537208557129, acc: 0.9062, auc: 0.8365, precision: 1.0, recall: 0.1429\n",
      "2019-03-19T11:32:05.522582, step: 48, loss: 0.821922779083252, acc: 0.8438, auc: 0.7449, precision: 0.7143, recall: 0.2174\n",
      "2019-03-19T11:32:09.713380, step: 49, loss: 0.5893117785453796, acc: 0.875, auc: 0.8347, precision: 0.5714, recall: 0.2353\n",
      "2019-03-19T11:32:14.099655, step: 50, loss: 0.6339976191520691, acc: 0.8594, auc: 0.8268, precision: 0.5, recall: 0.2222\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:32:53.867357, step: 50, loss: 2.1859556568993463, acc: 0.24653333333333338, auc: 0.0, precision: 1.0, recall: 0.24653333333333338\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-50\n",
      "\n",
      "2019-03-19T11:32:59.087405, step: 51, loss: 0.581750750541687, acc: 0.8906, auc: 0.8711, precision: 0.6667, recall: 0.25\n",
      "2019-03-19T11:33:03.915498, step: 52, loss: 0.5487209558486938, acc: 0.8594, auc: 0.9136, precision: 1.0, recall: 0.0526\n",
      "2019-03-19T11:33:08.695721, step: 53, loss: 0.37966060638427734, acc: 0.9141, auc: 0.9561, precision: 1.0, recall: 0.2143\n",
      "2019-03-19T11:33:13.285451, step: 54, loss: 0.6808359622955322, acc: 0.8594, auc: 0.8109, precision: 0.8333, recall: 0.2273\n",
      "2019-03-19T11:33:17.869200, step: 55, loss: 0.5450776815414429, acc: 0.9141, auc: 0.8161, precision: 1.0, recall: 0.1538\n",
      "2019-03-19T11:33:22.461922, step: 56, loss: 0.5799719095230103, acc: 0.8672, auc: 0.8783, precision: 0.625, recall: 0.2632\n",
      "2019-03-19T11:33:27.037692, step: 57, loss: 0.5002051591873169, acc: 0.8516, auc: 0.9101, precision: 0.6667, recall: 0.1905\n",
      "2019-03-19T11:33:31.387065, step: 58, loss: 0.626484215259552, acc: 0.875, auc: 0.8422, precision: 0.8, recall: 0.3636\n",
      "2019-03-19T11:33:35.805256, step: 59, loss: 0.6272273063659668, acc: 0.8828, auc: 0.8656, precision: 0.8, recall: 0.381\n",
      "2019-03-19T11:33:40.457821, step: 60, loss: 0.5821086168289185, acc: 0.8906, auc: 0.848, precision: 0.75, recall: 0.3333\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:34:20.669336, step: 60, loss: 1.6647208796607122, acc: 0.49652222222222225, auc: 0.0, precision: 1.0, recall: 0.49652222222222225\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-60\n",
      "\n",
      "2019-03-19T11:34:26.036988, step: 61, loss: 0.38063913583755493, acc: 0.9453, auc: 0.9585, precision: 0.6364, recall: 0.7\n",
      "2019-03-19T11:34:30.535961, step: 62, loss: 0.3464575409889221, acc: 0.9219, auc: 0.972, precision: 0.6, recall: 0.2727\n",
      "2019-03-19T11:34:35.024962, step: 63, loss: 0.43370044231414795, acc: 0.9297, auc: 0.7965, precision: 0.5, recall: 0.1111\n",
      "2019-03-19T11:34:39.373339, step: 64, loss: 0.6964410543441772, acc: 0.8359, auc: 0.8585, precision: 0.5, recall: 0.0952\n",
      "2019-03-19T11:34:44.030891, step: 65, loss: 0.7758984565734863, acc: 0.8359, auc: 0.8413, precision: 0.8, recall: 0.1667\n",
      "2019-03-19T11:34:48.699409, step: 66, loss: 0.47930967807769775, acc: 0.8828, auc: 0.9091, precision: 1.0, recall: 0.1667\n",
      "2019-03-19T11:34:52.915142, step: 67, loss: 0.6007644534111023, acc: 0.8516, auc: 0.9226, precision: 1.0, recall: 0.1739\n",
      "2019-03-19T11:34:57.469965, step: 68, loss: 0.5749372839927673, acc: 0.8672, auc: 0.8657, precision: 0.6667, recall: 0.1111\n",
      "2019-03-19T11:35:02.124525, step: 69, loss: 0.4831273555755615, acc: 0.8906, auc: 0.9189, precision: 0.6667, recall: 0.3529\n",
      "2019-03-19T11:35:06.754150, step: 70, loss: 0.5059908628463745, acc: 0.9141, auc: 0.9383, precision: 0.8333, recall: 0.6522\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:35:47.756551, step: 70, loss: 1.5034243265787761, acc: 0.6015666666666667, auc: 0.0, precision: 1.0, recall: 0.6015666666666667\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-70\n",
      "\n",
      "2019-03-19T11:35:53.056382, step: 71, loss: 0.6899763345718384, acc: 0.8672, auc: 0.7885, precision: 0.5833, recall: 0.3684\n",
      "2019-03-19T11:35:57.621181, step: 72, loss: 0.4113595485687256, acc: 0.9062, auc: 0.9454, precision: 0.5, recall: 0.4167\n",
      "start training model\n",
      "2019-03-19T11:36:02.136113, step: 73, loss: 0.5377852916717529, acc: 0.8906, auc: 0.868, precision: 0.6364, recall: 0.4118\n",
      "2019-03-19T11:36:06.660020, step: 74, loss: 0.6555052995681763, acc: 0.8281, auc: 0.9099, precision: 0.8333, recall: 0.3333\n",
      "2019-03-19T11:36:11.305603, step: 75, loss: 0.5680533647537231, acc: 0.8906, auc: 0.8727, precision: 0.7, recall: 0.3889\n",
      "2019-03-19T11:36:15.828514, step: 76, loss: 0.5815251469612122, acc: 0.875, auc: 0.8145, precision: 0.3333, recall: 0.1429\n",
      "2019-03-19T11:36:20.665584, step: 77, loss: 0.4564862847328186, acc: 0.8984, auc: 0.9526, precision: 1.0, recall: 0.1875\n",
      "2019-03-19T11:36:25.380979, step: 78, loss: 0.5218726396560669, acc: 0.8828, auc: 0.899, precision: 0.6667, recall: 0.125\n",
      "2019-03-19T11:36:30.136268, step: 79, loss: 0.6048732995986938, acc: 0.8516, auc: 0.8912, precision: 0.6, recall: 0.15\n",
      "2019-03-19T11:36:34.822741, step: 80, loss: 0.3982071578502655, acc: 0.9297, auc: 0.9411, precision: 0.8333, recall: 0.3846\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:37:18.509965, step: 80, loss: 2.1118511888715954, acc: 0.45658888888888893, auc: 0.0, precision: 1.0, recall: 0.45658888888888893\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-80\n",
      "\n",
      "2019-03-19T11:37:23.905543, step: 81, loss: 0.5710361003875732, acc: 0.8906, auc: 0.8667, precision: 0.8, recall: 0.4\n",
      "2019-03-19T11:37:28.362630, step: 82, loss: 0.43589308857917786, acc: 0.8828, auc: 0.9537, precision: 0.7778, recall: 0.35\n",
      "2019-03-19T11:37:32.958345, step: 83, loss: 0.5578442811965942, acc: 0.8984, auc: 0.8545, precision: 0.7778, recall: 0.3889\n",
      "2019-03-19T11:37:37.520152, step: 84, loss: 0.44994133710861206, acc: 0.8906, auc: 0.9407, precision: 0.8, recall: 0.4\n",
      "2019-03-19T11:37:42.126837, step: 85, loss: 0.6057029962539673, acc: 0.8984, auc: 0.8447, precision: 0.75, recall: 0.3529\n",
      "2019-03-19T11:37:46.740505, step: 86, loss: 0.5193260908126831, acc: 0.8828, auc: 0.907, precision: 0.875, recall: 0.3333\n",
      "2019-03-19T11:37:51.095863, step: 87, loss: 0.4299236536026001, acc: 0.9297, auc: 0.9174, precision: 0.8889, recall: 0.5\n",
      "2019-03-19T11:37:55.773360, step: 88, loss: 0.3096168041229248, acc: 0.9531, auc: 0.9842, precision: 0.75, recall: 0.75\n",
      "2019-03-19T11:38:00.363092, step: 89, loss: 0.41814589500427246, acc: 0.9531, auc: 0.8791, precision: 1.0, recall: 0.6\n",
      "2019-03-19T11:38:04.800233, step: 90, loss: 0.45131590962409973, acc: 0.8984, auc: 0.9597, precision: 1.0, recall: 0.4091\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:38:45.897378, step: 90, loss: 2.2855328718821206, acc: 0.43143333333333334, auc: 0.0, precision: 1.0, recall: 0.43143333333333334\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-90\n",
      "\n",
      "2019-03-19T11:38:51.217160, step: 91, loss: 0.34489870071411133, acc: 0.9375, auc: 0.9314, precision: 1.0, recall: 0.2\n",
      "2019-03-19T11:38:55.762011, step: 92, loss: 0.4058973491191864, acc: 0.9219, auc: 0.9186, precision: 1.0, recall: 0.3333\n",
      "2019-03-19T11:39:00.527273, step: 93, loss: 0.4944801330566406, acc: 0.8906, auc: 0.9101, precision: 1.0, recall: 0.2222\n",
      "2019-03-19T11:39:05.070129, step: 94, loss: 0.4640619158744812, acc: 0.9219, auc: 0.8903, precision: 0.7778, recall: 0.4667\n",
      "2019-03-19T11:39:09.768571, step: 95, loss: 0.604354202747345, acc: 0.875, auc: 0.8847, precision: 0.7, recall: 0.35\n",
      "2019-03-19T11:39:14.356309, step: 96, loss: 0.5122361779212952, acc: 0.8828, auc: 0.903, precision: 0.8, recall: 0.2222\n",
      "2019-03-19T11:39:19.591315, step: 97, loss: 0.4411466717720032, acc: 0.9297, auc: 0.9012, precision: 0.8889, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T11:39:24.544075, step: 98, loss: 0.422291100025177, acc: 0.9453, auc: 0.8785, precision: 1.0, recall: 0.5333\n",
      "2019-03-19T11:39:29.781078, step: 99, loss: 0.4859163463115692, acc: 0.8906, auc: 0.8702, precision: 0.4, recall: 0.1538\n",
      "2019-03-19T11:39:34.846537, step: 100, loss: 0.5195876359939575, acc: 0.8906, auc: 0.9051, precision: 0.6875, recall: 0.55\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:40:18.675383, step: 100, loss: 1.8533211019304063, acc: 0.5026222222222222, auc: 0.0, precision: 1.0, recall: 0.5026222222222222\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-100\n",
      "\n",
      "2019-03-19T11:40:24.311318, step: 101, loss: 0.353743314743042, acc: 0.9141, auc: 0.9652, precision: 0.7, recall: 0.4667\n",
      "2019-03-19T11:40:28.873123, step: 102, loss: 0.35567164421081543, acc: 0.9297, auc: 0.9445, precision: 0.875, recall: 0.4667\n",
      "2019-03-19T11:40:33.349159, step: 103, loss: 0.46162140369415283, acc: 0.9062, auc: 0.8454, precision: 0.75, recall: 0.375\n",
      "2019-03-19T11:40:37.997735, step: 104, loss: 0.5876970887184143, acc: 0.8672, auc: 0.9274, precision: 1.0, recall: 0.32\n",
      "2019-03-19T11:40:42.698169, step: 105, loss: 0.4666498899459839, acc: 0.8984, auc: 0.9126, precision: 0.6923, recall: 0.5\n",
      "2019-03-19T11:40:47.394616, step: 106, loss: 0.5973032712936401, acc: 0.8828, auc: 0.8819, precision: 0.6923, recall: 0.45\n",
      "2019-03-19T11:40:51.974374, step: 107, loss: 0.44922465085983276, acc: 0.9062, auc: 0.9485, precision: 0.8125, recall: 0.5909\n",
      "2019-03-19T11:40:56.522219, step: 108, loss: 0.4922845661640167, acc: 0.8984, auc: 0.9254, precision: 0.8462, recall: 0.5\n",
      "start training model\n",
      "2019-03-19T11:41:01.175779, step: 109, loss: 0.5492026805877686, acc: 0.8984, auc: 0.8586, precision: 0.7778, recall: 0.3889\n",
      "2019-03-19T11:41:06.135521, step: 110, loss: 0.4746054708957672, acc: 0.9219, auc: 0.8997, precision: 0.7, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:41:49.240303, step: 110, loss: 1.6850035852856107, acc: 0.5460111111111111, auc: 0.0, precision: 1.0, recall: 0.5460111111111111\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-110\n",
      "\n",
      "2019-03-19T11:41:54.782488, step: 111, loss: 0.41249632835388184, acc: 0.8984, auc: 0.9616, precision: 0.6923, recall: 0.5\n",
      "2019-03-19T11:41:59.428069, step: 112, loss: 0.5017815828323364, acc: 0.8984, auc: 0.9275, precision: 0.9091, recall: 0.4545\n",
      "2019-03-19T11:42:04.852570, step: 113, loss: 0.5483251810073853, acc: 0.9062, auc: 0.8879, precision: 0.9091, recall: 0.4762\n",
      "2019-03-19T11:42:10.399743, step: 114, loss: 0.41157639026641846, acc: 0.9453, auc: 0.856, precision: 1.0, recall: 0.5333\n",
      "2019-03-19T11:42:15.373447, step: 115, loss: 0.4224240779876709, acc: 0.875, auc: 0.9747, precision: 0.875, recall: 0.3182\n",
      "2019-03-19T11:42:20.135718, step: 116, loss: 0.3949386179447174, acc: 0.9219, auc: 0.9274, precision: 1.0, recall: 0.4118\n",
      "2019-03-19T11:42:25.031632, step: 117, loss: 0.23580893874168396, acc: 0.9531, auc: 0.9839, precision: 1.0, recall: 0.4\n",
      "2019-03-19T11:42:29.970430, step: 118, loss: 0.6647846698760986, acc: 0.8516, auc: 0.9097, precision: 0.8333, recall: 0.2174\n",
      "2019-03-19T11:42:35.302179, step: 119, loss: 0.4185944199562073, acc: 0.9531, auc: 0.9141, precision: 0.9167, recall: 0.6875\n",
      "2019-03-19T11:42:40.389580, step: 120, loss: 0.5138158798217773, acc: 0.8984, auc: 0.9232, precision: 0.9091, recall: 0.4545\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:43:25.914891, step: 120, loss: 1.4260911014344957, acc: 0.6406333333333333, auc: 0.0, precision: 1.0, recall: 0.6406333333333333\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-120\n",
      "\n",
      "2019-03-19T11:43:31.889919, step: 121, loss: 0.3124033510684967, acc: 0.9453, auc: 0.9737, precision: 0.6154, recall: 0.8\n",
      "2019-03-19T11:43:36.977321, step: 122, loss: 0.6218410730361938, acc: 0.875, auc: 0.8986, precision: 0.75, recall: 0.5769\n",
      "2019-03-19T11:43:41.857276, step: 123, loss: 0.33625292778015137, acc: 0.9609, auc: 0.9665, precision: 0.8667, recall: 0.8125\n",
      "2019-03-19T11:43:46.790090, step: 124, loss: 0.5231692790985107, acc: 0.9141, auc: 0.8747, precision: 0.7333, recall: 0.6111\n",
      "2019-03-19T11:43:51.567326, step: 125, loss: 0.4729282855987549, acc: 0.875, auc: 0.9462, precision: 0.8182, recall: 0.3913\n",
      "2019-03-19T11:43:56.553993, step: 126, loss: 0.3784477710723877, acc: 0.9375, auc: 0.9102, precision: 0.6667, recall: 0.4\n",
      "2019-03-19T11:44:01.812934, step: 127, loss: 0.3850860893726349, acc: 0.8984, auc: 0.9454, precision: 0.8333, recall: 0.2941\n",
      "2019-03-19T11:44:06.572214, step: 128, loss: 0.5075394511222839, acc: 0.9062, auc: 0.8967, precision: 0.7778, recall: 0.4118\n",
      "2019-03-19T11:44:11.645652, step: 129, loss: 0.3958583474159241, acc: 0.9297, auc: 0.908, precision: 1.0, recall: 0.25\n",
      "2019-03-19T11:44:16.405928, step: 130, loss: 0.3929447531700134, acc: 0.9141, auc: 0.9242, precision: 1.0, recall: 0.3529\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:45:01.561229, step: 130, loss: 2.2493693828582764, acc: 0.4774333333333333, auc: 0.0, precision: 1.0, recall: 0.4774333333333333\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-130\n",
      "\n",
      "2019-03-19T11:45:07.488519, step: 131, loss: 0.4113517999649048, acc: 0.9062, auc: 0.9511, precision: 0.9167, recall: 0.5\n",
      "2019-03-19T11:45:12.335542, step: 132, loss: 0.581856906414032, acc: 0.8281, auc: 0.9082, precision: 0.5, recall: 0.1364\n",
      "2019-03-19T11:45:17.189566, step: 133, loss: 0.45779934525489807, acc: 0.8672, auc: 0.9187, precision: 0.5556, recall: 0.2778\n",
      "2019-03-19T11:45:22.171251, step: 134, loss: 0.43672725558280945, acc: 0.9062, auc: 0.9515, precision: 0.8462, recall: 0.5238\n",
      "2019-03-19T11:45:27.127003, step: 135, loss: 0.4132944941520691, acc: 0.9141, auc: 0.9657, precision: 0.6667, recall: 0.7778\n",
      "2019-03-19T11:45:32.037877, step: 136, loss: 0.4541836380958557, acc: 0.9297, auc: 0.927, precision: 0.875, recall: 0.6667\n",
      "2019-03-19T11:45:36.787182, step: 137, loss: 0.448914110660553, acc: 0.8984, auc: 0.9233, precision: 0.5833, recall: 0.4667\n",
      "2019-03-19T11:45:41.806766, step: 138, loss: 0.5052392482757568, acc: 0.9141, auc: 0.9218, precision: 0.8333, recall: 0.5263\n",
      "2019-03-19T11:45:46.630870, step: 139, loss: 0.41928067803382874, acc: 0.9141, auc: 0.891, precision: 0.6364, recall: 0.5\n",
      "2019-03-19T11:45:51.305375, step: 140, loss: 0.435729444026947, acc: 0.8906, auc: 0.8684, precision: 0.5, recall: 0.2143\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:46:34.874913, step: 140, loss: 2.219777822494507, acc: 0.46962222222222216, auc: 0.0, precision: 1.0, recall: 0.46962222222222216\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-140\n",
      "\n",
      "2019-03-19T11:46:40.385185, step: 141, loss: 0.22625097632408142, acc: 0.9766, auc: 0.9552, precision: 1.0, recall: 0.625\n",
      "2019-03-19T11:46:45.314009, step: 142, loss: 0.3501851558685303, acc: 0.9141, auc: 0.9576, precision: 0.8571, recall: 0.375\n",
      "2019-03-19T11:46:49.878807, step: 143, loss: 0.42237192392349243, acc: 0.9062, auc: 0.9498, precision: 0.8889, recall: 0.4211\n",
      "2019-03-19T11:46:54.750784, step: 144, loss: 0.7238255739212036, acc: 0.8359, auc: 0.9106, precision: 0.8571, recall: 0.2308\n",
      "start training model\n",
      "2019-03-19T11:46:59.838186, step: 145, loss: 0.41048088669776917, acc: 0.9219, auc: 0.9141, precision: 0.8182, recall: 0.5294\n",
      "2019-03-19T11:47:05.095136, step: 146, loss: 0.40498897433280945, acc: 0.9453, auc: 0.8695, precision: 1.0, recall: 0.3636\n",
      "2019-03-19T11:47:10.165580, step: 147, loss: 0.4240643382072449, acc: 0.9062, auc: 0.9374, precision: 0.875, recall: 0.3889\n",
      "2019-03-19T11:47:15.084433, step: 148, loss: 0.5387133955955505, acc: 0.8828, auc: 0.9088, precision: 0.6923, recall: 0.45\n",
      "2019-03-19T11:47:20.122965, step: 149, loss: 0.43655094504356384, acc: 0.9062, auc: 0.9334, precision: 0.8889, recall: 0.4211\n",
      "2019-03-19T11:47:25.021870, step: 150, loss: 0.3809300661087036, acc: 0.9297, auc: 0.9653, precision: 0.9286, recall: 0.619\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:48:10.550172, step: 150, loss: 1.545791467030843, acc: 0.5920111111111112, auc: 0.0, precision: 1.0, recall: 0.5920111111111112\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-150\n",
      "\n",
      "2019-03-19T11:48:18.570735, step: 151, loss: 0.3744778633117676, acc: 0.9141, auc: 0.9555, precision: 0.7143, recall: 0.5882\n",
      "2019-03-19T11:48:27.355252, step: 152, loss: 0.28564321994781494, acc: 0.9297, auc: 0.9746, precision: 0.875, recall: 0.4667\n",
      "2019-03-19T11:48:35.438646, step: 153, loss: 0.40412044525146484, acc: 0.9297, auc: 0.9211, precision: 0.7, recall: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T11:48:43.477165, step: 154, loss: 0.3724292516708374, acc: 0.9297, auc: 0.9571, precision: 0.8, recall: 0.6667\n",
      "2019-03-19T11:48:51.398984, step: 155, loss: 0.6035870313644409, acc: 0.875, auc: 0.9112, precision: 1.0, recall: 0.2727\n",
      "2019-03-19T11:48:59.277922, step: 156, loss: 0.543547511100769, acc: 0.875, auc: 0.9126, precision: 1.0, recall: 0.1111\n",
      "2019-03-19T11:49:06.133598, step: 157, loss: 0.47186097502708435, acc: 0.8984, auc: 0.9014, precision: 0.7, recall: 0.4118\n",
      "2019-03-19T11:49:14.106286, step: 158, loss: 0.46134689450263977, acc: 0.9062, auc: 0.9177, precision: 1.0, recall: 0.3333\n",
      "2019-03-19T11:49:22.515808, step: 159, loss: 0.45328181982040405, acc: 0.9219, auc: 0.8984, precision: 0.875, recall: 0.4375\n",
      "2019-03-19T11:49:30.572273, step: 160, loss: 0.46669691801071167, acc: 0.9062, auc: 0.9184, precision: 1.0, recall: 0.3684\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:50:42.792736, step: 160, loss: 1.6312008963690863, acc: 0.5894222222222223, auc: 0.0, precision: 1.0, recall: 0.5894222222222223\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-160\n",
      "\n",
      "2019-03-19T11:50:51.903381, step: 161, loss: 0.30745288729667664, acc: 0.9531, auc: 0.9699, precision: 0.8889, recall: 0.6154\n",
      "2019-03-19T11:50:59.820220, step: 162, loss: 0.45468899607658386, acc: 0.9219, auc: 0.9039, precision: 0.9091, recall: 0.5263\n",
      "2019-03-19T11:51:06.609073, step: 163, loss: 0.46263203024864197, acc: 0.8906, auc: 0.9431, precision: 0.8571, recall: 0.5\n",
      "2019-03-19T11:51:14.819128, step: 164, loss: 0.28913527727127075, acc: 0.9609, auc: 0.9847, precision: 0.9167, recall: 0.7333\n",
      "2019-03-19T11:51:23.137892, step: 165, loss: 0.40547505021095276, acc: 0.9297, auc: 0.9507, precision: 0.8889, recall: 0.6957\n",
      "2019-03-19T11:51:31.268159, step: 166, loss: 0.4496687948703766, acc: 0.8906, auc: 0.9269, precision: 0.5833, recall: 0.4375\n",
      "2019-03-19T11:51:39.314651, step: 167, loss: 0.44472527503967285, acc: 0.8984, auc: 0.9384, precision: 0.6923, recall: 0.5\n",
      "2019-03-19T11:51:46.874445, step: 168, loss: 0.3956216275691986, acc: 0.9141, auc: 0.9515, precision: 0.8571, recall: 0.5714\n",
      "2019-03-19T11:51:54.605781, step: 169, loss: 0.4252855181694031, acc: 0.8906, auc: 0.955, precision: 0.9, recall: 0.4091\n",
      "2019-03-19T11:52:03.178862, step: 170, loss: 0.5360326766967773, acc: 0.8828, auc: 0.9445, precision: 1.0, recall: 0.3478\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T11:53:15.302077, step: 170, loss: 2.3534071180555554, acc: 0.4566, auc: 0.0, precision: 1.0, recall: 0.4566\n",
      "Saved model checkpoint to ../model/AdversarialLSTM/model/my-model-170\n",
      "\n",
      "2019-03-19T11:53:26.124151, step: 171, loss: 0.3219766914844513, acc: 0.9297, auc: 0.9346, precision: 0.7143, recall: 0.4167\n",
      "2019-03-19T11:53:34.497766, step: 172, loss: 0.28111207485198975, acc: 0.9531, auc: 0.9634, precision: 1.0, recall: 0.6\n",
      "2019-03-19T11:53:43.289267, step: 173, loss: 0.465873122215271, acc: 0.9453, auc: 0.8783, precision: 0.9231, recall: 0.6667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1f546396d195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start training model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatchTrain\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainReviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mtrainStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mcurrentStep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-1f546396d195>\u001b[0m in \u001b[0;36mtrainStep\u001b[1;34m(batchX, batchY)\u001b[0m\n\u001b[0;32m     69\u001b[0m             _, summary, step, loss, predictions, binaryPreds = sess.run(\n\u001b[0;32m     70\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummaryOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinaryPreds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mtimeStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinaryPreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\progrom\\python\\python\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "indexFreqs = data.indexFreqs\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/AdversarialLSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/AdversarialLSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
