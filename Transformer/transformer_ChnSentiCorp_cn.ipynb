{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.19\n",
    "    @Purpose: 中文文本分类实战（八）—— Transformer模型\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets: ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度。\n",
    "    @Link : https://github.com/jiangxinyang227/textClassifier/tree/master/Transformer\n",
    "    @Reference : https://www.cnblogs.com/jiangxinyang/p/10210813.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "ChnSentiCorp 中文情感分析数据集，其目标是判断一段话的情感态度，总共有三个数据文件，在/data/ChnSentiCorp_cn_data目录下，包括ChnSentiCorp_htl_all.csv。在进行文本分类时需要有标签的数据（labeledTrainData），数据预处理如文本分类实战（一）—— word2vec预训练词向量中一样，预处理后的文件为/data/ChnSentiCorp_htl_all.csv/labeledTrain.csv。\n",
    "\n",
    "\n",
    "## Transformer 模型结构\n",
    "\n",
    "　Transformer模型来自于论文Attention Is All You Need，关于Transformer具体的介绍见这篇。Transformer模型具体结构如下图：\n",
    "\n",
    "![avatar](../images/Transformer.png)\n",
    "\n",
    "\n",
    "　　Transformer结构有两种：Encoder和Decoder，在文本分类中只使用到了Encoder，Decoder是生成式模型，主要用于自然语言生成的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 10\n",
    "    checkpointEvery = 10\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    filters = 128  # 内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize，因为要确保每个layer后的输出维度和输入维度是一致的。\n",
    "    numHeads = 8  # Attention 的头数\n",
    "    numBlocks = 1  # 设置transformer block的数量\n",
    "    epsilon = 1e-8  # LayerNorm 层中的最小除数\n",
    "    keepProp = 0.9  # multi head attention 中的dropout\n",
    "    \n",
    "    dropoutKeepProb = 0.5 # 全连接层的dropout\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/ChnSentiCorp_cn_data/labeledCharTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/cnews_data/stopwords.txt\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath,encoding=\"utf-8\")\n",
    "        labels = df[\"label\"].tolist()\n",
    "        review = df[\"review_cut_word\"].tolist()\n",
    "        #print(\"review:{0}\".format(review[0:1]))\n",
    "        reviews = [str(line).strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../data/ChnSentiCorp_cn_data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\" ,encoding='utf-8') as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (4612, 200)\n",
      "train label shape: (4612, 1)\n",
      "eval data shape: (1154, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置嵌入\n",
    "def fixedPositionEmbedding(batchSize, sequenceLen):\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], name=\"embeddedPosition\")\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层, 位置向量的定义方式有两种：一是直接用固定的one-hot的形式传入，然后和词向量拼接，在当前的数据集上表现效果更好。另一种\n",
    "        # 就是按照论文中的方法实现，这样的效果反而更差，可能是增大了模型的复杂度，在小数据集上表现不佳。\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "\n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            for i in range(config.model.numBlocks):\n",
    "                with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    multiHeadAtt = self._multiheadAttention(rawKeys=self.embedded, queries=self.embeddedWords,\n",
    "                                                            keys=self.embeddedWords)\n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    self.embeddedWords = self._feedForward(multiHeadAtt, \n",
    "                                                           [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "                \n",
    "            outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "\n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "\n",
    "#         with tf.name_scope(\"wordEmbedding\"):\n",
    "#             self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "#             self.wordEmbedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        \n",
    "#         with tf.name_scope(\"positionEmbedding\"):\n",
    "#             print(self.wordEmbedded)\n",
    "#             self.positionEmbedded = self._positionEmbedding()\n",
    "            \n",
    "#         self.embeddedWords = self.wordEmbedded + self.positionEmbedded\n",
    "            \n",
    "#         with tf.name_scope(\"transformer\"):\n",
    "#             for i in range(config.model.numBlocks):\n",
    "#                 with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     multiHeadAtt = self._multiheadAttention(rawKeys=self.wordEmbedded, queries=self.embeddedWords,\n",
    "#                                                             keys=self.embeddedWords)\n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     self.embeddedWords = self._feedForward(multiHeadAtt, [config.model.filters, config.model.embeddingSize])\n",
    "                \n",
    "#             outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize)])\n",
    "\n",
    "#         outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=self.dropoutKeepProb)\n",
    "    \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        # LayerNorm层和BN层有所不同\n",
    "        epsilon = self.config.model.epsilon\n",
    "\n",
    "        inputsShape = inputs.get_shape() # [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "        paramsShape = inputsShape[-1:]\n",
    "\n",
    "        # LayerNorm是在最后的维度上计算输入的数据的均值和方差，BN层是考虑所有维度的\n",
    "        # mean, variance的维度都是[batch_size, sequence_len, 1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "\n",
    "        beta = tf.Variable(tf.zeros(paramsShape))\n",
    "\n",
    "        gamma = tf.Variable(tf.ones(paramsShape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "        \n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, causality=False, scope=\"multiheadAttention\"):\n",
    "        # rawKeys 的作用是为了计算mask时用的，因为keys是加上了position embedding的，其中不存在padding为0的值\n",
    "        \n",
    "        numHeads = self.config.model.numHeads\n",
    "        keepProp = self.config.model.keepProp\n",
    "        \n",
    "        if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "        # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "        # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "        # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]\n",
    "        Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "        K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "\n",
    "        # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "        # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "        K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "        V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "\n",
    "        # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "        similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "\n",
    "        # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "        scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "        # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "        # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "        # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "        # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "\n",
    "        # 将每一时序上的向量中的值相加取平均值\n",
    "        keyMasks = tf.sign(tf.abs(tf.reduce_sum(rawKeys, axis=-1)))  # 维度[batch_size, time_step]\n",
    "\n",
    "        # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度\n",
    "        keyMasks = tf.tile(keyMasks, [numHeads, 1]) \n",
    "\n",
    "        # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]\n",
    "        keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "\n",
    "        # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "        paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "\n",
    "        # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "        # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "        maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * numHeads, queries_len, key_len]\n",
    "\n",
    "        # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "        # Decoder是生成模型，主要用在语言生成中\n",
    "        if causality:\n",
    "            diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "            maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "        # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]\n",
    "        weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "        # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        outputs = tf.matmul(weights, V_)\n",
    "\n",
    "        # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "        \n",
    "        outputs = tf.nn.dropout(outputs, keep_prob=keepProp)\n",
    "\n",
    "        # 对每个subLayers建立残差连接，即H(x) = F(x) + x\n",
    "        outputs += queries\n",
    "        # normalization 层\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _feedForward(self, inputs, filters, scope=\"multiheadAttention\"):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        \n",
    "        # 内层\n",
    "        params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 外层\n",
    "        params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "\n",
    "        # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "        # 维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 残差连接\n",
    "        outputs += inputs\n",
    "\n",
    "        # 归一化处理\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _positionEmbedding(self, scope=\"positionEmbedding\"):\n",
    "        # 生成可训练的位置向量\n",
    "        batchSize = self.config.batchSize\n",
    "        sequenceLen = self.config.sequenceLength\n",
    "        embeddingSize = self.config.model.embeddingSize\n",
    "        \n",
    "        # 生成位置的索引，并扩张到batch中所有的样本上\n",
    "        positionIndex = tf.tile(tf.expand_dims(tf.range(sequenceLen), 0), [batchSize, 1])\n",
    "\n",
    "        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分\n",
    "        positionEmbedding = np.array([[pos / np.power(10000, (i-i%2) / embeddingSize) for i in range(embeddingSize)] \n",
    "                                      for pos in range(sequenceLen)])\n",
    "\n",
    "        # 然后根据奇偶性分别用sin和cos函数来包装\n",
    "        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])\n",
    "        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])\n",
    "\n",
    "        # 将positionEmbedding转换成tensor的格式\n",
    "        positionEmbedding_ = tf.cast(positionEmbedding, dtype=tf.float32)\n",
    "\n",
    "        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]\n",
    "        positionEmbedded = tf.nn.embedding_lookup(positionEmbedding_, positionIndex)\n",
    "\n",
    "        return positionEmbedded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = 0\n",
    "    try:\n",
    "        auc = roc_auc_score(trueY, predY)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/hist is illegal; using dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/sparsity is illegal; using dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/hist is illegal; using dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/sparsity is illegal; using dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/hist is illegal; using dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/sparsity is illegal; using dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/hist is illegal; using dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/sparsity is illegal; using dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/hist is illegal; using dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/sparsity is illegal; using dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/hist is illegal; using dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/sparsity is illegal; using dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/hist is illegal; using transformer/transformer-1/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/hist is illegal; using transformer/transformer-1/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/hist is illegal; using conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/sparsity is illegal; using conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/hist is illegal; using conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/sparsity is illegal; using conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/hist is illegal; using conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/sparsity is illegal; using conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/hist is illegal; using conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/sparsity is illegal; using conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/hist is illegal; using transformer/transformer-1/Variable_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/hist is illegal; using transformer/transformer-1/Variable_3_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_3_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\pythonWp\\nlp\\textClassifier\\Transformer\\summarys\n",
      "\n",
      "start training model\n",
      "2019-03-19T12:16:37.743370, step: 1, loss: 0.5664293766021729, acc: 0.8125, auc: 0.5612, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:16:40.940824, step: 2, loss: 28.585981369018555, acc: 0.1641, auc: 0.6395, precision: 0.1641, recall: 1.0\n",
      "2019-03-19T12:16:44.200110, step: 3, loss: 1.1427102088928223, acc: 0.6797, auc: 0.7178, precision: 0.2917, recall: 0.6667\n",
      "2019-03-19T12:16:47.589051, step: 4, loss: 5.499493598937988, acc: 0.8281, auc: 0.5948, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:16:50.427467, step: 5, loss: 5.407867431640625, acc: 0.8828, auc: 0.4413, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:16:53.292806, step: 6, loss: 6.483226776123047, acc: 0.8594, auc: 0.4732, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:16:56.427427, step: 7, loss: 3.819209575653076, acc: 0.8906, auc: 0.4148, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:16:59.650810, step: 8, loss: 1.422389030456543, acc: 0.9062, auc: 0.444, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:02.886162, step: 9, loss: 9.318288803100586, acc: 0.1484, auc: 0.3972, precision: 0.1008, recall: 0.8571\n",
      "2019-03-19T12:17:06.151436, step: 10, loss: 0.9188563227653503, acc: 0.8672, auc: 0.5665, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:17:35.841074, step: 10, loss: 19.436777750651043, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-10\n",
      "\n",
      "2019-03-19T12:17:39.816449, step: 11, loss: 1.9465564489364624, acc: 0.8984, auc: 0.5084, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:42.992957, step: 12, loss: 4.236835479736328, acc: 0.8359, auc: 0.5866, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:46.133562, step: 13, loss: 4.268190860748291, acc: 0.8438, auc: 0.6884, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:49.291122, step: 14, loss: 3.6109156608581543, acc: 0.8594, auc: 0.6237, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:52.488575, step: 15, loss: 1.605454921722412, acc: 0.9062, auc: 0.6037, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:17:55.662091, step: 16, loss: 1.8996222019195557, acc: 0.8359, auc: 0.6005, precision: 0.4444, recall: 0.2\n",
      "2019-03-19T12:17:58.842590, step: 17, loss: 1.9496005773544312, acc: 0.7266, auc: 0.5553, precision: 0.2273, recall: 0.2174\n",
      "2019-03-19T12:18:02.033065, step: 18, loss: 4.0522990226745605, acc: 0.3594, auc: 0.7112, precision: 0.1383, recall: 0.9286\n",
      "2019-03-19T12:18:04.830586, step: 19, loss: 1.1579678058624268, acc: 0.7969, auc: 0.6948, precision: 0.3158, recall: 0.3158\n",
      "2019-03-19T12:18:07.669005, step: 20, loss: 1.7860151529312134, acc: 0.8594, auc: 0.6365, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:18:35.520551, step: 20, loss: 17.283294465806748, acc: 0.0, auc: 0.0, precision: 0.0, recall: 0.0\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-20\n",
      "\n",
      "2019-03-19T12:18:39.559753, step: 21, loss: 1.7853658199310303, acc: 0.8984, auc: 0.5826, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:18:42.873897, step: 22, loss: 3.2348341941833496, acc: 0.8359, auc: 0.5367, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:18:46.087308, step: 23, loss: 2.9999020099639893, acc: 0.8281, auc: 0.6003, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:18:49.252844, step: 24, loss: 1.590012550354004, acc: 0.8672, auc: 0.5723, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:18:52.427359, step: 25, loss: 0.8680679202079773, acc: 0.8438, auc: 0.482, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:18:55.624811, step: 26, loss: 2.763394355773926, acc: 0.3281, auc: 0.5428, precision: 0.1183, recall: 0.7333\n",
      "2019-03-19T12:18:58.835230, step: 27, loss: 0.5924943685531616, acc: 0.8438, auc: 0.5569, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:19:01.974838, step: 28, loss: 1.1048073768615723, acc: 0.8672, auc: 0.6354, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:19:05.144366, step: 29, loss: 1.5245996713638306, acc: 0.8438, auc: 0.7227, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:19:08.319877, step: 30, loss: 1.3110568523406982, acc: 0.8516, auc: 0.5874, precision: 0.3333, recall: 0.0556\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:19:36.538450, step: 30, loss: 4.829616175757514, acc: 0.2039888888888889, auc: 0.0, precision: 1.0, recall: 0.2039888888888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to ../model/Transformer/model/my-model-30\n",
      "\n",
      "2019-03-19T12:19:40.424064, step: 31, loss: 1.7218533754348755, acc: 0.7266, auc: 0.5623, precision: 0.2222, recall: 0.16\n",
      "2019-03-19T12:19:43.624508, step: 32, loss: 1.6002180576324463, acc: 0.6016, auc: 0.5934, precision: 0.2083, recall: 0.4348\n",
      "2019-03-19T12:19:46.762121, step: 33, loss: 0.9740555286407471, acc: 0.7344, auc: 0.7037, precision: 0.3056, recall: 0.55\n",
      "2019-03-19T12:19:49.961569, step: 34, loss: 0.8175125122070312, acc: 0.8359, auc: 0.7365, precision: 0.5, recall: 0.0952\n",
      "2019-03-19T12:19:53.163012, step: 35, loss: 0.8051372766494751, acc: 0.8828, auc: 0.6497, precision: 1.0, recall: 0.1176\n",
      "2019-03-19T12:19:56.411329, step: 36, loss: 0.49059629440307617, acc: 0.9297, auc: 0.5462, precision: 0.5, recall: 0.1111\n",
      "start training model\n",
      "2019-03-19T12:19:59.643691, step: 37, loss: 0.5245209336280823, acc: 0.875, auc: 0.816, precision: 1.0, recall: 0.2727\n",
      "2019-03-19T12:20:02.805238, step: 38, loss: 0.6323275566101074, acc: 0.7734, auc: 0.7068, precision: 0.2083, recall: 0.3333\n",
      "2019-03-19T12:20:06.016655, step: 39, loss: 0.5773007273674011, acc: 0.8203, auc: 0.6149, precision: 0.1765, recall: 0.25\n",
      "2019-03-19T12:20:09.250011, step: 40, loss: 0.6536204218864441, acc: 0.8828, auc: 0.62, precision: 0.75, recall: 0.1765\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:20:37.974231, step: 40, loss: 6.0641518698798285, acc: 0.0026, auc: 0.0, precision: 0.3333333333333333, recall: 0.0026\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-40\n",
      "\n",
      "2019-03-19T12:20:41.990496, step: 41, loss: 1.0048911571502686, acc: 0.8359, auc: 0.693, precision: 1.0, recall: 0.0455\n",
      "2019-03-19T12:20:45.212883, step: 42, loss: 0.6740424036979675, acc: 0.8359, auc: 0.7058, precision: 0.6, recall: 0.1364\n",
      "2019-03-19T12:20:48.354499, step: 43, loss: 0.6372241973876953, acc: 0.8125, auc: 0.7843, precision: 0.4, recall: 0.6667\n",
      "2019-03-19T12:20:51.518029, step: 44, loss: 0.400468647480011, acc: 0.8594, auc: 0.7985, precision: 0.4286, recall: 0.375\n",
      "2019-03-19T12:20:54.708501, step: 45, loss: 0.6740087270736694, acc: 0.8516, auc: 0.7884, precision: 0.7, recall: 0.3043\n",
      "2019-03-19T12:20:57.883015, step: 46, loss: 0.607456624507904, acc: 0.8438, auc: 0.7557, precision: 0.5455, recall: 0.2857\n",
      "2019-03-19T12:21:01.079471, step: 47, loss: 0.4829895794391632, acc: 0.8594, auc: 0.7939, precision: 0.4444, recall: 0.2353\n",
      "2019-03-19T12:21:04.296872, step: 48, loss: 0.44340452551841736, acc: 0.8672, auc: 0.8089, precision: 0.4118, recall: 0.5\n",
      "2019-03-19T12:21:07.510282, step: 49, loss: 0.504932165145874, acc: 0.8516, auc: 0.7517, precision: 0.3846, recall: 0.3125\n",
      "2019-03-19T12:21:10.728680, step: 50, loss: 0.4542839825153351, acc: 0.8359, auc: 0.8073, precision: 0.4286, recall: 0.3158\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:21:39.087876, step: 50, loss: 2.8827112780676947, acc: 0.12066666666666664, auc: 0.0, precision: 1.0, recall: 0.12066666666666664\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-50\n",
      "\n",
      "2019-03-19T12:21:43.078210, step: 51, loss: 0.4415608048439026, acc: 0.8672, auc: 0.7917, precision: 0.6364, recall: 0.35\n",
      "2019-03-19T12:21:46.277655, step: 52, loss: 0.3134447932243347, acc: 0.9062, auc: 0.8277, precision: 1.0, recall: 0.2\n",
      "2019-03-19T12:21:49.450175, step: 53, loss: 0.33264729380607605, acc: 0.8438, auc: 0.8928, precision: 0.5625, recall: 0.4091\n",
      "2019-03-19T12:21:52.667575, step: 54, loss: 0.4794025719165802, acc: 0.875, auc: 0.7694, precision: 0.4286, recall: 0.4286\n",
      "2019-03-19T12:21:55.856052, step: 55, loss: 0.7203949689865112, acc: 0.8359, auc: 0.6412, precision: 0.0, recall: 0.0\n",
      "2019-03-19T12:21:59.057495, step: 56, loss: 0.2657725512981415, acc: 0.8984, auc: 0.849, precision: 0.6, recall: 0.2143\n",
      "2019-03-19T12:22:02.237994, step: 57, loss: 0.2664409279823303, acc: 0.9062, auc: 0.8459, precision: 0.6, recall: 0.4286\n",
      "2019-03-19T12:22:05.414503, step: 58, loss: 0.27366238832473755, acc: 0.8984, auc: 0.9145, precision: 0.75, recall: 0.4737\n",
      "2019-03-19T12:22:08.599989, step: 59, loss: 0.42219895124435425, acc: 0.8516, auc: 0.7644, precision: 0.3333, recall: 0.3571\n",
      "2019-03-19T12:22:11.796444, step: 60, loss: 0.273303747177124, acc: 0.9062, auc: 0.8477, precision: 0.4, recall: 0.1818\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:22:40.373059, step: 60, loss: 3.3365010685390897, acc: 0.09028888888888889, auc: 0.0, precision: 1.0, recall: 0.09028888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-60\n",
      "\n",
      "2019-03-19T12:22:44.352424, step: 61, loss: 0.4139522910118103, acc: 0.8906, auc: 0.8548, precision: 0.8, recall: 0.2353\n",
      "2019-03-19T12:22:47.565833, step: 62, loss: 0.48567789793014526, acc: 0.8672, auc: 0.8575, precision: 0.9, recall: 0.36\n",
      "2019-03-19T12:22:50.745334, step: 63, loss: 0.3277963399887085, acc: 0.8828, auc: 0.8577, precision: 0.5385, recall: 0.4375\n",
      "2019-03-19T12:22:54.003625, step: 64, loss: 0.4064442813396454, acc: 0.8828, auc: 0.8561, precision: 0.5909, recall: 0.6842\n",
      "2019-03-19T12:22:57.242965, step: 65, loss: 0.4808220863342285, acc: 0.8359, auc: 0.7588, precision: 0.5, recall: 0.2857\n",
      "2019-03-19T12:23:00.437428, step: 66, loss: 0.39069950580596924, acc: 0.9062, auc: 0.8202, precision: 0.6667, recall: 0.2857\n",
      "2019-03-19T12:23:03.602965, step: 67, loss: 0.5506572723388672, acc: 0.8438, auc: 0.7642, precision: 0.625, recall: 0.2273\n",
      "2019-03-19T12:23:06.806402, step: 68, loss: 0.2748899757862091, acc: 0.8906, auc: 0.8861, precision: 0.5333, recall: 0.5333\n",
      "2019-03-19T12:23:09.951994, step: 69, loss: 0.3042139410972595, acc: 0.8828, auc: 0.8686, precision: 0.6, recall: 0.3529\n",
      "2019-03-19T12:23:13.141469, step: 70, loss: 0.3484841585159302, acc: 0.8672, auc: 0.84, precision: 0.5, recall: 0.5882\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:23:41.798867, step: 70, loss: 2.1751868989732532, acc: 0.22484444444444446, auc: 0.0, precision: 1.0, recall: 0.22484444444444446\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-70\n",
      "\n",
      "2019-03-19T12:23:45.737340, step: 71, loss: 0.3183111846446991, acc: 0.875, auc: 0.8792, precision: 0.5385, recall: 0.4118\n",
      "2019-03-19T12:23:48.905870, step: 72, loss: 0.3535824418067932, acc: 0.8672, auc: 0.9177, precision: 0.8333, recall: 0.2381\n",
      "start training model\n",
      "2019-03-19T12:23:52.061437, step: 73, loss: 0.23461422324180603, acc: 0.9141, auc: 0.8578, precision: 0.5714, recall: 0.3333\n",
      "2019-03-19T12:23:55.228969, step: 74, loss: 0.42566460371017456, acc: 0.8828, auc: 0.8518, precision: 0.625, recall: 0.5263\n",
      "2019-03-19T12:23:58.399493, step: 75, loss: 0.34149056673049927, acc: 0.9062, auc: 0.8258, precision: 0.6, recall: 0.4286\n",
      "2019-03-19T12:24:01.550073, step: 76, loss: 0.2707372307777405, acc: 0.875, auc: 0.9271, precision: 0.6154, recall: 0.4211\n",
      "2019-03-19T12:24:04.776447, step: 77, loss: 0.38191112875938416, acc: 0.875, auc: 0.8008, precision: 0.5, recall: 0.0625\n",
      "2019-03-19T12:24:08.041722, step: 78, loss: 0.5018701553344727, acc: 0.8203, auc: 0.8182, precision: 0.4762, recall: 0.4545\n",
      "2019-03-19T12:24:11.188309, step: 79, loss: 0.3214121460914612, acc: 0.9141, auc: 0.8713, precision: 0.9091, recall: 0.5\n",
      "2019-03-19T12:24:14.520402, step: 80, loss: 0.2523222863674164, acc: 0.9062, auc: 0.9304, precision: 1.0, recall: 0.4783\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:24:43.342361, step: 80, loss: 0.9983782172203064, acc: 0.4861111111111111, auc: 0.0, precision: 1.0, recall: 0.4861111111111111\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-80\n",
      "\n",
      "2019-03-19T12:24:47.089344, step: 81, loss: 0.32896357774734497, acc: 0.8828, auc: 0.8571, precision: 0.5263, recall: 0.625\n",
      "2019-03-19T12:24:49.887865, step: 82, loss: 0.26072824001312256, acc: 0.8828, auc: 0.9121, precision: 0.625, recall: 0.5263\n",
      "2019-03-19T12:24:52.639511, step: 83, loss: 0.22388289868831635, acc: 0.9297, auc: 0.9117, precision: 1.0, recall: 0.3571\n",
      "2019-03-19T12:24:55.560701, step: 84, loss: 0.31007707118988037, acc: 0.9062, auc: 0.8696, precision: 1.0, recall: 0.2941\n",
      "2019-03-19T12:24:58.720256, step: 85, loss: 0.29215481877326965, acc: 0.8828, auc: 0.9051, precision: 0.7273, recall: 0.4\n",
      "2019-03-19T12:25:01.914717, step: 86, loss: 0.4443034529685974, acc: 0.8594, auc: 0.8103, precision: 0.4444, recall: 0.5\n",
      "2019-03-19T12:25:05.091226, step: 87, loss: 0.24980294704437256, acc: 0.9062, auc: 0.929, precision: 0.6923, recall: 0.5294\n",
      "2019-03-19T12:25:08.251778, step: 88, loss: 0.3043740689754486, acc: 0.9141, auc: 0.8889, precision: 1.0, recall: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T12:25:11.453220, step: 89, loss: 0.31608664989471436, acc: 0.875, auc: 0.9009, precision: 0.875, recall: 0.3182\n",
      "2019-03-19T12:25:14.676604, step: 90, loss: 0.27401190996170044, acc: 0.9141, auc: 0.9524, precision: 0.5882, recall: 0.7143\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:25:43.393844, step: 90, loss: 1.315181228849623, acc: 0.4296888888888889, auc: 0.0, precision: 1.0, recall: 0.4296888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-90\n",
      "\n",
      "2019-03-19T12:25:47.369217, step: 91, loss: 0.3610806465148926, acc: 0.8594, auc: 0.8696, precision: 0.5294, recall: 0.4737\n",
      "2019-03-19T12:25:50.628506, step: 92, loss: 0.25006598234176636, acc: 0.9141, auc: 0.9086, precision: 0.8333, recall: 0.3333\n",
      "2019-03-19T12:25:53.871835, step: 93, loss: 0.20540723204612732, acc: 0.9219, auc: 0.9271, precision: 0.5, recall: 0.2\n",
      "2019-03-19T12:25:57.104195, step: 94, loss: 0.3702833652496338, acc: 0.8984, auc: 0.8606, precision: 1.0, recall: 0.2353\n",
      "2019-03-19T12:26:00.300651, step: 95, loss: 0.22374582290649414, acc: 0.9531, auc: 0.9163, precision: 0.8125, recall: 0.8125\n",
      "2019-03-19T12:26:03.278691, step: 96, loss: 0.2862454950809479, acc: 0.8594, auc: 0.9126, precision: 0.5263, recall: 0.5263\n",
      "2019-03-19T12:26:06.055284, step: 97, loss: 0.21387192606925964, acc: 0.8984, auc: 0.951, precision: 0.7273, recall: 0.4444\n",
      "2019-03-19T12:26:08.806916, step: 98, loss: 0.3119526505470276, acc: 0.8828, auc: 0.9367, precision: 0.8235, recall: 0.5385\n",
      "2019-03-19T12:26:11.542601, step: 99, loss: 0.2781226336956024, acc: 0.9062, auc: 0.8841, precision: 0.5833, recall: 0.5\n",
      "2019-03-19T12:26:14.323169, step: 100, loss: 0.23996508121490479, acc: 0.9141, auc: 0.928, precision: 0.7619, recall: 0.7273\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:26:42.848921, step: 100, loss: 1.8327458169725206, acc: 0.33940000000000003, auc: 0.0, precision: 1.0, recall: 0.33940000000000003\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-100\n",
      "\n",
      "2019-03-19T12:26:47.174359, step: 101, loss: 0.28894805908203125, acc: 0.9297, auc: 0.886, precision: 0.9167, recall: 0.5789\n",
      "2019-03-19T12:26:50.385773, step: 102, loss: 0.30023542046546936, acc: 0.9062, auc: 0.8707, precision: 1.0, recall: 0.2941\n",
      "2019-03-19T12:26:53.577242, step: 103, loss: 0.236456960439682, acc: 0.9141, auc: 0.9514, precision: 0.6957, recall: 0.8\n",
      "2019-03-19T12:26:56.812595, step: 104, loss: 0.23207972943782806, acc: 0.8984, auc: 0.9457, precision: 0.5714, recall: 0.5333\n",
      "2019-03-19T12:26:59.995088, step: 105, loss: 0.4179307222366333, acc: 0.8906, auc: 0.8928, precision: 0.8, recall: 0.5217\n",
      "2019-03-19T12:27:03.268339, step: 106, loss: 0.5022092461585999, acc: 0.8672, auc: 0.8765, precision: 0.8571, recall: 0.2727\n",
      "2019-03-19T12:27:06.566523, step: 107, loss: 0.3708598017692566, acc: 0.8906, auc: 0.8906, precision: 0.55, recall: 0.6875\n",
      "2019-03-19T12:27:09.844761, step: 108, loss: 0.20530250668525696, acc: 0.9219, auc: 0.9699, precision: 0.5789, recall: 0.8462\n",
      "start training model\n",
      "2019-03-19T12:27:13.158900, step: 109, loss: 0.22308866679668427, acc: 0.9297, auc: 0.9581, precision: 1.0, recall: 0.4\n",
      "2019-03-19T12:27:16.492989, step: 110, loss: 0.2039385437965393, acc: 0.9062, auc: 0.9837, precision: 1.0, recall: 0.1429\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:27:45.474521, step: 110, loss: 2.492227898703681, acc: 0.2508666666666667, auc: 0.0, precision: 1.0, recall: 0.2508666666666667\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-110\n",
      "\n",
      "2019-03-19T12:27:49.585533, step: 111, loss: 0.1853846311569214, acc: 0.9219, auc: 0.9715, precision: 0.875, recall: 0.4375\n",
      "2019-03-19T12:27:52.764036, step: 112, loss: 0.22287617623806, acc: 0.9297, auc: 0.9157, precision: 0.9091, recall: 0.5556\n",
      "2019-03-19T12:27:55.965479, step: 113, loss: 0.5699262619018555, acc: 0.8047, auc: 0.9407, precision: 0.4359, recall: 0.85\n",
      "2019-03-19T12:27:59.189860, step: 114, loss: 0.15006467700004578, acc: 0.9375, auc: 0.9706, precision: 0.8, recall: 0.5714\n",
      "2019-03-19T12:28:02.359387, step: 115, loss: 0.25915777683258057, acc: 0.9297, auc: 0.9806, precision: 0.8333, recall: 0.3846\n",
      "2019-03-19T12:28:05.520937, step: 116, loss: 0.6747969388961792, acc: 0.8906, auc: 0.88, precision: 1.0, recall: 0.125\n",
      "2019-03-19T12:28:08.751301, step: 117, loss: 0.3600410223007202, acc: 0.9062, auc: 0.9325, precision: 1.0, recall: 0.25\n",
      "2019-03-19T12:28:11.900883, step: 118, loss: 0.2815382182598114, acc: 0.8906, auc: 0.9136, precision: 0.6154, recall: 0.4706\n",
      "2019-03-19T12:28:15.187100, step: 119, loss: 0.569815456867218, acc: 0.8359, auc: 0.939, precision: 0.5, recall: 0.9524\n",
      "2019-03-19T12:28:18.479302, step: 120, loss: 0.3168871998786926, acc: 0.875, auc: 0.8788, precision: 0.5625, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:28:47.253387, step: 120, loss: 3.0091850492689343, acc: 0.20572222222222225, auc: 0.0, precision: 1.0, recall: 0.20572222222222225\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-120\n",
      "\n",
      "2019-03-19T12:28:51.269649, step: 121, loss: 0.23548153042793274, acc: 0.9375, auc: 0.9215, precision: 1.0, recall: 0.2727\n",
      "2019-03-19T12:28:54.536917, step: 122, loss: 0.46906453371047974, acc: 0.8828, auc: 0.901, precision: 1.0, recall: 0.1667\n",
      "2019-03-19T12:28:57.761298, step: 123, loss: 0.29200267791748047, acc: 0.8984, auc: 0.9305, precision: 0.8, recall: 0.4211\n",
      "2019-03-19T12:29:00.929828, step: 124, loss: 0.344357967376709, acc: 0.8516, auc: 0.9015, precision: 0.4, recall: 0.5333\n",
      "2019-03-19T12:29:04.116311, step: 125, loss: 0.28377488255500793, acc: 0.8984, auc: 0.9505, precision: 0.5, recall: 0.9231\n",
      "2019-03-19T12:29:07.292822, step: 126, loss: 0.3031952977180481, acc: 0.9062, auc: 0.9266, precision: 0.9091, recall: 0.4762\n",
      "2019-03-19T12:29:10.499249, step: 127, loss: 0.19745878875255585, acc: 0.9219, auc: 0.9697, precision: 0.9231, recall: 0.5714\n",
      "2019-03-19T12:29:13.780478, step: 128, loss: 0.2241310477256775, acc: 0.9453, auc: 0.947, precision: 0.9091, recall: 0.625\n",
      "2019-03-19T12:29:17.174408, step: 129, loss: 0.31829196214675903, acc: 0.9219, auc: 0.8228, precision: 0.5455, recall: 0.5455\n",
      "2019-03-19T12:29:20.380837, step: 130, loss: 0.158341184258461, acc: 0.9531, auc: 0.9636, precision: 0.6667, recall: 0.6667\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:29:48.983381, step: 130, loss: 1.8964334858788385, acc: 0.4461777777777778, auc: 0.0, precision: 1.0, recall: 0.4461777777777778\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-130\n",
      "\n",
      "2019-03-19T12:29:52.578802, step: 131, loss: 0.18337926268577576, acc: 0.9219, auc: 0.9751, precision: 0.875, recall: 0.6364\n",
      "2019-03-19T12:29:55.548833, step: 132, loss: 0.26553037762641907, acc: 0.9219, auc: 0.9395, precision: 0.8889, recall: 0.6667\n",
      "2019-03-19T12:29:58.756258, step: 133, loss: 0.1936875581741333, acc: 0.9141, auc: 0.9325, precision: 0.5333, recall: 0.6667\n",
      "2019-03-19T12:30:02.001123, step: 134, loss: 0.2677309811115265, acc: 0.9297, auc: 0.957, precision: 1.0, recall: 0.6538\n",
      "2019-03-19T12:30:05.280464, step: 135, loss: 0.1777157336473465, acc: 0.9531, auc: 0.9586, precision: 1.0, recall: 0.7391\n",
      "2019-03-19T12:30:08.549726, step: 136, loss: 0.23718982934951782, acc: 0.9219, auc: 0.9549, precision: 0.6, recall: 0.8571\n",
      "2019-03-19T12:30:11.734212, step: 137, loss: 0.26825475692749023, acc: 0.875, auc: 0.9476, precision: 0.8182, recall: 0.6\n",
      "2019-03-19T12:30:14.952609, step: 138, loss: 0.4405181109905243, acc: 0.875, auc: 0.8343, precision: 0.5833, recall: 0.3889\n",
      "2019-03-19T12:30:18.165023, step: 139, loss: 0.39751744270324707, acc: 0.8828, auc: 0.9103, precision: 0.8667, recall: 0.5\n",
      "2019-03-19T12:30:21.390401, step: 140, loss: 0.23044592142105103, acc: 0.9297, auc: 0.9406, precision: 0.8125, recall: 0.6842\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:30:49.896204, step: 140, loss: 0.7393212715784708, acc: 0.7074555555555556, auc: 0.0, precision: 1.0, recall: 0.7074555555555556\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-140\n",
      "\n",
      "2019-03-19T12:30:53.889530, step: 141, loss: 0.250133216381073, acc: 0.9297, auc: 0.981, precision: 0.7333, recall: 0.9565\n",
      "2019-03-19T12:30:57.082994, step: 142, loss: 0.2182554304599762, acc: 0.9453, auc: 0.9222, precision: 0.8667, recall: 0.7222\n",
      "2019-03-19T12:31:00.285434, step: 143, loss: 0.28088265657424927, acc: 0.9297, auc: 0.9386, precision: 0.6875, recall: 0.7333\n",
      "2019-03-19T12:31:03.452967, step: 144, loss: 0.298461377620697, acc: 0.9141, auc: 0.9523, precision: 0.875, recall: 0.4118\n",
      "start training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T12:31:06.648426, step: 145, loss: 0.2588319778442383, acc: 0.9219, auc: 0.9546, precision: 0.8571, recall: 0.4\n",
      "2019-03-19T12:31:09.804988, step: 146, loss: 0.16933077573776245, acc: 0.9141, auc: 0.9816, precision: 0.913, recall: 0.7\n",
      "2019-03-19T12:31:13.043334, step: 147, loss: 0.14817455410957336, acc: 0.9609, auc: 0.9671, precision: 0.875, recall: 0.8235\n",
      "2019-03-19T12:31:16.254750, step: 148, loss: 0.4251139461994171, acc: 0.8516, auc: 0.9308, precision: 0.4839, recall: 0.8333\n",
      "2019-03-19T12:31:19.462174, step: 149, loss: 0.1850402057170868, acc: 0.9297, auc: 0.9615, precision: 0.8889, recall: 0.5\n",
      "2019-03-19T12:31:22.695532, step: 150, loss: 0.16471421718597412, acc: 0.9219, auc: 0.985, precision: 1.0, recall: 0.2857\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:31:50.365571, step: 150, loss: 4.402641402350532, acc: 0.12761111111111112, auc: 0.0, precision: 1.0, recall: 0.12761111111111112\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-150\n",
      "\n",
      "2019-03-19T12:31:54.378843, step: 151, loss: 0.3501484990119934, acc: 0.8984, auc: 0.9638, precision: 1.0, recall: 0.3158\n",
      "2019-03-19T12:31:57.545379, step: 152, loss: 0.14090557396411896, acc: 0.9375, auc: 0.9874, precision: 0.8571, recall: 0.6667\n",
      "2019-03-19T12:32:00.752804, step: 153, loss: 0.2195378690958023, acc: 0.9375, auc: 0.9323, precision: 0.7143, recall: 0.7143\n",
      "2019-03-19T12:32:03.933303, step: 154, loss: 0.3708506226539612, acc: 0.8594, auc: 0.9298, precision: 0.4167, recall: 0.7143\n",
      "2019-03-19T12:32:07.140730, step: 155, loss: 0.06550372391939163, acc: 0.9688, auc: 0.9963, precision: 0.9444, recall: 0.85\n",
      "2019-03-19T12:32:10.334195, step: 156, loss: 0.18340462446212769, acc: 0.9297, auc: 0.9883, precision: 1.0, recall: 0.4375\n",
      "2019-03-19T12:32:13.521675, step: 157, loss: 0.3759635090827942, acc: 0.9219, auc: 0.9528, precision: 1.0, recall: 0.4118\n",
      "2019-03-19T12:32:16.706162, step: 158, loss: 0.41600877046585083, acc: 0.8906, auc: 0.9443, precision: 0.8333, recall: 0.4545\n",
      "2019-03-19T12:32:19.864719, step: 159, loss: 0.2369891107082367, acc: 0.9219, auc: 0.9483, precision: 0.7368, recall: 0.7368\n",
      "2019-03-19T12:32:23.063169, step: 160, loss: 0.37669670581817627, acc: 0.8984, auc: 0.9525, precision: 0.5926, recall: 0.8889\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:32:51.667710, step: 160, loss: 1.0405461523267958, acc: 0.6466999999999999, auc: 0.0, precision: 1.0, recall: 0.6466999999999999\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-160\n",
      "\n",
      "2019-03-19T12:32:55.979184, step: 161, loss: 0.17498576641082764, acc: 0.9453, auc: 0.9705, precision: 0.7727, recall: 0.8947\n",
      "2019-03-19T12:32:59.231491, step: 162, loss: 0.1707952469587326, acc: 0.9375, auc: 0.9768, precision: 0.9231, recall: 0.6316\n",
      "2019-03-19T12:33:02.559597, step: 163, loss: 0.25784850120544434, acc: 0.9453, auc: 0.9502, precision: 1.0, recall: 0.5882\n",
      "2019-03-19T12:33:05.739095, step: 164, loss: 0.38173753023147583, acc: 0.8828, auc: 0.9644, precision: 0.8889, recall: 0.3636\n",
      "2019-03-19T12:33:08.809890, step: 165, loss: 0.15010416507720947, acc: 0.9531, auc: 0.9822, precision: 0.8947, recall: 0.8095\n",
      "2019-03-19T12:33:11.585468, step: 166, loss: 0.2516154646873474, acc: 0.9141, auc: 0.9488, precision: 0.6818, recall: 0.7895\n",
      "2019-03-19T12:33:14.353070, step: 167, loss: 0.3146110773086548, acc: 0.8906, auc: 0.9339, precision: 0.3889, recall: 0.7\n",
      "2019-03-19T12:33:17.521601, step: 168, loss: 0.174618661403656, acc: 0.9375, auc: 0.9704, precision: 0.8333, recall: 0.625\n",
      "2019-03-19T12:33:20.674175, step: 169, loss: 0.2569451332092285, acc: 0.9062, auc: 0.991, precision: 1.0, recall: 0.4545\n",
      "2019-03-19T12:33:23.851681, step: 170, loss: 0.16734419763088226, acc: 0.9453, auc: 0.9806, precision: 1.0, recall: 0.4167\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:33:52.356490, step: 170, loss: 2.0473881827460394, acc: 0.5086777777777779, auc: 0.0, precision: 1.0, recall: 0.5086777777777779\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-170\n",
      "\n",
      "2019-03-19T12:33:56.290971, step: 171, loss: 0.1722434163093567, acc: 0.9453, auc: 0.9652, precision: 0.9286, recall: 0.6842\n",
      "2019-03-19T12:33:59.467480, step: 172, loss: 0.37347519397735596, acc: 0.8906, auc: 0.8985, precision: 0.6667, recall: 0.6667\n",
      "2019-03-19T12:34:02.684880, step: 173, loss: 0.5235939621925354, acc: 0.8828, auc: 0.9268, precision: 0.5, recall: 0.8\n",
      "2019-03-19T12:34:05.930205, step: 174, loss: 0.16577905416488647, acc: 0.9453, auc: 0.9596, precision: 0.7895, recall: 0.8333\n",
      "2019-03-19T12:34:09.149599, step: 175, loss: 0.4147011339664459, acc: 0.8984, auc: 0.9458, precision: 0.8889, recall: 0.4\n",
      "2019-03-19T12:34:12.411880, step: 176, loss: 0.23147930204868317, acc: 0.9375, auc: 0.9841, precision: 0.9091, recall: 0.5882\n",
      "2019-03-19T12:34:15.802817, step: 177, loss: 0.1345304250717163, acc: 0.9766, auc: 0.9759, precision: 0.9167, recall: 0.8462\n",
      "2019-03-19T12:34:19.085042, step: 178, loss: 0.31697511672973633, acc: 0.8516, auc: 0.9429, precision: 0.4783, recall: 0.6111\n",
      "2019-03-19T12:34:22.308426, step: 179, loss: 0.1489611566066742, acc: 0.9375, auc: 0.968, precision: 0.75, recall: 0.6429\n",
      "2019-03-19T12:34:25.570706, step: 180, loss: 0.18784229457378387, acc: 0.9375, auc: 0.9596, precision: 0.8571, recall: 0.6667\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:34:52.999389, step: 180, loss: 3.089356634351942, acc: 0.3446333333333333, auc: 0.0, precision: 1.0, recall: 0.3446333333333333\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-180\n",
      "\n",
      "start training model\n",
      "2019-03-19T12:34:57.236065, step: 181, loss: 0.1244392991065979, acc: 0.9531, auc: 0.9856, precision: 0.9, recall: 0.6429\n",
      "2019-03-19T12:35:00.475405, step: 182, loss: 0.19016556441783905, acc: 0.9141, auc: 0.9741, precision: 0.8462, recall: 0.55\n",
      "2019-03-19T12:35:03.692805, step: 183, loss: 0.07510142773389816, acc: 0.9766, auc: 0.9934, precision: 0.9412, recall: 0.8889\n",
      "2019-03-19T12:35:06.886269, step: 184, loss: 0.09187421202659607, acc: 0.9609, auc: 0.9947, precision: 0.7778, recall: 0.9333\n",
      "2019-03-19T12:35:10.041835, step: 185, loss: 0.07564961910247803, acc: 0.9688, auc: 0.9904, precision: 0.9375, recall: 0.8333\n",
      "2019-03-19T12:35:13.223331, step: 186, loss: 0.10170990228652954, acc: 0.9531, auc: 0.9917, precision: 1.0, recall: 0.7391\n",
      "2019-03-19T12:35:16.297114, step: 187, loss: 0.15190887451171875, acc: 0.9609, auc: 0.9818, precision: 0.9091, recall: 0.8696\n",
      "2019-03-19T12:35:19.063717, step: 188, loss: 0.14639197289943695, acc: 0.9609, auc: 0.976, precision: 0.8667, recall: 0.8125\n",
      "2019-03-19T12:35:21.876200, step: 189, loss: 0.09016174077987671, acc: 0.9688, auc: 0.9903, precision: 0.9412, recall: 0.8421\n",
      "2019-03-19T12:35:24.831302, step: 190, loss: 0.1274237036705017, acc: 0.9531, auc: 0.9724, precision: 0.9231, recall: 0.7059\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:35:53.367026, step: 190, loss: 1.4968399736616347, acc: 0.5598888888888889, auc: 0.0, precision: 1.0, recall: 0.5598888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-190\n",
      "\n",
      "2019-03-19T12:35:57.300511, step: 191, loss: 0.1370370090007782, acc: 0.9297, auc: 0.9847, precision: 0.9231, recall: 0.6\n",
      "2019-03-19T12:36:00.540851, step: 192, loss: 0.21164658665657043, acc: 0.9297, auc: 0.9724, precision: 0.6316, recall: 0.8571\n",
      "2019-03-19T12:36:03.789167, step: 193, loss: 0.15482115745544434, acc: 0.9375, auc: 0.972, precision: 0.7895, recall: 0.7895\n",
      "2019-03-19T12:36:07.005570, step: 194, loss: 0.07694670557975769, acc: 0.9688, auc: 0.995, precision: 0.7857, recall: 0.9167\n",
      "2019-03-19T12:36:10.249899, step: 195, loss: 0.14134103059768677, acc: 0.9453, auc: 0.9876, precision: 1.0, recall: 0.5333\n",
      "2019-03-19T12:36:13.448348, step: 196, loss: 0.22353199124336243, acc: 0.9297, auc: 0.9606, precision: 0.9091, recall: 0.5556\n",
      "2019-03-19T12:36:16.682703, step: 197, loss: 0.07205526530742645, acc: 0.9688, auc: 0.9959, precision: 1.0, recall: 0.7333\n",
      "2019-03-19T12:36:19.884146, step: 198, loss: 0.1617182344198227, acc: 0.9453, auc: 0.9648, precision: 0.9286, recall: 0.6842\n",
      "2019-03-19T12:36:23.088580, step: 199, loss: 0.14049729704856873, acc: 0.9297, auc: 0.9797, precision: 0.7778, recall: 0.7368\n",
      "2019-03-19T12:36:26.259105, step: 200, loss: 0.27964651584625244, acc: 0.9062, auc: 0.9317, precision: 0.8095, recall: 0.68\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:36:55.034189, step: 200, loss: 1.505635380744934, acc: 0.5659666666666667, auc: 0.0, precision: 1.0, recall: 0.5659666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to ../model/Transformer/model/my-model-200\n",
      "\n",
      "2019-03-19T12:36:59.063418, step: 201, loss: 0.1823279857635498, acc: 0.9219, auc: 0.9793, precision: 0.6, recall: 0.8571\n",
      "2019-03-19T12:37:01.890861, step: 202, loss: 0.09732115268707275, acc: 0.9609, auc: 0.9953, precision: 1.0, recall: 0.5455\n",
      "2019-03-19T12:37:04.958661, step: 203, loss: 0.3055267930030823, acc: 0.9219, auc: 0.9598, precision: 1.0, recall: 0.375\n",
      "2019-03-19T12:37:08.154120, step: 204, loss: 0.2115844339132309, acc: 0.9531, auc: 0.9442, precision: 0.8333, recall: 0.7143\n",
      "2019-03-19T12:37:11.426375, step: 205, loss: 0.24102194607257843, acc: 0.9219, auc: 0.9685, precision: 0.8824, recall: 0.6522\n",
      "2019-03-19T12:37:14.796366, step: 206, loss: 0.1624055951833725, acc: 0.9453, auc: 0.9915, precision: 0.7273, recall: 0.9412\n",
      "2019-03-19T12:37:18.131450, step: 207, loss: 0.24243983626365662, acc: 0.9375, auc: 0.996, precision: 0.619, recall: 1.0\n",
      "2019-03-19T12:37:21.457560, step: 208, loss: 0.03090999461710453, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9375\n",
      "2019-03-19T12:37:24.789653, step: 209, loss: 0.23556768894195557, acc: 0.9062, auc: 0.989, precision: 1.0, recall: 0.5556\n",
      "2019-03-19T12:37:28.151666, step: 210, loss: 0.17725595831871033, acc: 0.9375, auc: 0.9864, precision: 1.0, recall: 0.3333\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:37:57.233930, step: 210, loss: 2.912601523929172, acc: 0.3446222222222222, auc: 0.0, precision: 1.0, recall: 0.3446222222222222\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-210\n",
      "\n",
      "2019-03-19T12:38:01.230248, step: 211, loss: 0.14577403664588928, acc: 0.9531, auc: 0.9676, precision: 1.0, recall: 0.625\n",
      "2019-03-19T12:38:04.429695, step: 212, loss: 0.1998434066772461, acc: 0.9219, auc: 0.9833, precision: 0.5714, recall: 0.9231\n",
      "2019-03-19T12:38:07.684994, step: 213, loss: 0.25920021533966064, acc: 0.9219, auc: 0.9571, precision: 0.7, recall: 0.7778\n",
      "2019-03-19T12:38:10.853524, step: 214, loss: 0.20436926186084747, acc: 0.9062, auc: 0.9668, precision: 0.7917, recall: 0.7308\n",
      "2019-03-19T12:38:14.044994, step: 215, loss: 0.27893245220184326, acc: 0.9219, auc: 0.9796, precision: 1.0, recall: 0.6154\n",
      "2019-03-19T12:38:17.206542, step: 216, loss: 0.22367331385612488, acc: 0.9453, auc: 0.9475, precision: 0.9091, recall: 0.625\n",
      "start training model\n",
      "2019-03-19T12:38:20.381057, step: 217, loss: 0.03150468319654465, acc: 0.9844, auc: 0.998, precision: 0.9231, recall: 0.9231\n",
      "2019-03-19T12:38:23.618406, step: 218, loss: 0.06790986657142639, acc: 0.9844, auc: 0.9913, precision: 1.0, recall: 0.8947\n",
      "2019-03-19T12:38:26.858742, step: 219, loss: 0.30048269033432007, acc: 0.8906, auc: 0.9569, precision: 0.625, recall: 0.75\n",
      "2019-03-19T12:38:30.096088, step: 220, loss: 0.10492601245641708, acc: 0.9609, auc: 0.985, precision: 0.8889, recall: 0.8421\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:38:57.827961, step: 220, loss: 2.4062447018093533, acc: 0.45397777777777776, auc: 0.0, precision: 1.0, recall: 0.45397777777777776\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-220\n",
      "\n",
      "2019-03-19T12:39:01.857191, step: 221, loss: 0.1326446533203125, acc: 0.9453, auc: 0.9889, precision: 0.8667, recall: 0.7222\n",
      "2019-03-19T12:39:05.090548, step: 222, loss: 0.10646781325340271, acc: 0.9688, auc: 0.9918, precision: 1.0, recall: 0.7895\n",
      "2019-03-19T12:39:08.271047, step: 223, loss: 0.04366797208786011, acc: 0.9844, auc: 0.998, precision: 1.0, recall: 0.9167\n",
      "2019-03-19T12:39:11.460521, step: 224, loss: 0.10192043334245682, acc: 0.9609, auc: 0.9905, precision: 0.8333, recall: 0.8824\n",
      "2019-03-19T12:39:14.698866, step: 225, loss: 0.13682860136032104, acc: 0.9453, auc: 0.9847, precision: 0.8824, recall: 0.75\n",
      "2019-03-19T12:39:17.950174, step: 226, loss: 0.08972552418708801, acc: 0.9766, auc: 0.9835, precision: 0.9286, recall: 0.8667\n",
      "2019-03-19T12:39:21.191511, step: 227, loss: 0.10475225746631622, acc: 0.9609, auc: 0.9937, precision: 0.8182, recall: 0.9474\n",
      "2019-03-19T12:39:24.373007, step: 228, loss: 0.0842442587018013, acc: 0.9609, auc: 0.995, precision: 0.8667, recall: 0.8125\n",
      "2019-03-19T12:39:27.552507, step: 229, loss: 0.06545881181955338, acc: 0.9766, auc: 0.9916, precision: 1.0, recall: 0.8125\n",
      "2019-03-19T12:39:30.777886, step: 230, loss: 0.16638417541980743, acc: 0.9375, auc: 0.9835, precision: 0.8182, recall: 0.6\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:39:59.387412, step: 230, loss: 2.752333084742228, acc: 0.3836777777777778, auc: 0.0, precision: 1.0, recall: 0.3836777777777778\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-230\n",
      "\n",
      "2019-03-19T12:40:03.356801, step: 231, loss: 0.12815910577774048, acc: 0.9688, auc: 0.9619, precision: 0.875, recall: 0.7\n",
      "2019-03-19T12:40:06.612103, step: 232, loss: 0.1284315288066864, acc: 0.9609, auc: 0.9873, precision: 1.0, recall: 0.7059\n",
      "2019-03-19T12:40:09.887348, step: 233, loss: 0.10391153395175934, acc: 0.9453, auc: 0.9919, precision: 0.8667, recall: 0.7222\n",
      "2019-03-19T12:40:13.124693, step: 234, loss: 0.11926920711994171, acc: 0.9531, auc: 0.991, precision: 0.9, recall: 0.8182\n",
      "2019-03-19T12:40:16.317158, step: 235, loss: 0.059791162610054016, acc: 0.9922, auc: 0.999, precision: 0.9474, recall: 1.0\n",
      "2019-03-19T12:40:19.524587, step: 236, loss: 0.2697099447250366, acc: 0.9375, auc: 0.9907, precision: 0.6, recall: 1.0\n",
      "2019-03-19T12:40:22.739992, step: 237, loss: 0.09440494328737259, acc: 0.9766, auc: 0.9923, precision: 1.0, recall: 0.8421\n",
      "2019-03-19T12:40:25.918495, step: 238, loss: 0.3114391565322876, acc: 0.9375, auc: 0.9768, precision: 1.0, recall: 0.6364\n",
      "2019-03-19T12:40:29.100987, step: 239, loss: 0.25876712799072266, acc: 0.9141, auc: 0.992, precision: 1.0, recall: 0.5417\n",
      "2019-03-19T12:40:32.289466, step: 240, loss: 0.08005392551422119, acc: 0.9688, auc: 0.9936, precision: 1.0, recall: 0.8182\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:41:00.440217, step: 240, loss: 0.8307058347596062, acc: 0.7369888888888889, auc: 0.0, precision: 1.0, recall: 0.7369888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-240\n",
      "\n",
      "2019-03-19T12:41:04.428558, step: 241, loss: 0.2473941147327423, acc: 0.9062, auc: 0.9986, precision: 0.6129, recall: 1.0\n",
      "2019-03-19T12:41:07.664907, step: 242, loss: 0.10512496531009674, acc: 0.9688, auc: 0.9799, precision: 1.0, recall: 0.75\n",
      "2019-03-19T12:41:10.861362, step: 243, loss: 0.05662595480680466, acc: 0.9844, auc: 0.9979, precision: 0.8947, recall: 1.0\n",
      "2019-03-19T12:41:14.016927, step: 244, loss: 0.1816558837890625, acc: 0.9219, auc: 0.9917, precision: 1.0, recall: 0.5\n",
      "2019-03-19T12:41:17.185458, step: 245, loss: 0.1000286415219307, acc: 0.9688, auc: 0.9806, precision: 0.8571, recall: 0.8571\n",
      "2019-03-19T12:41:20.366955, step: 246, loss: 0.07427167892456055, acc: 0.9766, auc: 0.992, precision: 0.8571, recall: 0.9231\n",
      "2019-03-19T12:41:23.595324, step: 247, loss: 0.09393847733736038, acc: 0.9609, auc: 0.9893, precision: 0.9444, recall: 0.8095\n",
      "2019-03-19T12:41:26.849625, step: 248, loss: 0.10063929855823517, acc: 0.9766, auc: 0.9855, precision: 0.9333, recall: 0.875\n",
      "2019-03-19T12:41:30.055058, step: 249, loss: 0.11137688905000687, acc: 0.9375, auc: 0.9879, precision: 0.8125, recall: 0.7222\n",
      "2019-03-19T12:41:33.271460, step: 250, loss: 0.073346808552742, acc: 0.9609, auc: 0.9974, precision: 0.8, recall: 0.9412\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:42:01.947809, step: 250, loss: 2.790894720289442, acc: 0.3585222222222222, auc: 0.0, precision: 1.0, recall: 0.3585222222222222\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-250\n",
      "\n",
      "2019-03-19T12:42:05.936148, step: 251, loss: 0.11822435259819031, acc: 0.9453, auc: 0.9899, precision: 1.0, recall: 0.6111\n",
      "2019-03-19T12:42:09.171499, step: 252, loss: 0.06786973774433136, acc: 0.9609, auc: 0.9933, precision: 0.8667, recall: 0.8125\n",
      "start training model\n",
      "2019-03-19T12:42:12.384909, step: 253, loss: 0.07190866023302078, acc: 0.9531, auc: 0.9961, precision: 1.0, recall: 0.625\n",
      "2019-03-19T12:42:15.603306, step: 254, loss: 0.04835699126124382, acc: 0.9844, auc: 0.9935, precision: 1.0, recall: 0.8667\n",
      "2019-03-19T12:42:18.770839, step: 255, loss: 0.025065239518880844, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-03-19T12:42:21.963307, step: 256, loss: 0.11996437609195709, acc: 0.9609, auc: 0.9969, precision: 0.8077, recall: 1.0\n",
      "2019-03-19T12:42:25.148791, step: 257, loss: 0.03901351988315582, acc: 0.9766, auc: 0.9988, precision: 0.9565, recall: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T12:42:28.376167, step: 258, loss: 0.06166854500770569, acc: 0.9609, auc: 0.995, precision: 0.9091, recall: 0.7143\n",
      "2019-03-19T12:42:31.502807, step: 259, loss: 0.1207493469119072, acc: 0.9609, auc: 0.9956, precision: 1.0, recall: 0.6429\n",
      "2019-03-19T12:42:34.777055, step: 260, loss: 0.15157967805862427, acc: 0.9219, auc: 0.9973, precision: 1.0, recall: 0.5238\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:43:03.024549, step: 260, loss: 1.8868075874116685, acc: 0.5199555555555556, auc: 0.0, precision: 1.0, recall: 0.5199555555555556\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-260\n",
      "\n",
      "2019-03-19T12:43:06.938089, step: 261, loss: 0.17744800448417664, acc: 0.9375, auc: 0.9772, precision: 1.0, recall: 0.5294\n",
      "2019-03-19T12:43:10.146513, step: 262, loss: 0.18469783663749695, acc: 0.9453, auc: 0.9981, precision: 0.7308, recall: 1.0\n",
      "2019-03-19T12:43:13.406798, step: 263, loss: 0.12074724584817886, acc: 0.9609, auc: 0.9855, precision: 0.85, recall: 0.8947\n",
      "2019-03-19T12:43:16.598267, step: 264, loss: 0.04066509008407593, acc: 0.9844, auc: 1.0, precision: 0.913, recall: 1.0\n",
      "2019-03-19T12:43:19.732889, step: 265, loss: 0.05689568072557449, acc: 0.9688, auc: 0.9958, precision: 0.9444, recall: 0.85\n",
      "2019-03-19T12:43:22.918373, step: 266, loss: 0.07867664098739624, acc: 0.9453, auc: 0.9984, precision: 1.0, recall: 0.5882\n",
      "2019-03-19T12:43:26.131784, step: 267, loss: 0.23439379036426544, acc: 0.9453, auc: 0.9793, precision: 1.0, recall: 0.6111\n",
      "2019-03-19T12:43:29.328239, step: 268, loss: 0.12240802496671677, acc: 0.9609, auc: 0.9913, precision: 1.0, recall: 0.7368\n",
      "2019-03-19T12:43:32.520707, step: 269, loss: 0.09320619702339172, acc: 0.9609, auc: 0.986, precision: 0.8667, recall: 0.8125\n",
      "2019-03-19T12:43:35.698212, step: 270, loss: 0.09410183131694794, acc: 0.9688, auc: 0.9982, precision: 0.7895, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:44:04.281808, step: 270, loss: 0.8755040599240197, acc: 0.730911111111111, auc: 0.0, precision: 1.0, recall: 0.730911111111111\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-270\n",
      "\n",
      "2019-03-19T12:44:08.255187, step: 271, loss: 0.11345026642084122, acc: 0.9531, auc: 0.999, precision: 0.75, recall: 1.0\n",
      "2019-03-19T12:44:11.469596, step: 272, loss: 0.061303697526454926, acc: 0.9766, auc: 0.9926, precision: 0.9474, recall: 0.9\n",
      "2019-03-19T12:44:14.772780, step: 273, loss: 0.05028560012578964, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.8333\n",
      "2019-03-19T12:44:18.056988, step: 274, loss: 0.11694346368312836, acc: 0.9453, auc: 0.9964, precision: 1.0, recall: 0.7083\n",
      "2019-03-19T12:44:21.258431, step: 275, loss: 0.075670525431633, acc: 0.9609, auc: 0.993, precision: 1.0, recall: 0.5455\n",
      "2019-03-19T12:44:24.525697, step: 276, loss: 0.04605140537023544, acc: 0.9844, auc: 0.9967, precision: 1.0, recall: 0.875\n",
      "2019-03-19T12:44:27.782990, step: 277, loss: 0.06722942739725113, acc: 0.9688, auc: 0.9931, precision: 0.9333, recall: 0.8235\n",
      "2019-03-19T12:44:30.948528, step: 278, loss: 0.08483454585075378, acc: 0.9922, auc: 0.9941, precision: 0.9091, recall: 1.0\n",
      "2019-03-19T12:44:34.174905, step: 279, loss: 0.052861765027046204, acc: 0.9766, auc: 1.0, precision: 0.8333, recall: 1.0\n",
      "2019-03-19T12:44:37.374353, step: 280, loss: 0.11997669190168381, acc: 0.9531, auc: 0.9893, precision: 0.8571, recall: 0.8571\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:45:05.247499, step: 280, loss: 2.283374455240038, acc: 0.4808888888888889, auc: 0.0, precision: 1.0, recall: 0.4808888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-280\n",
      "\n",
      "2019-03-19T12:45:09.256782, step: 281, loss: 0.11227018386125565, acc: 0.9609, auc: 0.9914, precision: 1.0, recall: 0.7727\n",
      "2019-03-19T12:45:12.505100, step: 282, loss: 0.0253421813249588, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.8947\n",
      "2019-03-19T12:45:15.705545, step: 283, loss: 0.07056938856840134, acc: 0.9844, auc: 0.9876, precision: 0.9091, recall: 0.9091\n",
      "2019-03-19T12:45:18.849142, step: 284, loss: 0.02002790942788124, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9412\n",
      "2019-03-19T12:45:22.023657, step: 285, loss: 0.07698065042495728, acc: 0.9688, auc: 0.9931, precision: 1.0, recall: 0.7143\n",
      "2019-03-19T12:45:25.250050, step: 286, loss: 0.049077436327934265, acc: 0.9766, auc: 0.9989, precision: 0.8889, recall: 0.9412\n",
      "2019-03-19T12:45:28.469427, step: 287, loss: 0.03882020711898804, acc: 0.9844, auc: 0.9984, precision: 0.9412, recall: 0.9412\n",
      "2019-03-19T12:45:31.626985, step: 288, loss: 0.06734003126621246, acc: 0.9766, auc: 0.9946, precision: 0.9231, recall: 0.96\n",
      "start training model\n",
      "2019-03-19T12:45:34.900236, step: 289, loss: 0.06238827109336853, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.8636\n",
      "2019-03-19T12:45:38.136589, step: 290, loss: 0.11278005689382553, acc: 0.9766, auc: 0.9815, precision: 0.9474, recall: 0.9\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:46:06.672309, step: 290, loss: 1.5069364309310913, acc: 0.602411111111111, auc: 0.0, precision: 1.0, recall: 0.602411111111111\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-290\n",
      "\n",
      "2019-03-19T12:46:10.831194, step: 291, loss: 0.07881905883550644, acc: 0.9844, auc: 0.9984, precision: 0.8947, recall: 1.0\n",
      "2019-03-19T12:46:14.170268, step: 292, loss: 0.02884930931031704, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.8333\n",
      "2019-03-19T12:46:17.415593, step: 293, loss: 0.07077819108963013, acc: 0.9609, auc: 0.9954, precision: 0.9091, recall: 0.8696\n",
      "2019-03-19T12:46:20.301909, step: 294, loss: 0.028606794774532318, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9091\n",
      "2019-03-19T12:46:23.231048, step: 295, loss: 0.06045234203338623, acc: 0.9688, auc: 0.9978, precision: 1.0, recall: 0.75\n",
      "2019-03-19T12:46:26.393595, step: 296, loss: 0.0405435785651207, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9444\n",
      "2019-03-19T12:46:29.608002, step: 297, loss: 0.02472905069589615, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9333\n",
      "2019-03-19T12:46:32.786507, step: 298, loss: 0.02170167863368988, acc: 0.9922, auc: 1.0, precision: 0.9091, recall: 1.0\n",
      "2019-03-19T12:46:35.981965, step: 299, loss: 0.09995116293430328, acc: 0.9609, auc: 0.9857, precision: 0.9286, recall: 0.7647\n",
      "2019-03-19T12:46:39.165455, step: 300, loss: 0.057675644755363464, acc: 0.9766, auc: 0.9989, precision: 0.8824, recall: 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:47:07.819862, step: 300, loss: 1.5702000459035237, acc: 0.586811111111111, auc: 0.0, precision: 1.0, recall: 0.586811111111111\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-300\n",
      "\n",
      "2019-03-19T12:47:11.881007, step: 301, loss: 0.03220410645008087, acc: 0.9844, auc: 0.9987, precision: 1.0, recall: 0.8462\n",
      "2019-03-19T12:47:15.107382, step: 302, loss: 0.07686560600996017, acc: 0.9844, auc: 0.9947, precision: 0.9524, recall: 0.9524\n",
      "2019-03-19T12:47:18.307829, step: 303, loss: 0.05099788308143616, acc: 0.9766, auc: 0.9978, precision: 1.0, recall: 0.8571\n",
      "2019-03-19T12:47:21.552156, step: 304, loss: 0.051909204572439194, acc: 0.9453, auc: 1.0, precision: 1.0, recall: 0.65\n",
      "2019-03-19T12:47:24.855328, step: 305, loss: 0.023180443793535233, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9524\n",
      "2019-03-19T12:47:28.119612, step: 306, loss: 0.0793393924832344, acc: 0.9609, auc: 0.9931, precision: 0.875, recall: 0.8235\n",
      "2019-03-19T12:47:31.444714, step: 307, loss: 0.10287037491798401, acc: 0.9688, auc: 0.9969, precision: 0.8696, recall: 0.9524\n",
      "2019-03-19T12:47:34.749877, step: 308, loss: 0.05896836891770363, acc: 0.9922, auc: 0.9986, precision: 0.95, recall: 1.0\n",
      "2019-03-19T12:47:38.063022, step: 309, loss: 0.03727957606315613, acc: 0.9766, auc: 0.9991, precision: 1.0, recall: 0.8636\n",
      "2019-03-19T12:47:41.417057, step: 310, loss: 0.06983517855405807, acc: 0.9609, auc: 0.9986, precision: 1.0, recall: 0.7368\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:48:09.751319, step: 310, loss: 3.8020724190606012, acc: 0.3020777777777778, auc: 0.0, precision: 1.0, recall: 0.3020777777777778\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-310\n",
      "\n",
      "2019-03-19T12:48:13.878289, step: 311, loss: 0.1752438247203827, acc: 0.9219, auc: 0.9981, precision: 1.0, recall: 0.4737\n",
      "2019-03-19T12:48:17.059784, step: 312, loss: 0.05942434445023537, acc: 0.9844, auc: 0.9931, precision: 1.0, recall: 0.8571\n",
      "2019-03-19T12:48:20.276186, step: 313, loss: 0.07845744490623474, acc: 0.9688, auc: 0.9959, precision: 0.8235, recall: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19T12:48:23.494583, step: 314, loss: 0.16377988457679749, acc: 0.9531, auc: 0.9975, precision: 0.75, recall: 1.0\n",
      "2019-03-19T12:48:26.699020, step: 315, loss: 0.041681546717882156, acc: 0.9844, auc: 0.9995, precision: 0.9412, recall: 0.9412\n",
      "2019-03-19T12:48:29.834638, step: 316, loss: 0.04517344385385513, acc: 0.9766, auc: 0.9962, precision: 0.9231, recall: 0.8571\n",
      "2019-03-19T12:48:33.054033, step: 317, loss: 0.049309954047203064, acc: 0.9922, auc: 0.9992, precision: 1.0, recall: 0.96\n",
      "2019-03-19T12:48:36.285394, step: 318, loss: 0.05691087618470192, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.6429\n",
      "2019-03-19T12:48:39.501795, step: 319, loss: 0.25657975673675537, acc: 0.9297, auc: 0.9977, precision: 1.0, recall: 0.55\n",
      "2019-03-19T12:48:42.738145, step: 320, loss: 0.06281723082065582, acc: 0.9766, auc: 0.9984, precision: 1.0, recall: 0.8235\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:49:11.248937, step: 320, loss: 1.5599736637539334, acc: 0.6328, auc: 0.0, precision: 1.0, recall: 0.6328\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-320\n",
      "\n",
      "2019-03-19T12:49:15.338006, step: 321, loss: 0.11251121759414673, acc: 0.9609, auc: 0.995, precision: 0.7619, recall: 1.0\n",
      "2019-03-19T12:49:18.538450, step: 322, loss: 0.17583927512168884, acc: 0.9375, auc: 0.996, precision: 0.6923, recall: 1.0\n",
      "2019-03-19T12:49:21.735904, step: 323, loss: 0.03377286717295647, acc: 0.9922, auc: 1.0, precision: 0.9333, recall: 1.0\n",
      "2019-03-19T12:49:24.931362, step: 324, loss: 0.20463649928569794, acc: 0.9688, auc: 0.9676, precision: 1.0, recall: 0.75\n",
      "start training model\n",
      "2019-03-19T12:49:28.103883, step: 325, loss: 0.027677852660417557, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9474\n",
      "2019-03-19T12:49:31.281390, step: 326, loss: 0.04952719807624817, acc: 0.9688, auc: 0.9994, precision: 1.0, recall: 0.7143\n",
      "2019-03-19T12:49:34.460890, step: 327, loss: 0.07221678644418716, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.8235\n",
      "2019-03-19T12:49:37.607489, step: 328, loss: 0.1578541100025177, acc: 0.9375, auc: 0.9965, precision: 1.0, recall: 0.68\n",
      "2019-03-19T12:49:40.480798, step: 329, loss: 0.039942070841789246, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.8333\n",
      "2019-03-19T12:49:43.307244, step: 330, loss: 0.10513006150722504, acc: 0.9688, auc: 0.998, precision: 0.7647, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:50:11.621069, step: 330, loss: 0.9396520455678304, acc: 0.7586888888888889, auc: 0.0, precision: 1.0, recall: 0.7586888888888889\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-330\n",
      "\n",
      "2019-03-19T12:50:15.556550, step: 331, loss: 0.059399209916591644, acc: 0.9766, auc: 0.9985, precision: 0.8571, recall: 1.0\n",
      "2019-03-19T12:50:18.783923, step: 332, loss: 0.13300485908985138, acc: 0.9375, auc: 0.9932, precision: 0.72, recall: 0.9474\n",
      "2019-03-19T12:50:22.053185, step: 333, loss: 0.013065909966826439, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9333\n",
      "2019-03-19T12:50:25.222713, step: 334, loss: 0.08803890645503998, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.7778\n",
      "2019-03-19T12:50:28.385258, step: 335, loss: 0.07283742725849152, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.8667\n",
      "2019-03-19T12:50:31.543815, step: 336, loss: 0.14740389585494995, acc: 0.9531, auc: 1.0, precision: 1.0, recall: 0.6842\n",
      "2019-03-19T12:50:34.740271, step: 337, loss: 0.028402192518115044, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.8462\n",
      "2019-03-19T12:50:37.925757, step: 338, loss: 0.0056551359593868256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-19T12:50:41.088303, step: 339, loss: 0.029278747737407684, acc: 0.9922, auc: 0.9992, precision: 1.0, recall: 0.9091\n",
      "2019-03-19T12:50:44.391476, step: 340, loss: 0.06770583987236023, acc: 0.9766, auc: 0.9954, precision: 1.0, recall: 0.8696\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:51:12.930190, step: 340, loss: 1.0988196863068476, acc: 0.7187666666666667, auc: 0.0, precision: 1.0, recall: 0.7187666666666667\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-340\n",
      "\n",
      "2019-03-19T12:51:16.941469, step: 341, loss: 0.16657617688179016, acc: 0.9453, auc: 1.0, precision: 0.72, recall: 1.0\n",
      "2019-03-19T12:51:20.117976, step: 342, loss: 0.08221167325973511, acc: 0.9766, auc: 0.9883, precision: 0.9333, recall: 0.875\n",
      "2019-03-19T12:51:22.940434, step: 343, loss: 0.059015803039073944, acc: 0.9844, auc: 0.9982, precision: 1.0, recall: 0.9048\n",
      "2019-03-19T12:51:25.916478, step: 344, loss: 0.06819837540388107, acc: 0.9766, auc: 0.996, precision: 0.9412, recall: 0.8889\n",
      "2019-03-19T12:51:29.142854, step: 345, loss: 0.07465620338916779, acc: 0.9766, auc: 0.9968, precision: 1.0, recall: 0.8235\n",
      "2019-03-19T12:51:32.405133, step: 346, loss: 0.15162743628025055, acc: 0.9609, auc: 0.9909, precision: 1.0, recall: 0.7222\n",
      "2019-03-19T12:51:35.569674, step: 347, loss: 0.044015444815158844, acc: 0.9766, auc: 0.9982, precision: 0.9286, recall: 0.8667\n",
      "2019-03-19T12:51:38.714269, step: 348, loss: 0.09641940146684647, acc: 0.9531, auc: 0.9991, precision: 0.7692, recall: 1.0\n",
      "2019-03-19T12:51:41.995498, step: 349, loss: 0.10729429125785828, acc: 0.9688, auc: 0.9984, precision: 0.8095, recall: 1.0\n",
      "2019-03-19T12:51:45.300663, step: 350, loss: 0.03008291870355606, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.95\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:52:14.266239, step: 350, loss: 3.565517134136624, acc: 0.37674444444444444, auc: 0.0, precision: 1.0, recall: 0.37674444444444444\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-350\n",
      "\n",
      "2019-03-19T12:52:18.158835, step: 351, loss: 0.005379247013479471, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-19T12:52:21.366260, step: 352, loss: 0.2548682987689972, acc: 0.9375, auc: 0.9836, precision: 1.0, recall: 0.6667\n",
      "2019-03-19T12:52:24.639512, step: 353, loss: 0.04482477903366089, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.8125\n",
      "2019-03-19T12:52:27.794079, step: 354, loss: 0.032368648797273636, acc: 0.9844, auc: 0.9985, precision: 0.9444, recall: 0.9444\n",
      "2019-03-19T12:52:30.934685, step: 355, loss: 0.08102090656757355, acc: 0.9844, auc: 0.9893, precision: 0.9524, recall: 0.9524\n",
      "2019-03-19T12:52:34.139118, step: 356, loss: 0.07874028384685516, acc: 0.9844, auc: 0.9972, precision: 0.95, recall: 0.95\n",
      "2019-03-19T12:52:37.303660, step: 357, loss: 0.0163692906498909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-19T12:52:40.530034, step: 358, loss: 0.08773726224899292, acc: 0.9766, auc: 0.9876, precision: 0.9545, recall: 0.913\n",
      "2019-03-19T12:52:43.883073, step: 359, loss: 0.020603958517313004, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9474\n",
      "2019-03-19T12:52:47.129395, step: 360, loss: 0.06078079715371132, acc: 0.9766, auc: 0.9946, precision: 0.8571, recall: 0.9231\n",
      "\n",
      "Evaluation:\n",
      "2019-03-19T12:53:15.673099, step: 360, loss: 2.8243475490146213, acc: 0.4522666666666667, auc: 0.0, precision: 1.0, recall: 0.4522666666666667\n",
      "Saved model checkpoint to ../model/Transformer/model/my-model-360\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-11-ee8eab1ee25e>:142: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../model/Transformer/savedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "embeddedPosition = fixedPositionEmbedding(config.batchSize, config.sequenceLength)\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Transformer/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: 1.0,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Transformer/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(transformer.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(transformer.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(transformer.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
